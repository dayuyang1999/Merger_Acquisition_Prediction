
CUDA availability: True
#### mat_b tensor([[[ 0.1657,  0.4276, -0.1087,  0.0316, -0.4133,  0.6113, -0.0493,
          -0.0313,  0.1738, -1.0094, -0.0073, -0.3054,  0.1418,  0.1074,
           0.4121,  0.0130, -0.5374, -0.1073, -0.1976, -0.0483,  0.3141,
          -0.0442, -0.0182,  0.0252, -0.2093,  0.0278, -0.2882, -0.4932,
          -0.3521,  0.3730,  0.0507,  0.1509],
         [ 0.1746,  0.4201, -0.1094,  0.0421, -0.4171,  0.6030, -0.0497,
          -0.0356,  0.1764, -1.0125, -0.0141, -0.3066,  0.1543,  0.1093,
           0.4253,  0.0147, -0.5397, -0.1032, -0.2013, -0.0552,  0.3116,
          -0.0395, -0.0043,  0.0280, -0.2111,  0.0183, -0.2886, -0.5059,
          -0.3622,  0.3764,  0.0455,  0.1512],
         [ 0.1891,  0.4093, -0.1109,  0.0575, -0.4232,  0.5894, -0.0508,
          -0.0431,  0.1807, -1.0157, -0.0238, -0.3091,  0.1741,  0.1122,
           0.4460,  0.0163, -0.5436, -0.0974, -0.2058, -0.0673,  0.3077,
          -0.0338,  0.0162,  0.0325, -0.2131,  0.0020, -0.2902, -0.5249,
          -0.3772,  0.3810,  0.0407,  0.1514],
         [ 0.2024,  0.3994, -0.1126,  0.0716, -0.4287,  0.5770, -0.0514,
          -0.0499,  0.1846, -1.0184, -0.0325, -0.3114,  0.1922,  0.1153,
           0.4653,  0.0177, -0.5471, -0.0924, -0.2099, -0.0778,  0.3042,
          -0.0286,  0.0350,  0.0364, -0.2152, -0.0130, -0.2918, -0.5421,
          -0.3911,  0.3853,  0.0365,  0.1512],
         [ 0.2128,  0.3960, -0.1119,  0.0983, -0.4321,  0.5705, -0.0494,
          -0.0586,  0.1831, -1.0257, -0.0414, -0.3067,  0.1995,  0.1212,
           0.4685,  0.0236, -0.5363, -0.0899, -0.2099, -0.0781,  0.2943,
          -0.0218,  0.0442,  0.0363, -0.2186, -0.0142, -0.2892, -0.5480,
          -0.4029,  0.3830,  0.0248,  0.1533]]], grad_fn=<AddBackward0>) torch.Size([1, 5, 32])
  0%|                                                                                                                                                                                       | 0/496 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 124, in <module>
    main()
  File "main.py", line 120, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 84, in train
    loss.backward() # required_graph = True
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 141, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 50, in _make_grads
    raise RuntimeError("grad can be implicitly created only for scalar outputs")
RuntimeError: grad can be implicitly created only for scalar outputs