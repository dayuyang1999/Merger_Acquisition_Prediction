
CUDA availability: True
#### mat_b tensor([[[ 2.2080e-01, -3.0836e-01, -8.4820e-02,  7.5283e-02,  4.6516e-02,
          -1.3589e-01,  3.8876e-01,  3.2759e-01, -1.6121e-01, -1.0685e-02,
           2.5559e-01, -9.9148e-02, -7.2449e-02,  2.5138e-01, -1.3357e-01,
          -4.8747e-01,  4.7302e-02, -5.3528e-02, -3.9824e-01, -2.7580e-01,
          -1.2111e-01, -2.4189e-01, -8.2843e-03, -2.9470e-02,  2.7733e-01,
           1.7810e-01, -7.3426e-02, -6.0478e-01,  2.9275e-01,  4.9306e-01,
           1.5286e-02, -1.5823e-01],
         [ 2.3306e-01, -3.0803e-01, -8.5034e-02,  8.0835e-02,  4.0841e-02,
          -1.4452e-01,  3.9029e-01,  3.3326e-01, -1.5949e-01, -1.3887e-02,
           2.5555e-01, -1.0316e-01, -7.6640e-02,  2.5093e-01, -1.3378e-01,
          -4.8843e-01,  3.9911e-02, -5.4088e-02, -3.9354e-01, -2.8574e-01,
          -1.3024e-01, -2.4376e-01, -1.6232e-03, -2.5983e-02,  2.7891e-01,
           1.7415e-01, -6.7090e-02, -6.0073e-01,  3.0102e-01,  4.9403e-01,
           1.1083e-02, -1.6001e-01],
         [ 2.5168e-01, -3.0730e-01, -8.5253e-02,  8.9572e-02,  3.0348e-02,
          -1.5647e-01,  3.9312e-01,  3.4163e-01, -1.5533e-01, -1.9373e-02,
           2.5587e-01, -1.0940e-01, -8.3395e-02,  2.5048e-01, -1.3327e-01,
          -4.8973e-01,  2.8041e-02, -5.3755e-02, -3.8555e-01, -3.0233e-01,
          -1.4335e-01, -2.4780e-01,  8.0840e-03, -2.0028e-02,  2.8120e-01,
           1.6705e-01, -5.7391e-02, -5.9310e-01,  3.1475e-01,  4.9444e-01,
           5.6841e-03, -1.6292e-01],
         [ 2.6909e-01, -3.0648e-01, -8.5506e-02,  9.7758e-02,  2.0583e-02,
          -1.6767e-01,  3.9584e-01,  3.4951e-01, -1.5169e-01, -2.4542e-02,
           2.5612e-01, -1.1500e-01, -8.9692e-02,  2.5005e-01, -1.3289e-01,
          -4.9096e-01,  1.6898e-02, -5.3402e-02, -3.7788e-01, -3.1782e-01,
          -1.5576e-01, -2.5163e-01,  1.7366e-02, -1.4627e-02,  2.8319e-01,
           1.6042e-01, -4.8173e-02, -5.8599e-01,  3.2750e-01,  4.9487e-01,
           5.4327e-04, -1.6575e-01],
         [ 2.8004e-01, -3.0431e-01, -8.8964e-02,  1.0428e-01,  1.9193e-02,
          -1.8126e-01,  3.9921e-01,  3.5590e-01, -1.5349e-01, -3.2065e-02,
           2.5283e-01, -1.1595e-01, -9.9847e-02,  2.5585e-01, -1.2717e-01,
          -4.9415e-01,  4.6306e-03, -5.3959e-02, -3.6816e-01, -3.2823e-01,
          -1.6783e-01, -2.4119e-01,  2.4810e-02, -1.6153e-02,  2.7929e-01,
           1.5546e-01, -4.4131e-02, -5.8030e-01,  3.2849e-01,  4.9936e-01,
          -5.1659e-03, -1.7150e-01],
         [ 2.9608e-01, -3.0311e-01, -8.8824e-02,  1.1272e-01,  4.2234e-03,
          -1.8790e-01,  4.0324e-01,  3.6232e-01, -1.4522e-01, -3.8706e-02,
           2.5432e-01, -1.2182e-01, -1.0672e-01,  2.5585e-01, -1.2456e-01,
          -4.9471e-01, -7.5225e-03, -5.0026e-02, -3.5866e-01, -3.4709e-01,
          -1.7696e-01, -2.4886e-01,  3.1961e-02, -8.8746e-03,  2.8105e-01,
           1.4614e-01, -3.5530e-02, -5.6940e-01,  3.4404e-01,  4.9638e-01,
          -6.7367e-03, -1.7455e-01],
         [ 3.1911e-01, -3.0127e-01, -8.9074e-02,  1.2449e-01, -1.3873e-02,
          -1.9981e-01,  4.0854e-01,  3.7224e-01, -1.3680e-01, -4.7575e-02,
           2.5554e-01, -1.2917e-01, -1.1631e-01,  2.5578e-01, -1.2233e-01,
          -4.9594e-01, -2.4446e-02, -4.6144e-02, -3.4557e-01, -3.7169e-01,
          -1.9206e-01, -2.5744e-01,  4.3670e-02, -2.4991e-04,  2.8300e-01,
           1.3451e-01, -2.2872e-02, -5.5619e-01,  3.6387e-01,  4.9423e-01,
          -1.0992e-02, -1.7915e-01],
         [ 3.3869e-01, -2.9966e-01, -8.8843e-02,  1.3447e-01, -3.0255e-02,
          -2.0902e-01,  4.1325e-01,  3.8062e-01, -1.2922e-01, -5.4813e-02,
           2.5703e-01, -1.3533e-01, -1.2387e-01,  2.5494e-01, -1.2142e-01,
          -4.9663e-01, -3.8620e-02, -4.2272e-02, -3.3454e-01, -3.9284e-01,
          -2.0463e-01, -2.6676e-01,  5.3854e-02,  7.5396e-03,  2.8499e-01,
           1.2448e-01, -1.1476e-02, -5.4482e-01,  3.8167e-01,  4.9180e-01,
          -1.4218e-02, -1.8283e-01],
         [ 2.9728e-01, -3.0449e-01, -8.5579e-02,  1.1206e-01, -1.3770e-03,
          -1.8221e-01,  4.0204e-01,  3.6157e-01, -1.4087e-01, -3.5060e-02,
           2.5774e-01, -1.2457e-01, -1.0128e-01,  2.5003e-01, -1.2973e-01,
          -4.9239e-01, -3.4777e-03, -4.8931e-02, -3.6259e-01, -3.4784e-01,
          -1.7365e-01, -2.6172e-01,  3.1184e-02, -3.7365e-03,  2.8601e-01,
           1.4615e-01, -3.3016e-02, -5.6972e-01,  3.5184e-01,  4.9222e-01,
          -4.6032e-03, -1.7104e-01],
         [ 2.9728e-01, -3.0449e-01, -8.5579e-02,  1.1206e-01, -1.3770e-03,
          -1.8221e-01,  4.0204e-01,  3.6157e-01, -1.4087e-01, -3.5060e-02,
           2.5774e-01, -1.2457e-01, -1.0128e-01,  2.5003e-01, -1.2973e-01,
          -4.9239e-01, -3.4777e-03, -4.8931e-02, -3.6259e-01, -3.4784e-01,
          -1.7365e-01, -2.6172e-01,  3.1184e-02, -3.7365e-03,  2.8601e-01,
           1.4615e-01, -3.3016e-02, -5.6972e-01,  3.5184e-01,  4.9222e-01,
          -4.6032e-03, -1.7104e-01],
         [ 2.9728e-01, -3.0449e-01, -8.5579e-02,  1.1206e-01, -1.3770e-03,
          -1.8221e-01,  4.0204e-01,  3.6157e-01, -1.4087e-01, -3.5060e-02,
           2.5774e-01, -1.2457e-01, -1.0128e-01,  2.5003e-01, -1.2973e-01,
          -4.9239e-01, -3.4777e-03, -4.8931e-02, -3.6259e-01, -3.4784e-01,
          -1.7365e-01, -2.6172e-01,  3.1184e-02, -3.7365e-03,  2.8601e-01,
           1.4615e-01, -3.3016e-02, -5.6972e-01,  3.5184e-01,  4.9222e-01,
          -4.6032e-03, -1.7104e-01],
         [ 3.8208e-01, -2.9582e-01, -8.9733e-02,  1.5490e-01, -5.5061e-02,
          -2.3695e-01,  4.2145e-01,  4.0089e-01, -1.2318e-01, -6.8167e-02,
           2.5747e-01, -1.4576e-01, -1.3989e-01,  2.5346e-01, -1.2426e-01,
          -5.0000e-01, -6.7758e-02, -3.9508e-02, -3.1287e-01, -4.3110e-01,
          -2.3872e-01, -2.7866e-01,  8.0395e-02,  1.8908e-02,  2.8865e-01,
           1.0838e-01,  1.4730e-02, -5.2739e-01,  4.1335e-01,  4.9274e-01,
          -2.7523e-02, -1.9187e-01],
         [ 2.9728e-01, -3.0449e-01, -8.5579e-02,  1.1206e-01, -1.3770e-03,
          -1.8221e-01,  4.0204e-01,  3.6157e-01, -1.4087e-01, -3.5060e-02,
           2.5774e-01, -1.2457e-01, -1.0128e-01,  2.5003e-01, -1.2973e-01,
          -4.9239e-01, -3.4777e-03, -4.8931e-02, -3.6259e-01, -3.4784e-01,
          -1.7365e-01, -2.6172e-01,  3.1184e-02, -3.7365e-03,  2.8601e-01,
           1.4615e-01, -3.3016e-02, -5.6972e-01,  3.5184e-01,  4.9222e-01,
          -4.6032e-03, -1.7104e-01],
         [ 3.8050e-01, -2.9855e-01, -8.3030e-02,  1.5125e-01, -5.9278e-02,
          -2.2611e-01,  4.1799e-01,  3.9887e-01, -1.1916e-01, -5.8106e-02,
           2.6332e-01, -1.4891e-01, -1.2697e-01,  2.4199e-01, -1.3595e-01,
          -4.9509e-01, -5.6258e-02, -3.9856e-02, -3.2458e-01, -4.2590e-01,
          -2.3072e-01, -2.9948e-01,  7.8523e-02,  2.5820e-02,  2.9782e-01,
           1.1137e-01,  1.8572e-02, -5.3186e-01,  4.2273e-01,  4.8730e-01,
          -2.4788e-02, -1.8415e-01],
         [ 3.7262e-01, -2.9974e-01, -8.2091e-02,  1.4700e-01, -5.4738e-02,
          -2.2011e-01,  4.1565e-01,  3.9500e-01, -1.2025e-01, -5.4120e-02,
           2.6386e-01, -1.4759e-01, -1.2216e-01,  2.4061e-01, -1.3722e-01,
          -4.9396e-01, -4.9095e-02, -4.0982e-02, -3.3033e-01, -4.1782e-01,
          -2.2386e-01, -2.9974e-01,  7.3513e-02,  2.4575e-02,  2.9852e-01,
           1.1515e-01,  1.4182e-02, -5.3628e-01,  4.1802e-01,  4.8673e-01,
          -2.2441e-02, -1.8134e-01],
         [ 2.9728e-01, -3.0449e-01, -8.5579e-02,  1.1206e-01, -1.3770e-03,
          -1.8221e-01,  4.0204e-01,  3.6157e-01, -1.4087e-01, -3.5060e-02,
           2.5774e-01, -1.2457e-01, -1.0128e-01,  2.5003e-01, -1.2973e-01,
          -4.9239e-01, -3.4777e-03, -4.8931e-02, -3.6259e-01, -3.4784e-01,
          -1.7365e-01, -2.6172e-01,  3.1184e-02, -3.7365e-03,  2.8601e-01,
           1.4615e-01, -3.3016e-02, -5.6972e-01,  3.5184e-01,  4.9222e-01,
          -4.6032e-03, -1.7104e-01],
         [ 2.9728e-01, -3.0449e-01, -8.5579e-02,  1.1206e-01, -1.3770e-03,
          -1.8221e-01,  4.0204e-01,  3.6157e-01, -1.4087e-01, -3.5060e-02,
           2.5774e-01, -1.2457e-01, -1.0128e-01,  2.5003e-01, -1.2973e-01,
          -4.9239e-01, -3.4777e-03, -4.8931e-02, -3.6259e-01, -3.4784e-01,
          -1.7365e-01, -2.6172e-01,  3.1184e-02, -3.7365e-03,  2.8601e-01,
           1.4615e-01, -3.3016e-02, -5.6972e-01,  3.5184e-01,  4.9222e-01,
          -4.6032e-03, -1.7104e-01],
         [ 2.9728e-01, -3.0449e-01, -8.5579e-02,  1.1206e-01, -1.3770e-03,
          -1.8221e-01,  4.0204e-01,  3.6157e-01, -1.4087e-01, -3.5060e-02,
           2.5774e-01, -1.2457e-01, -1.0128e-01,  2.5003e-01, -1.2973e-01,
          -4.9239e-01, -3.4777e-03, -4.8931e-02, -3.6259e-01, -3.4784e-01,
          -1.7365e-01, -2.6172e-01,  3.1184e-02, -3.7365e-03,  2.8601e-01,
           1.4615e-01, -3.3016e-02, -5.6972e-01,  3.5184e-01,  4.9222e-01,
          -4.6032e-03, -1.7104e-01]]], grad_fn=<AddBackward0>) torch.Size([1, 18, 32])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<AddBackward0>) torch.Size([1, 13, 32])
  0%|â–                                                                                                                                        | 1/496 [00:00<00:54,  9.09it/s]
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 199, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 359, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1