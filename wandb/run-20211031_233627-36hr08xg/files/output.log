
CUDA availability: True
torch.Size([1, 13]) torch.Size([1, 13, 32])
-1
torch.Size([1, 130]) torch.Size([1, 130, 32])
-1
### event lambdas:  tensor([[0.3144, 0.1108, 0.1387, 0.1735, 0.1981, 0.2228, 0.1799, 0.1644, 0.0856,
         0.2005, 0.2005, 0.1197, 0.1533],
        [0.2451, 0.0844, 0.1060, 0.1332, 0.1525, 0.1720, 0.1382, 0.1261, 0.0650,
         0.1543, 0.1543, 0.0913, 0.1174],
        [0.0588, 0.0190, 0.0241, 0.0306, 0.0353, 0.0401, 0.0318, 0.0289, 0.0146,
         0.0358, 0.0358, 0.0206, 0.0268],
        [0.2756, 0.0959, 0.1203, 0.1508, 0.1724, 0.1942, 0.1564, 0.1428, 0.0740,
         0.1745, 0.1745, 0.1037, 0.1331],
        [0.4036, 0.1464, 0.1826, 0.2272, 0.2585, 0.2897, 0.2353, 0.2156, 0.1136,
         0.2615, 0.2615, 0.1580, 0.2014],
        [0.1090, 0.0359, 0.0453, 0.0574, 0.0661, 0.0749, 0.0596, 0.0542, 0.0275,
         0.0669, 0.0669, 0.0389, 0.0504],
        [0.2796, 0.0974, 0.1222, 0.1532, 0.1751, 0.1972, 0.1589, 0.1451, 0.0752,
         0.1772, 0.1772, 0.1053, 0.1352],
        [0.0174, 0.0056, 0.0071, 0.0090, 0.0104, 0.0118, 0.0093, 0.0085, 0.0042,
         0.0105, 0.0105, 0.0060, 0.0079],
        [0.0985, 0.0323, 0.0409, 0.0518, 0.0596, 0.0676, 0.0538, 0.0489, 0.0247,
         0.0604, 0.0604, 0.0350, 0.0454],
        [0.3542, 0.1264, 0.1580, 0.1972, 0.2248, 0.2525, 0.2044, 0.1870, 0.0979,
         0.2275, 0.2275, 0.1366, 0.1745],
        [0.0080, 0.0025, 0.0032, 0.0041, 0.0047, 0.0054, 0.0043, 0.0039, 0.0019,
         0.0048, 0.0048, 0.0028, 0.0036],
        [0.1042, 0.0342, 0.0433, 0.0548, 0.0631, 0.0716, 0.0570, 0.0518, 0.0262,
         0.0639, 0.0639, 0.0371, 0.0481],
        [0.0913, 0.0299, 0.0378, 0.0479, 0.0552, 0.0626, 0.0498, 0.0452, 0.0229,
         0.0559, 0.0559, 0.0324, 0.0420]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(477.3982, grad_fn=<NegBackward>) non event loss:  tensor([30894.2471], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(4.8405, grad_fn=<SumBackward0>)
torch.Size([1, 6]) torch.Size([1, 6, 32])
-1
torch.Size([1, 60]) torch.Size([1, 60, 32])
-1
### event lambdas:  tensor([[0.0046, 0.0056, 0.0114, 0.0054, 0.0046, 0.0036],
        [0.0018, 0.0022, 0.0045, 0.0021, 0.0018, 0.0014],
        [0.0014, 0.0018, 0.0036, 0.0017, 0.0014, 0.0011],
        [0.0014, 0.0017, 0.0034, 0.0016, 0.0014, 0.0011],
        [0.0014, 0.0017, 0.0034, 0.0016, 0.0014, 0.0011],
        [0.0014, 0.0017, 0.0034, 0.0016, 0.0014, 0.0011]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(221.9354, grad_fn=<NegBackward>) non event loss:  tensor([772.4583], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.9050, grad_fn=<SumBackward0>)
torch.Size([1, 5]) torch.Size([1, 5, 32])
-1
torch.Size([1, 50]) torch.Size([1, 50, 32])
-1
### event lambdas:  tensor([[1.7334e-02, 1.4904e-02, 9.6754e-03, 1.1984e-02, 1.6971e-02],
        [1.2983e-03, 1.1150e-03, 7.2212e-04, 8.9540e-04, 1.2709e-03],
        [1.2192e-03, 1.0471e-03, 6.7808e-04, 8.4081e-04, 1.1934e-03],
        [5.9299e-04, 5.0924e-04, 3.2976e-04, 4.0891e-04, 5.8046e-04],
        [8.8633e-05, 7.6113e-05, 4.9283e-05, 6.1115e-05, 8.6761e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(176.5157, grad_fn=<NegBackward>) non event loss:  tensor([106.6429], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.8989, grad_fn=<SumBackward0>)
torch.Size([1, 6]) torch.Size([1, 6, 32])
-1
torch.Size([1, 60]) torch.Size([1, 60, 32])
-1
### event lambdas:  tensor([[7.0178e-04, 1.0609e-03, 1.2160e-03, 4.7995e-04, 1.9028e-03, 1.1418e-03],
        [6.5481e-04, 9.8990e-04, 1.1346e-03, 4.4783e-04, 1.7755e-03, 1.0654e-03],
        [4.8708e-04, 7.3636e-04, 8.4405e-04, 3.3311e-04, 1.3209e-03, 7.9256e-04],
        [1.7778e-04, 2.6879e-04, 3.0811e-04, 1.2158e-04, 4.8225e-04, 2.8931e-04],
        [8.0591e-05, 1.2185e-04, 1.3967e-04, 5.5111e-05, 2.1863e-04, 1.3115e-04],
        [1.1531e-04, 1.7435e-04, 1.9985e-04, 7.8857e-05, 3.1282e-04, 1.8766e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(283.6877, grad_fn=<NegBackward>) non event loss:  tensor([175.8898], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0639, grad_fn=<SumBackward0>)
torch.Size([1, 3]) torch.Size([1, 3, 32])
-1
torch.Size([1, 30]) torch.Size([1, 30, 32])
-1
### event lambdas:  tensor([[7.3501e-05, 1.5156e-04, 6.6760e-05],
        [1.0022e-04, 2.0665e-04, 9.1031e-05],
        [1.0019e-04, 2.0658e-04, 9.0999e-05]], grad_fn=<SoftplusBackward>)
  1%|█▏                                                                                                                                              | 4/496 [00:00<01:38,  4.97it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 107, in train
    loss, pos_timing_loss, neg_timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 195, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 365, in forward
    z_vt_i = self.gnn_choice(features_i.squeeze(), edges_i.squeeze()) # (N_i_1, embedding_z)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 468, in forward
    x = self.convs[i](x, edge_index)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 508, in forward
    out = F.normalize(out, p=2)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 4270, in normalize
    denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 401, in norm
    return torch.norm(self, p, dim, keepdim, dtype=dtype)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/functional.py", line 1420, in norm
    return _VF.norm(input, p, _dim, keepdim=keepdim)  # type: ignore
KeyboardInterrupt