
CUDA availability: True
  1%|█▍                                                                                                                                              | 5/496 [00:00<01:33,  5.24it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 107, in train
    loss, pos_timing_loss, neg_timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 178, in forward
    mat_c = self.c_net(arr_c)[0] # (B, L2, embedding_c);
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 821, in forward
    result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 193, in format_stack
    def format_stack(f=None, limit=None):
KeyboardInterrupt
torch.Size([1, 9])
-1
torch.Size([1, 90])
-1
### event lambdas:  tensor([[4.0150, 3.1490, 3.1309, 3.5905, 3.5905, 3.5924, 3.6911, 4.0018, 3.5055],
        [3.6698, 2.8142, 2.7964, 3.2493, 3.2493, 3.2512, 3.3488, 3.6567, 3.1654],
        [3.9693, 3.1045, 3.0865, 3.5453, 3.5453, 3.5472, 3.6458, 3.9562, 3.4604],
        [2.5666, 1.7798, 1.7641, 2.1733, 2.1733, 2.1750, 2.2654, 2.5542, 2.0961],
        [2.9961, 2.1739, 2.1572, 2.5886, 2.5886, 2.5904, 2.6845, 2.9833, 2.5080],
        [3.8550, 2.9934, 2.9754, 3.4321, 3.4321, 3.4341, 3.5323, 3.8418, 3.3476],
        [3.5627, 2.7111, 2.6935, 3.1438, 3.1438, 3.1457, 3.2429, 3.5497, 3.0602],
        [2.5632, 1.7768, 1.7611, 2.1700, 2.1700, 2.1718, 2.2621, 2.5508, 2.0929],
        [4.0153, 3.1493, 3.1312, 3.5908, 3.5908, 3.5927, 3.6914, 4.0021, 3.5058]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(-88.4417, grad_fn=<NegBackward>) non event loss:  tensor([2556288.4197], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(3.9584, grad_fn=<SumBackward0>)
torch.Size([1, 6])
-1
torch.Size([1, 60])
-1
### event lambdas:  tensor([[1.1405, 2.3792, 1.3812, 1.5652, 2.4869, 1.9020],
        [0.7676, 1.8429, 0.9619, 1.1160, 1.9432, 1.4091],
        [1.2718, 2.5507, 1.5247, 1.7163, 2.6600, 2.0637],
        [0.4186, 1.2217, 0.5469, 0.6545, 1.3064, 0.8722],
        [1.3618, 2.6646, 1.6223, 1.8183, 2.7749, 2.1719],
        [1.3671, 2.6712, 1.6280, 1.8242, 2.7816, 2.1782]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(-15.3744, grad_fn=<NegBackward>) non event loss:  tensor([851879.0229], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.6161, grad_fn=<SumBackward0>)
torch.Size([1, 5])
-1
torch.Size([1, 50])
-1
### event lambdas:  tensor([[0.1929, 0.1192, 0.2097, 0.2521, 0.3211],
        [0.2103, 0.1304, 0.2285, 0.2741, 0.3483],
        [0.3054, 0.1927, 0.3306, 0.3930, 0.4922],
        [0.2027, 0.1255, 0.2203, 0.2645, 0.3364],
        [0.2178, 0.1352, 0.2365, 0.2836, 0.3598]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(35.7067, grad_fn=<NegBackward>) non event loss:  tensor([65142.9945], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1891, grad_fn=<SumBackward0>)
torch.Size([1, 12])
-1
torch.Size([1, 120])
-1
### event lambdas:  tensor([[0.0871, 0.2126, 0.0639, 0.0710, 0.0658, 0.0712, 0.0724, 0.0779, 0.0673,
         0.0421, 0.0569, 0.0919],
        [0.0357, 0.0905, 0.0260, 0.0290, 0.0268, 0.0291, 0.0296, 0.0319, 0.0275,
         0.0171, 0.0231, 0.0377],
        [0.0356, 0.0903, 0.0260, 0.0289, 0.0267, 0.0290, 0.0295, 0.0318, 0.0274,
         0.0170, 0.0231, 0.0376],
        [0.0417, 0.1051, 0.0304, 0.0339, 0.0313, 0.0339, 0.0345, 0.0372, 0.0321,
         0.0199, 0.0270, 0.0440],
        [0.0240, 0.0614, 0.0175, 0.0195, 0.0180, 0.0195, 0.0199, 0.0214, 0.0184,
         0.0114, 0.0155, 0.0254],
        [0.0349, 0.0884, 0.0254, 0.0283, 0.0262, 0.0284, 0.0289, 0.0311, 0.0268,
         0.0166, 0.0226, 0.0368],
        [0.0355, 0.0898, 0.0258, 0.0288, 0.0266, 0.0288, 0.0293, 0.0316, 0.0272,
         0.0169, 0.0229, 0.0374],
        [0.0349, 0.0884, 0.0254, 0.0283, 0.0262, 0.0284, 0.0289, 0.0311, 0.0268,
         0.0166, 0.0226, 0.0368],
        [0.0204, 0.0522, 0.0148, 0.0165, 0.0153, 0.0165, 0.0168, 0.0182, 0.0156,
         0.0097, 0.0131, 0.0215],
        [0.0216, 0.0553, 0.0157, 0.0175, 0.0162, 0.0175, 0.0179, 0.0193, 0.0166,
         0.0103, 0.0139, 0.0228],
        [0.0450, 0.1131, 0.0328, 0.0365, 0.0338, 0.0366, 0.0373, 0.0402, 0.0346,
         0.0215, 0.0292, 0.0475],
        [0.0221, 0.0565, 0.0161, 0.0179, 0.0165, 0.0179, 0.0183, 0.0197, 0.0169,
         0.0105, 0.0142, 0.0233]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(509.7975, grad_fn=<NegBackward>) non event loss:  tensor([19428.9366], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0258, grad_fn=<SumBackward0>)
torch.Size([1, 7])
-1
torch.Size([1, 70])
-1
### event lambdas:  tensor([[0.0175, 0.0097, 0.0159, 0.0110, 0.0087, 0.0189, 0.0077],
        [0.0128, 0.0071, 0.0116, 0.0080, 0.0064, 0.0138, 0.0056],
        [0.0141, 0.0078, 0.0128, 0.0089, 0.0070, 0.0152, 0.0062],
        [0.0133, 0.0074, 0.0121, 0.0084, 0.0067, 0.0144, 0.0059],
        [0.0126, 0.0070, 0.0114, 0.0079, 0.0063, 0.0136, 0.0055],
        [0.0088, 0.0049, 0.0080, 0.0055, 0.0044, 0.0095, 0.0039],
        [0.0132, 0.0073, 0.0120, 0.0083, 0.0066, 0.0142, 0.0058]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(231.0914, grad_fn=<NegBackward>) non event loss:  tensor([5486.2284], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0103, grad_fn=<SumBackward0>)