
CUDA availability: True
### event lambdas:  tensor([[ 1.0261, 20.3339,  9.7005,  3.0310, 11.4384]],
       grad_fn=<SoftplusBackward>)
### non event lambdas:  tensor([[1.2707e-02, 4.4907e+00, 2.8735e+01, 2.2396e-02, 2.6482e+00, 6.0046e-02,
         2.1600e+00, 2.0286e-02, 7.6773e+00, 1.7276e+01, 1.3745e+01, 4.3401e+00,
         6.1969e+00, 5.1517e+00, 1.3297e+00, 2.8876e+01, 2.5666e+00, 1.0284e-01,
         2.8833e+00, 2.4157e+01, 7.8772e+00, 1.8205e+01, 7.0263e-01, 7.1579e+00,
         2.4597e+01, 1.6361e+01, 2.2616e+01, 3.5870e+01, 4.6228e-01, 6.5863e+00,
         1.4970e+01, 1.9098e+01, 2.0448e+01, 7.1792e+00, 1.1592e-01, 6.1126e+01,
         1.9222e-02, 9.9109e-01, 6.9202e-01, 3.2333e-02, 1.2234e-01, 1.3782e+00,
         3.8719e+00, 3.3465e-01, 1.1581e+01, 2.0447e+01, 1.8646e+00, 6.2737e-02,
         1.3613e+01, 4.2343e-02]], grad_fn=<SoftplusBackward>)
##### event loss: -8.856152534484863 non event loss:  3657297.976501465 chocie_l: 2949.915283203125
### event lambdas:  tensor([[ 0.4305, 33.6166, 14.0625, 21.4407,  4.0522]],
       grad_fn=<SoftplusBackward>)
### non event lambdas:  tensor([[1.4532e+01, 1.6308e+01, 2.3914e+01, 3.2079e+01, 1.3658e+01, 1.0649e+01,
         3.5407e+01, 9.9935e+00, 6.6862e-03, 3.0675e+01, 7.1944e+00, 1.7230e+01,
         3.6188e+00, 1.0708e+01, 2.3326e+01, 9.4493e-01, 2.0253e+01, 2.2685e+00,
         1.4052e+01, 3.0124e+01, 1.5758e-02, 2.3455e+00, 4.1781e+01, 2.4377e+01,
         2.2215e-02, 1.4707e+01, 6.3000e+00, 3.5003e+00, 1.3718e+01, 1.7817e+01,
         3.7583e+01, 1.0618e+01, 1.6749e+00, 1.5893e+01, 2.5776e+00, 2.5880e-01,
         4.5315e+00, 5.6047e+00, 2.1559e-01, 2.6864e+01, 5.0535e-02, 2.8623e+01,
         8.8562e-02, 7.3176e-01, 4.5953e-02, 6.3822e+00, 8.3422e+00, 3.3507e+01,
         3.4563e+01, 9.8572e+00]], grad_fn=<SoftplusBackward>)
##### event loss: -9.780220985412598 non event loss:  3781565.9326171875 chocie_l: 2642.39599609375
### event lambdas:  tensor([[7.6859e-04, 1.7358e-04, 1.3335e-04, 9.3889e-05, 9.5812e-05, 2.6946e-04,
         1.4541e-04, 8.3244e-05, 1.5471e-04, 2.4232e-04, 1.8683e-04, 9.1561e-04,
         8.3626e-05, 9.1978e-04, 1.1987e-04, 1.6012e-04, 2.9019e-04, 1.1098e-04,
         2.1696e-04, 1.4343e-04, 8.2454e-04, 1.7541e-02, 7.0894e-04]],
       grad_fn=<SoftplusBackward>)
### non event lambdas:  tensor([[5.0229e-04, 3.9231e-04, 2.3043e-04, 3.9127e-03, 5.0883e-03, 1.0742e-04,
         1.7290e-04, 7.2752e-04, 2.0345e-04, 2.5667e-03, 1.2245e-04, 1.4195e-04,
         1.2012e-04, 2.7762e-04, 6.6429e-04, 2.5691e-03, 9.9572e-05, 2.5965e-04,
         6.0162e-04, 1.1903e-04, 1.5187e-03, 5.7404e-05, 3.2169e-04, 8.3232e-05,
         3.4486e-02, 7.9295e-05, 8.2161e-05, 7.7942e-04, 5.4483e-05, 1.9909e-04,
         1.0259e-03, 1.6961e-04, 3.0143e-04, 4.9887e-05, 9.0698e-05, 7.1506e-05,
         4.1954e-04, 5.9478e-04, 2.7259e-04, 7.2146e-05, 2.3071e-04, 2.7702e-04,
         5.5917e-04, 9.0614e-04, 8.8326e-05, 2.3534e-04, 9.8506e-05, 8.4414e-05,
         2.7968e-04, 5.5644e-04, 2.4777e-04, 9.7330e-05, 2.0557e-04, 1.2176e-04,
         4.2940e-04, 1.1965e-04, 3.9771e-04, 6.6371e-05, 1.0531e-04, 1.1977e-04,
         1.2573e-04, 1.5280e-03, 1.8695e-04, 7.9338e-05, 7.5731e-05, 7.7103e-05,
         5.1384e-04, 6.0291e-03, 1.3877e-04, 3.7469e-04, 1.2197e-04, 1.8621e-03,
         8.0196e-04, 6.8597e-04, 6.7665e-05, 3.9213e-04, 4.5344e-04, 1.4087e-03,
         1.3071e-04, 6.4533e-05, 1.1689e-03, 1.0606e-03, 1.9265e-04, 2.3678e-03,
         1.2973e-04, 7.1186e-04, 4.1020e-04, 1.2619e-03, 4.5167e-04, 1.6693e-04,
         2.7899e-04, 1.1089e-04, 1.3011e-04, 2.2294e-04, 2.6188e-04, 1.1424e-04,
         1.3760e-04, 1.6266e-03, 6.5100e-05, 8.6158e-05, 9.4368e-05, 1.6024e-04,
         1.1712e-04, 3.1255e-04, 4.6229e-04, 2.6717e-04, 8.4006e-05, 8.4409e-05,
         4.8364e-04, 1.5730e-04, 5.2878e-04, 2.5633e-04, 3.4352e-04, 1.3013e-04,
         3.2029e-04, 6.2462e-04, 1.1162e-04, 9.8281e-05, 1.5534e-04, 7.0421e-05,
         4.8327e-04, 2.2848e-04, 1.6006e-03, 1.5039e-04, 5.5530e-04, 1.1821e-03,
         4.9630e-04, 2.9184e-04, 2.4187e-04, 1.2935e-04, 2.0950e-04, 4.1293e-04,
         9.3241e-05, 1.0479e-04, 1.0253e-04, 4.6018e-04, 9.0847e-05, 1.6001e-04,
         1.1300e-04, 1.5172e-03, 1.6083e-03, 3.4005e-04, 8.8381e-05, 6.1959e-04,
         1.2797e-04, 1.2844e-04, 3.3057e-04, 2.7603e-04, 2.3978e-04, 5.9047e-05,
         1.6325e-04, 2.4587e-04, 1.4112e-04, 2.1906e-04, 3.5101e-04, 1.3255e-04,
         1.2518e-04, 6.9313e-05, 7.6304e-04, 1.0994e-04, 2.8634e-04, 9.9083e-05,
         2.7002e-04, 1.2567e-04, 2.8068e-04, 5.2596e-04, 1.2253e-04, 1.7740e-04,
         5.0529e-04, 4.7429e-05, 3.6319e-02, 1.9223e-04, 2.9688e-04, 3.9247e-04,
         1.1226e-01, 6.9490e-05, 6.9114e-05, 2.2872e-04, 5.6045e-04, 1.9876e-04,
         1.2444e-04, 1.3196e-04, 2.1369e-03, 2.4799e-04, 7.3191e-05, 1.1164e-04,
         1.2648e-04, 1.5616e-04, 1.5029e-04, 2.9667e-04, 9.8533e-05, 2.6178e-04,
         8.5309e-04, 1.1542e-04, 5.3242e-03, 8.4843e-05, 3.6632e-03, 1.5667e-04,
         1.3764e-04, 7.6780e-04, 4.6493e-04, 1.0491e-04, 1.4262e-04, 1.0562e-04,
         1.2335e-04, 9.5123e-05, 1.0096e-04, 8.2762e-04, 1.8182e-04, 1.4230e-04,
         5.8434e-04, 7.2806e-05, 1.1257e-04, 8.5627e-04, 5.8580e-05, 5.3298e-05,
         1.0686e-04, 5.7809e-04, 1.0606e-03, 1.2608e-04, 1.0496e-04, 8.1604e-05,
         1.1207e-04, 4.1058e-04, 1.0208e-04, 1.3026e-04, 2.5253e-04, 4.9329e-04,
         2.7087e-04, 1.4665e-04]], grad_fn=<SoftplusBackward>)
##### event loss: 189.48948669433594 non event loss:  2433.1418466567993 chocie_l: 2764.796142578125
### event lambdas:  tensor([[4.1688e-05, 2.0330e-05, 8.7421e-05, 4.9554e-06, 2.2391e-06, 1.7735e-08,
         1.4438e-05, 2.6796e-07, 1.7613e-07, 9.5732e-06, 6.5837e-08]],
       grad_fn=<SoftplusBackward>)
### non event lambdas:  tensor([[8.1318e-07, 1.4825e-05, 9.7444e-06, 2.2292e-06, 1.6143e-06, 1.1212e-05,
         4.4859e-06, 4.3510e-06, 4.3221e-06, 2.8089e-07, 4.4581e-06, 7.8304e-08,
         2.9079e-05, 8.6155e-08, 4.0555e-07, 9.2859e-07, 6.8354e-08, 5.7649e-06,
         8.3532e-09, 1.1468e-08, 5.3090e-06, 8.1370e-06, 3.4442e-07, 4.6474e-06,
         1.9387e-05, 2.1649e-07, 1.1251e-05, 4.1906e-06, 6.9127e-09, 8.3918e-06,
         9.7802e-06, 9.0823e-06, 2.9331e-05, 5.6068e-06, 2.9013e-06, 6.3510e-08,
         3.2863e-06, 3.7736e-07, 4.6367e-06, 1.6512e-05, 4.9189e-08, 5.6136e-07,
         7.0180e-07, 7.4233e-07, 7.1230e-08, 7.4793e-09, 1.7872e-05, 6.3816e-06,
         5.8116e-08, 6.1554e-06, 3.0192e-06, 2.8411e-07, 8.4945e-06, 1.6558e-06,
         1.0686e-06, 5.4169e-06, 1.0561e-05, 1.0408e-08, 3.3380e-05, 1.5519e-05,
         1.4605e-08, 1.4694e-06, 5.3513e-08, 3.0803e-05, 2.4851e-05, 2.4969e-08,
         1.8402e-07, 2.8654e-07, 1.1528e-05, 1.1658e-05, 8.5013e-06, 7.5253e-06,
         6.7543e-06, 2.3113e-05, 2.2406e-05, 1.5617e-06, 4.5437e-06, 9.6597e-06,
         8.7696e-07, 1.8153e-07, 2.4604e-07, 1.1136e-07, 9.6103e-08, 1.3931e-07,
         3.9260e-06, 1.5622e-05, 6.6218e-09, 2.1623e-09, 2.0261e-08, 3.9472e-07,
         1.2679e-05, 1.0065e-06, 2.6211e-07, 1.0346e-05, 2.8602e-07, 1.1226e-06,
         7.8432e-06, 1.5815e-07, 7.1434e-07, 9.7372e-06, 9.4077e-06, 2.0894e-06,
         2.0828e-05, 4.3574e-06, 2.9868e-08, 1.8957e-05, 4.2371e-06, 2.8950e-06,
         2.1162e-06, 1.4969e-05]], grad_fn=<SoftplusBackward>)
##### event loss: 143.22845458984375 non event loss:  2.5759566453052685 chocie_l: 146.08961486816406
### event lambdas:  tensor([[2.2230e-06, 9.8505e-07, 4.1059e-23, 2.7618e-11, 1.1158e-11, 1.3890e-10,
         2.5308e-07, 2.5943e-09]], grad_fn=<SoftplusBackward>)
### non event lambdas:  tensor([[1.9889e-07, 2.1359e-18, 1.2710e-06, 2.5412e-09, 1.4961e-19, 3.4796e-06,
         5.9039e-09, 4.9654e-09, 1.0313e-08, 3.9541e-08, 8.1447e-15, 1.8452e-16,
         2.5247e-10, 3.7797e-07, 4.6530e-18, 4.9969e-12, 8.6981e-18, 2.3004e-07,
         2.1067e-07, 4.3800e-06, 1.5287e-06, 3.1403e-07, 8.7584e-08, 1.1873e-11,
         2.0014e-19, 5.5160e-13, 2.1290e-07, 1.2574e-12, 1.2348e-07, 3.2266e-07,
         9.3219e-09, 3.9997e-10, 6.1297e-13, 9.0474e-08, 8.3659e-09, 3.4323e-07,
         1.7479e-10, 6.0441e-07, 7.7539e-08, 7.2175e-16, 1.9870e-20, 2.8971e-22,
         8.2855e-15, 8.3818e-11, 3.8842e-11, 2.5854e-11, 4.4734e-07, 5.2877e-10,
         6.9846e-08, 1.5395e-08, 3.5980e-11, 1.1243e-15, 3.9213e-12, 2.7750e-06,
         1.7971e-08, 1.2655e-12, 5.1630e-11, 2.1198e-06, 3.7753e-09, 7.2736e-11,
         1.1427e-14, 3.7847e-11, 5.9769e-09, 5.6880e-07, 2.0484e-08, 5.7167e-07,
         3.1903e-14, 5.4127e-07, 1.1535e-10, 1.3775e-08, 4.9371e-10, 1.4593e-15,
         2.1287e-07, 6.8892e-07, 1.8715e-08, 1.6573e-10, 3.9241e-14, 4.0516e-08,
         3.1976e-08, 1.8812e-09]], grad_fn=<SoftplusBackward>)
  1%|██▏                                                                                                                                                                                 | 6/496 [00:01<01:45,  4.65it/s]/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in SoftplusBackward. Traceback of forward call that caused the error:
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 107, in train
    loss, pos_timing_loss, neg_timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
##### event loss: 185.58245849609375 non event loss:  0.14136899918230483 chocie_l: 12.985905647277832
### event lambdas:  tensor([[1.1811e-08, 4.2920e-12, 9.5963e-14, 3.3156e-14, 9.2159e-11, 2.4115e-16,
         7.2612e-12]], grad_fn=<SoftplusBackward>)
### non event lambdas:  tensor([[2.3869e-07, 4.8908e-08, 3.7905e-11, 3.0897e-37, 7.4723e-17, 8.4231e-07,
         9.7154e-12, 2.3266e-11, 1.4984e-19, 2.8645e-13, 9.0609e-09, 1.1067e-07,
         2.2592e-07, 2.8951e-34, 4.2503e-08, 4.4133e-08, 1.5554e-07, 1.3441e-11,
         6.1706e-07, 1.5553e-32, 3.2118e-24, 4.3262e-07, 3.8794e-17, 1.1165e-33,
         3.8107e-12, 4.4333e-16, 4.1940e-08, 1.8493e-38, 4.6599e-16, 3.6796e-17,
         6.2829e-23, 9.9438e-13, 7.3693e-12, 1.5606e-16, 4.4211e-13, 2.2057e-06,
         1.0390e-12, 1.3292e-14, 8.0039e-11, 5.7025e-16, 2.6651e-12, 6.2522e-19,
         2.0654e-25, 2.1539e-18, 9.0995e-14, 3.7518e-26, 1.2706e-09, 4.3137e-09,
         1.2521e-24, 3.7398e-09, 2.5244e-08, 2.8778e-08, 8.8806e-08, 1.2356e-07,
         9.6083e-17, 1.9690e-10, 2.6110e-15, 3.3649e-13, 1.0580e-11, 2.6884e-11,
         1.1564e-06, 3.8325e-08, 1.2354e-09, 5.3601e-10, 3.8454e-17, 4.2418e-17,
         1.5354e-09, 6.6304e-17, 3.3588e-16, 1.5962e-15]],
       grad_fn=<SoftplusBackward>)
##### event loss: 190.157958984375 non event loss:  0.04315933133057115 chocie_l: 21.518016815185547
### event lambdas:  tensor([[3.0220e-10, 3.8536e-43, 1.4069e-22]], grad_fn=<SoftplusBackward>)
### non event lambdas:  tensor([[3.2873e-14, 8.4081e-40, 5.4015e-26, 6.5209e-18, 1.1649e-08, 3.8156e-15,
         1.4480e-23, 7.0249e-09, 1.5365e-39, 4.4533e-42, 1.6000e-23, 6.7711e-42,
         3.1597e-08, 7.9333e-18, 7.7516e-22, 4.1669e-37, 2.2027e-28, 8.8887e-17,
         1.3124e-08, 1.4925e-19, 5.0199e-09, 3.7305e-11, 5.3858e-28, 1.8658e-35,
         0.0000e+00, 1.6929e-35, 1.7732e-28, 3.3720e-08, 2.3896e-12, 8.4495e-30]],
       grad_fn=<SoftplusBackward>)
##### event loss: 169.89759826660156 non event loss:  0.0006688398815271057 chocie_l: 13.882834434509277
  File "/home/dalab5/Projects/MA_packed/model.py", line 183, in forward
    event_lambdas = self.timing_net(mat_b, mat_c, event_data) # (L3, )
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 290, in forward
    lambda_dt = self.f_lambda(rate)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 784, in forward
    return F.softplus(input, self.beta, self.threshold)
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378062065/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
  1%|██▏                                                                                                                                                                                 | 6/496 [00:01<01:59,  4.09it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 110, in train
    loss.backward() # required_graph = True
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Function 'SoftplusBackward' returned nan values in its 0th output.