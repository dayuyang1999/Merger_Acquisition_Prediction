
CUDA availability: True
  0%|â–Œ                                                                                                                                              | 2/496 [00:00<01:21,  6.05it/s]
Traceback (most recent call last):
  File "main.py", line 125, in <module>
    main()
  File "main.py", line 121, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 81, in train
    loss, timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 197, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 363, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
#### arr_b tensor([[[ 1.6044,  0.2160, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 1.8308,  0.2877, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 2.2026,  0.3684, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 2.5554,  0.4473, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 2.7832,  0.5706, -0.2877, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.1987,  0.5536, -0.2814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.7732,  0.5895, -0.2826, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 4.2905,  0.6102, -0.2557, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 5.3494,  0.8690, -0.2337, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 5.2525,  0.7692,  0.1711, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 5.0334,  0.7211,  0.2190, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 4.2967,  0.7003,  0.2996, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.3303, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.3933, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.3646, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464]]]) torch.Size([1, 22, 14])
#### mat_b tensor([[[ 5.5479e-03, -8.4583e-02,  1.5063e-02,  1.6237e-01, -9.8917e-02,
          -1.6313e-01, -9.6702e-02, -7.5706e-02,  2.1836e-01, -2.0772e-01,
           1.4086e-01,  1.0910e-02,  3.8476e-01, -2.9404e-01,  8.1622e-02,
           1.8883e-01, -1.5827e-01,  3.3820e-02,  7.3629e-02,  2.0789e-01,
          -1.0022e-01, -1.5991e-03,  6.1772e-02,  5.1510e-02, -1.0364e-01,
           1.9314e-02, -4.8957e-02,  2.8662e-01, -1.0493e-01, -5.3295e-02,
           1.9676e-02,  5.4025e-02],
         [ 1.9229e-02, -9.1188e-02, -6.4015e-03,  1.8642e-01, -1.1953e-01,
          -1.8370e-01, -9.5612e-02, -6.0781e-02,  2.1376e-01, -2.0119e-01,
           1.5821e-01,  1.4664e-02,  4.1938e-01, -3.0590e-01,  7.9998e-02,
           2.0197e-01, -1.4600e-01,  3.0550e-02,  6.8128e-02,  2.1235e-01,
          -1.0101e-01, -2.6811e-04,  5.5938e-02,  4.6631e-02, -8.4632e-02,
           2.1135e-02, -4.2079e-02,  3.0771e-01, -1.1263e-01, -5.9826e-02,
           3.3895e-02,  4.6980e-02],
         [ 4.0088e-02, -1.0749e-01, -4.1162e-02,  2.2193e-01, -1.5158e-01,
          -2.1265e-01, -9.6760e-02, -3.7720e-02,  2.1019e-01, -1.9110e-01,
           1.8821e-01,  1.8410e-02,  4.7129e-01, -3.2736e-01,  8.0478e-02,
           2.2538e-01, -1.2590e-01,  2.7115e-02,  6.3255e-02,  2.1713e-01,
          -1.0263e-01,  7.6445e-03,  4.7417e-02,  3.6247e-02, -5.6268e-02,
           2.6451e-02, -2.8649e-02,  3.4321e-01, -1.2496e-01, -7.0412e-02,
           5.4208e-02,  3.2616e-02],
         [ 5.9496e-02, -1.2432e-01, -7.5775e-02,  2.5253e-01, -1.8339e-01,
          -2.3824e-01, -9.8736e-02, -1.2177e-02,  2.0565e-01, -1.7986e-01,
           2.1587e-01,  2.0167e-02,  5.1811e-01, -3.5078e-01,  8.6270e-02,
           2.4455e-01, -1.0364e-01,  1.9424e-02,  5.6955e-02,  2.2118e-01,
          -1.0561e-01,  1.7504e-02,  3.3206e-02,  2.3426e-02, -3.4541e-02,
           3.4247e-02, -1.4269e-02,  3.8015e-01, -1.3839e-01, -8.3096e-02,
           7.5147e-02,  1.1736e-02],
         [ 8.9438e-02, -1.3362e-01, -7.7694e-02,  2.8558e-01, -1.9859e-01,
          -2.7937e-01, -1.0362e-01,  2.3454e-02,  2.0964e-01, -2.0242e-01,
           2.2523e-01,  2.5638e-02,  5.3416e-01, -3.6858e-01,  1.0003e-01,
           2.6764e-01, -9.8884e-02,  1.8758e-02,  3.5575e-02,  2.2224e-01,
          -9.6179e-02,  1.4703e-02,  2.4087e-02,  4.8644e-02, -2.4487e-02,
           5.9303e-02,  8.3349e-03,  3.8535e-01, -1.7776e-01, -1.0909e-01,
           9.4651e-02,  4.3879e-05],
         [ 1.0552e-01, -1.5933e-01, -1.2148e-01,  3.0963e-01, -2.4121e-01,
          -3.0385e-01, -1.0910e-01,  5.0295e-02,  2.0463e-01, -1.9494e-01,
           2.7174e-01,  2.0954e-02,  5.8644e-01, -4.0747e-01,  1.1648e-01,
           2.9370e-01, -6.7964e-02,  4.0951e-03,  2.7050e-02,  2.2131e-01,
          -1.1038e-01,  3.2214e-02,  1.3668e-03,  1.7576e-02, -3.1651e-03,
           6.0989e-02,  3.3480e-02,  4.3636e-01, -1.8330e-01, -1.2229e-01,
           1.1450e-01, -2.3801e-02],
         [ 1.3206e-01, -1.9241e-01, -1.8020e-01,  3.4832e-01, -2.9912e-01,
          -3.4249e-01, -1.1488e-01,  9.1274e-02,  1.9635e-01, -1.8320e-01,
           3.2985e-01,  1.7525e-02,  6.5946e-01, -4.5792e-01,  1.3622e-01,
           3.2752e-01, -2.6197e-02, -1.4371e-02,  1.3234e-02,  2.2239e-01,
          -1.2645e-01,  5.3409e-02, -2.8444e-02, -1.7354e-02,  2.5922e-02,
           6.8348e-02,  6.6061e-02,  5.0363e-01, -1.9675e-01, -1.4357e-01,
           1.4428e-01, -5.8384e-02],
         [ 1.5360e-01, -2.2397e-01, -2.3579e-01,  3.8171e-01, -3.5290e-01,
          -3.7538e-01, -1.2031e-01,  1.2671e-01,  1.8693e-01, -1.6988e-01,
           3.8421e-01,  1.4743e-02,  7.2664e-01, -5.0435e-01,  1.5260e-01,
           3.5669e-01,  1.3144e-02, -3.0707e-02,  1.7146e-03,  2.2266e-01,
          -1.4429e-01,  7.3665e-02, -5.4713e-02, -5.3170e-02,  5.0999e-02,
           7.2954e-02,  9.5094e-02,  5.6746e-01, -2.0384e-01, -1.6122e-01,
           1.6920e-01, -8.9530e-02],
         [ 9.0230e-02, -1.6442e-01, -1.4454e-01,  2.9789e-01, -2.5167e-01,
          -2.8405e-01, -1.0633e-01,  3.5983e-02,  1.9541e-01, -1.6705e-01,
           2.8477e-01,  1.6794e-02,  6.0312e-01, -4.1075e-01,  1.0916e-01,
           2.8439e-01, -5.4617e-02, -1.4931e-03,  4.0162e-02,  2.2167e-01,
          -1.2587e-01,  4.2530e-02, -1.0378e-03, -1.7690e-02, -1.4283e-03,
           4.2905e-02,  2.5142e-02,  4.5973e-01, -1.5285e-01, -1.0834e-01,
           1.0906e-01, -2.8240e-02],
         [ 9.0230e-02, -1.6442e-01, -1.4454e-01,  2.9789e-01, -2.5167e-01,
          -2.8405e-01, -1.0633e-01,  3.5983e-02,  1.9541e-01, -1.6705e-01,
           2.8477e-01,  1.6794e-02,  6.0312e-01, -4.1075e-01,  1.0916e-01,
           2.8439e-01, -5.4617e-02, -1.4931e-03,  4.0162e-02,  2.2167e-01,
          -1.2587e-01,  4.2530e-02, -1.0378e-03, -1.7690e-02, -1.4283e-03,
           4.2905e-02,  2.5142e-02,  4.5973e-01, -1.5285e-01, -1.0834e-01,
           1.0906e-01, -2.8240e-02],
         [ 9.0230e-02, -1.6442e-01, -1.4454e-01,  2.9789e-01, -2.5167e-01,
          -2.8405e-01, -1.0633e-01,  3.5983e-02,  1.9541e-01, -1.6705e-01,
           2.8477e-01,  1.6794e-02,  6.0312e-01, -4.1075e-01,  1.0916e-01,
           2.8439e-01, -5.4617e-02, -1.4931e-03,  4.0162e-02,  2.2167e-01,
          -1.2587e-01,  4.2530e-02, -1.0378e-03, -1.7690e-02, -1.4283e-03,
           4.2905e-02,  2.5142e-02,  4.5973e-01, -1.5285e-01, -1.0834e-01,
           1.0906e-01, -2.8240e-02],
         [ 2.0245e-01, -2.6458e-01, -3.4300e-01,  4.5603e-01, -4.5956e-01,
          -4.4534e-01, -1.3560e-01,  1.9841e-01,  1.5003e-01, -1.4707e-01,
           4.7802e-01,  2.0447e-02,  8.7446e-01, -5.9696e-01,  1.6591e-01,
           4.1891e-01,  8.4574e-02, -5.9988e-02, -2.6294e-02,  2.2972e-01,
          -1.7003e-01,  1.1383e-01, -1.1487e-01, -1.0469e-01,  8.6765e-02,
           9.3222e-02,  1.4271e-01,  6.9314e-01, -2.5189e-01, -2.0988e-01,
           2.1625e-01, -1.5959e-01],
         [ 9.0230e-02, -1.6442e-01, -1.4454e-01,  2.9789e-01, -2.5167e-01,
          -2.8405e-01, -1.0633e-01,  3.5983e-02,  1.9541e-01, -1.6705e-01,
           2.8477e-01,  1.6794e-02,  6.0312e-01, -4.1075e-01,  1.0916e-01,
           2.8439e-01, -5.4617e-02, -1.4931e-03,  4.0162e-02,  2.2167e-01,
          -1.2587e-01,  4.2530e-02, -1.0378e-03, -1.7690e-02, -1.4283e-03,
           4.2905e-02,  2.5142e-02,  4.5973e-01, -1.5285e-01, -1.0834e-01,
           1.0906e-01, -2.8240e-02],
         [ 1.7045e-01, -2.7891e-01, -3.7546e-01,  4.3164e-01, -4.6407e-01,
          -4.1145e-01, -1.2275e-01,  1.7675e-01,  1.4478e-01, -9.2424e-02,
           4.8138e-01,  1.4582e-02,  8.8696e-01, -5.8414e-01,  1.5122e-01,
           3.7593e-01,  1.0608e-01, -6.4627e-02, -2.1282e-03,  2.3969e-01,
          -1.9756e-01,  1.1305e-01, -1.0295e-01, -1.4969e-01,  9.0860e-02,
           5.6867e-02,  1.2169e-01,  7.1752e-01, -1.8367e-01, -1.8293e-01,
           2.0919e-01, -1.5768e-01],
         [ 1.5934e-01, -2.7097e-01, -3.5780e-01,  4.1677e-01, -4.4396e-01,
          -3.9370e-01, -1.1678e-01,  1.5928e-01,  1.4904e-01, -8.6558e-02,
           4.6173e-01,  1.4221e-02,  8.5925e-01, -5.6219e-01,  1.4467e-01,
           3.5913e-01,  9.4210e-02, -5.8225e-02,  6.6019e-03,  2.3799e-01,
          -1.9419e-01,  1.0677e-01, -8.8758e-02, -1.4563e-01,  8.4395e-02,
           5.1936e-02,  1.0780e-01,  6.9490e-01, -1.6558e-01, -1.6872e-01,
           1.9877e-01, -1.4530e-01],
         [ 9.0230e-02, -1.6442e-01, -1.4454e-01,  2.9789e-01, -2.5167e-01,
          -2.8405e-01, -1.0633e-01,  3.5983e-02,  1.9541e-01, -1.6705e-01,
           2.8477e-01,  1.6794e-02,  6.0312e-01, -4.1075e-01,  1.0916e-01,
           2.8439e-01, -5.4617e-02, -1.4931e-03,  4.0162e-02,  2.2167e-01,
          -1.2587e-01,  4.2530e-02, -1.0378e-03, -1.7690e-02, -1.4283e-03,
           4.2905e-02,  2.5142e-02,  4.5973e-01, -1.5285e-01, -1.0834e-01,
           1.0906e-01, -2.8240e-02],
         [ 9.0230e-02, -1.6442e-01, -1.4454e-01,  2.9789e-01, -2.5167e-01,
          -2.8405e-01, -1.0633e-01,  3.5983e-02,  1.9541e-01, -1.6705e-01,
           2.8477e-01,  1.6794e-02,  6.0312e-01, -4.1075e-01,  1.0916e-01,
           2.8439e-01, -5.4617e-02, -1.4931e-03,  4.0162e-02,  2.2167e-01,
          -1.2587e-01,  4.2530e-02, -1.0378e-03, -1.7690e-02, -1.4283e-03,
           4.2905e-02,  2.5142e-02,  4.5973e-01, -1.5285e-01, -1.0834e-01,
           1.0906e-01, -2.8240e-02],
         [ 9.0230e-02, -1.6442e-01, -1.4454e-01,  2.9789e-01, -2.5167e-01,
          -2.8405e-01, -1.0633e-01,  3.5983e-02,  1.9541e-01, -1.6705e-01,
           2.8477e-01,  1.6794e-02,  6.0312e-01, -4.1075e-01,  1.0916e-01,
           2.8439e-01, -5.4617e-02, -1.4931e-03,  4.0162e-02,  2.2167e-01,
          -1.2587e-01,  4.2530e-02, -1.0378e-03, -1.7690e-02, -1.4283e-03,
           4.2905e-02,  2.5142e-02,  4.5973e-01, -1.5285e-01, -1.0834e-01,
           1.0906e-01, -2.8240e-02],
         [ 1.2395e-01, -2.2547e-01, -2.9030e-01,  3.6670e-01, -3.7193e-01,
          -3.3694e-01, -1.0620e-01,  1.0156e-01,  1.5377e-01, -8.4395e-02,
           3.8614e-01,  2.0054e-02,  7.7223e-01, -4.9095e-01,  1.1148e-01,
           3.0912e-01,  4.5187e-02, -3.5178e-02,  2.9104e-02,  2.3778e-01,
          -1.7436e-01,  8.3089e-02, -5.0770e-02, -1.1010e-01,  4.9843e-02,
           4.1343e-02,  5.8279e-02,  6.1304e-01, -1.3824e-01, -1.3536e-01,
           1.6026e-01, -1.0617e-01],
         [ 8.1102e-02, -1.7185e-01, -1.4672e-01,  2.8474e-01, -2.5324e-01,
          -2.7519e-01, -1.1257e-01,  2.7638e-02,  1.9953e-01, -1.7533e-01,
           3.0113e-01,  1.0151e-02,  5.9974e-01, -4.2046e-01,  1.1880e-01,
           2.9155e-01, -5.3006e-02, -5.0907e-03,  4.3091e-02,  2.1458e-01,
          -1.3492e-01,  4.9806e-02, -4.3089e-03, -3.5168e-02, -1.5396e-03,
           3.2453e-02,  3.3465e-02,  4.6583e-01, -1.4137e-01, -1.0304e-01,
           1.0297e-01, -2.3363e-02],
         [ 8.4648e-02, -1.6836e-01, -1.4585e-01,  2.9036e-01, -2.5283e-01,
          -2.7829e-01, -1.0957e-01,  3.1244e-02,  1.9816e-01, -1.7172e-01,
           2.9405e-01,  1.2376e-02,  6.0084e-01, -4.1621e-01,  1.1462e-01,
           2.8889e-01, -5.3905e-02, -3.8499e-03,  4.2122e-02,  2.1771e-01,
          -1.3066e-01,  4.6618e-02, -3.4152e-03, -2.7849e-02, -1.0306e-03,
           3.6409e-02,  2.9676e-02,  4.6349e-01, -1.4591e-01, -1.0480e-01,
           1.0569e-01, -2.5359e-02],
         [ 8.2862e-02, -1.6962e-01, -1.4627e-01,  2.8795e-01, -2.5321e-01,
          -2.7645e-01, -1.1061e-01,  2.9728e-02,  1.9904e-01, -1.7322e-01,
           2.9702e-01,  1.0963e-02,  6.0012e-01, -4.1795e-01,  1.1637e-01,
           2.9033e-01, -5.3677e-02, -4.6040e-03,  4.2749e-02,  2.1644e-01,
          -1.3219e-01,  4.7927e-02, -4.1760e-03, -3.1100e-02, -9.0350e-04,
           3.4330e-02,  3.1127e-02,  4.6469e-01, -1.4369e-01, -1.0367e-01,
           1.0461e-01, -2.4437e-02]]], grad_fn=<AddBackward0>) torch.Size([1, 22, 32])
#### grad: None
#### arr_b tensor([[[ 1.5976,  0.2143, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 1.8232,  0.2856, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 2.1937,  0.3661, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 2.5452,  0.4447, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 2.7722,  0.5675, -0.2877, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.1862,  0.5506, -0.2814, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.7586,  0.5863, -0.2826, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 4.2741,  0.6070, -0.2558, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 5.3292,  0.8649, -0.2338, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 5.2327,  0.7654,  0.1694, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 5.0143,  0.7175,  0.2172, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 4.2803,  0.6967,  0.2975, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.3281, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.3909, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462]]]) torch.Size([1, 21, 14])
#### mat_b tensor([[[ 1.3904,  1.2511,  1.4567,  1.6358,  1.6066,  1.3074,  1.3041,
           1.7164, -0.9735,  1.3978,  1.8295,  1.4130,  2.1626,  0.9712,
           1.3077,  1.6767,  1.5021, -1.3155,  1.5340, -0.9878,  1.5920,
           1.5249,  1.1496, -1.3422, -1.3060,  1.2374,  1.4291, -1.1689,
           1.0679,  1.2085,  1.0023, -1.8313],
         [ 1.4813,  1.3409,  1.5366,  1.7525,  1.7067,  1.3861,  1.3969,
           1.8381, -1.0568,  1.5085,  1.9477,  1.5205,  2.3163,  1.0503,
           1.3833,  1.7897,  1.6093, -1.4139,  1.6260, -1.0602,  1.6971,
           1.6245,  1.2124, -1.4376, -1.3869,  1.3139,  1.5225, -1.2419,
           1.1396,  1.2913,  1.0804, -1.9540],
         [ 1.6274,  1.4854,  1.6642,  1.9401,  1.8624,  1.5148,  1.5456,
           2.0287, -1.1915,  1.6853,  2.1411,  1.6915,  2.5608,  1.1742,
           1.5065,  1.9717,  1.7814, -1.5693,  1.7750, -1.1760,  1.8640,
           1.7830,  1.3136, -1.5920, -1.5184,  1.4368,  1.6719, -1.3576,
           1.2567,  1.4255,  1.2069, -2.1486],
         [ 1.7743,  1.6316,  1.7922,  2.1222,  2.0100,  1.6470,  1.6924,
           2.2157, -1.3273,  1.8567,  2.3287,  1.8685,  2.8070,  1.2970,
           1.6357,  2.1500,  1.9581, -1.7220,  1.9277, -1.2984,  2.0407,
           1.9431,  1.4170, -1.7459, -1.6530,  1.5704,  1.8224, -1.4742,
           1.3725,  1.5568,  1.3411, -2.3448],
         [ 1.9012,  1.7363,  1.9136,  2.2703,  2.1533,  1.7344,  1.8015,
           2.3921, -1.4290,  1.9870,  2.4798,  2.0170,  2.9881,  1.3926,
           1.7379,  2.2990,  2.0887, -1.8604,  2.0239, -1.4054,  2.1855,
           2.0681,  1.4886, -1.8568, -1.7680,  1.6894,  1.9556, -1.5777,
           1.4416,  1.6550,  1.4387, -2.5121],
         [ 2.0624,  1.8913,  2.0556,  2.4648,  2.3048,  1.8775,  1.9627,
           2.5926, -1.5821,  2.1614,  2.6905,  2.2020,  3.2444,  1.5276,
           1.8753,  2.4899,  2.2824, -2.0230,  2.1832, -1.5370,  2.3636,
           2.2347,  1.6099, -2.0203, -1.9102,  1.8343,  2.1311, -1.7092,
           1.5690,  1.8062,  1.5885, -2.7084],
         [ 2.2980,  2.1185,  2.2657,  2.7534,  2.5322,  2.0834,  2.2012,
           2.8895, -1.8054,  2.4200,  2.9954,  2.4731,  3.6230,  1.7285,
           2.0780,  2.7751,  2.5610, -2.2664,  2.4177, -1.7330,  2.6288,
           2.4828,  1.7872, -2.2605, -2.1219,  2.0490,  2.3817, -1.9020,
           1.7527,  2.0236,  1.8052, -3.0011],
         [ 2.5084,  2.3218,  2.4505,  3.0119,  2.7309,  2.2694,  2.4151,
           3.1517, -2.0069,  2.6500,  3.2669,  2.7131,  3.9615,  1.9085,
           2.2591,  3.0289,  2.8119, -2.4810,  2.6295, -1.9082,  2.8637,
           2.7053,  1.9495, -2.4776, -2.3101,  2.2387,  2.6045, -2.0724,
           1.9201,  2.2195,  2.0014, -3.2596],
         [ 2.0480,  1.8963,  2.0363,  2.4580,  2.2742,  1.8874,  1.9699,
           2.5607, -1.5867,  2.1582,  2.6841,  2.1843,  3.2488,  1.5305,
           1.8722,  2.4820,  2.2829, -2.0052,  2.2012, -1.5262,  2.3498,
           2.2328,  1.6229, -2.0262, -1.8997,  1.8197,  2.1140, -1.6979,
           1.5872,  1.8096,  1.5934, -2.6858],
         [ 2.0480,  1.8963,  2.0363,  2.4580,  2.2742,  1.8874,  1.9699,
           2.5607, -1.5867,  2.1582,  2.6841,  2.1843,  3.2488,  1.5305,
           1.8722,  2.4820,  2.2829, -2.0052,  2.2012, -1.5262,  2.3498,
           2.2328,  1.6229, -2.0262, -1.8997,  1.8197,  2.1140, -1.6979,
           1.5872,  1.8096,  1.5934, -2.6858],
         [ 2.0480,  1.8963,  2.0363,  2.4580,  2.2742,  1.8874,  1.9699,
           2.5607, -1.5867,  2.1582,  2.6841,  2.1843,  3.2488,  1.5305,
           1.8722,  2.4820,  2.2829, -2.0052,  2.2012, -1.5262,  2.3498,
           2.2328,  1.6229, -2.0262, -1.8997,  1.8197,  2.1140, -1.6979,
           1.5872,  1.8096,  1.5934, -2.6858],
         [ 3.0027,  2.7738,  2.8651,  3.6047,  3.1931,  2.6788,  2.8872,
           3.7613, -2.4698,  3.1644,  3.8556,  3.2572,  4.7053,  2.3113,
           2.6579,  3.5933,  3.3786, -2.9577,  3.0968, -2.3176,  3.3984,
           3.2003,  2.3211, -2.9639, -2.7325,  2.6708,  3.1038, -2.4575,
           2.2807,  2.6602,  2.4494, -3.8556],
         [ 2.0480,  1.8963,  2.0363,  2.4580,  2.2742,  1.8874,  1.9699,
           2.5607, -1.5867,  2.1582,  2.6841,  2.1843,  3.2488,  1.5305,
           1.8722,  2.4820,  2.2829, -2.0052,  2.2012, -1.5262,  2.3498,
           2.2328,  1.6229, -2.0262, -1.8997,  1.8197,  2.1140, -1.6979,
           1.5872,  1.8096,  1.5934, -2.6858],
         [ 2.9244,  2.7310,  2.7769,  3.5275,  3.0819,  2.6503,  2.8455,
           3.6327, -2.4309,  3.0981,  3.7672,  3.1599,  4.6238,  2.2710,
           2.6031,  3.5104,  3.3167, -2.8639,  3.0783, -2.2546,  3.3110,
           3.1383,  2.3091, -2.9208, -2.6624,  2.5928,  3.0107, -2.3908,
           2.2725,  2.6179,  2.4117, -3.7457],
         [ 2.8205,  2.6400,  2.6870,  3.4048,  2.9810,  2.5682,  2.7505,
           3.5006, -2.3380,  2.9924,  3.6429,  3.0442,  4.4718,  2.1893,
           2.5197,  3.3923,  3.2003, -2.7612,  2.9874, -2.1687,  3.1988,
           3.0355,  2.2362, -2.8219, -2.5728,  2.5011,  2.9030, -2.3094,
           2.2019,  2.5283,  2.3211, -3.6192],
         [ 2.0480,  1.8963,  2.0363,  2.4580,  2.2742,  1.8874,  1.9699,
           2.5607, -1.5867,  2.1582,  2.6841,  2.1843,  3.2488,  1.5305,
           1.8722,  2.4820,  2.2829, -2.0052,  2.2012, -1.5262,  2.3498,
           2.2328,  1.6229, -2.0262, -1.8997,  1.8197,  2.1140, -1.6979,
           1.5872,  1.8096,  1.5934, -2.6858],
         [ 2.0480,  1.8963,  2.0363,  2.4580,  2.2742,  1.8874,  1.9699,
           2.5607, -1.5867,  2.1582,  2.6841,  2.1843,  3.2488,  1.5305,
           1.8722,  2.4820,  2.2829, -2.0052,  2.2012, -1.5262,  2.3498,
           2.2328,  1.6229, -2.0262, -1.8997,  1.8197,  2.1140, -1.6979,
           1.5872,  1.8096,  1.5934, -2.6858],
         [ 2.0480,  1.8963,  2.0363,  2.4580,  2.2742,  1.8874,  1.9699,
           2.5607, -1.5867,  2.1582,  2.6841,  2.1843,  3.2488,  1.5305,
           1.8722,  2.4820,  2.2829, -2.0052,  2.2012, -1.5262,  2.3498,
           2.2328,  1.6229, -2.0262, -1.8997,  1.8197,  2.1140, -1.6979,
           1.5872,  1.8096,  1.5934, -2.6858],
         [ 2.4996,  2.3562,  2.4139,  3.0202,  2.6857,  2.3032,  2.4522,
           3.1006, -2.0448,  2.6680,  3.2495,  2.6907,  3.9939,  1.9337,
           2.2549,  3.0245,  2.8283, -2.4475,  2.6980, -1.9036,  2.8579,
           2.7079,  1.9963, -2.5024, -2.2945,  2.2233,  2.5648, -2.0621,
           1.9665,  2.2437,  2.0302, -3.2382],
         [ 2.0171,  1.8697,  2.0130,  2.4254,  2.2378,  1.8706,  1.9445,
           2.5221, -1.5594,  2.1261,  2.6630,  2.1523,  3.2120,  1.5080,
           1.8565,  2.4535,  2.2576, -1.9802,  2.1753, -1.5008,  2.3146,
           2.2126,  1.6070, -2.0068, -1.8789,  1.7920,  2.0939, -1.6736,
           1.5775,  1.7882,  1.5721, -2.6444],
         [ 2.0298,  1.8807,  2.0226,  2.4388,  2.2528,  1.8775,  1.9549,
           2.5380, -1.5707,  2.1393,  2.6717,  2.1655,  3.2272,  1.5173,
           1.8630,  2.4652,  2.2681, -1.9905,  2.1860, -1.5113,  2.3291,
           2.2209,  1.6135, -2.0148, -1.8875,  1.8034,  2.1022, -1.6837,
           1.5815,  1.7971,  1.5809, -2.6614]]], grad_fn=<AddBackward0>) torch.Size([1, 21, 32])
#### grad: tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
#### arr_b tensor([[[ 1.7933,  0.2759, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 2.1590,  0.3554, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 2.5059,  0.4329, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 2.7299,  0.5542, -0.2899, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1385,  0.5375, -0.2836, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.7034,  0.5727, -0.2848, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 4.2122,  0.5932, -0.2584, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.4683, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.4683, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.4683, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 5.2534,  0.8476, -0.2367, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.4683, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 5.1582,  0.7494,  0.1613, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 4.9427,  0.7021,  0.2085, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.4683, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.4683, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.4683, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 4.2183,  0.6817,  0.2877, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.3179, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.3799, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476],
         [ 3.1673,  0.3516, -0.0870, -0.3476, -0.3476, -0.3476, -0.3476,
          -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476, -0.3476]]]) torch.Size([1, 21, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<AddBackward0>) torch.Size([1, 21, 32])