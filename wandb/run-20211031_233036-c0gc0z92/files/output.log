
CUDA availability: True
-1
-1
### event lambdas:  tensor([[0.1081, 0.0274, 0.0768, 0.0543, 0.0393, 0.1082],
        [0.1142, 0.0290, 0.0812, 0.0574, 0.0416, 0.1144],
        [0.0947, 0.0239, 0.0672, 0.0474, 0.0343, 0.0949],
        [0.1160, 0.0295, 0.0825, 0.0584, 0.0423, 0.1161],
        [0.0790, 0.0198, 0.0559, 0.0394, 0.0285, 0.0791],
        [0.0731, 0.0183, 0.0517, 0.0364, 0.0263, 0.0732]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(104.9048, grad_fn=<NegBackward>) non event loss:  tensor([16753.5201], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(3.7360, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0144, 0.0158, 0.0177, 0.0158, 0.0227, 0.0111, 0.0089, 0.0093, 0.0163,
         0.0149, 0.0075, 0.0091, 0.0162, 0.0084, 0.0149],
        [0.0143, 0.0157, 0.0175, 0.0157, 0.0226, 0.0110, 0.0088, 0.0092, 0.0162,
         0.0148, 0.0075, 0.0091, 0.0161, 0.0083, 0.0148],
        [0.0145, 0.0160, 0.0178, 0.0160, 0.0230, 0.0112, 0.0090, 0.0094, 0.0165,
         0.0150, 0.0076, 0.0092, 0.0164, 0.0085, 0.0150],
        [0.0237, 0.0261, 0.0291, 0.0261, 0.0374, 0.0183, 0.0147, 0.0153, 0.0269,
         0.0246, 0.0124, 0.0151, 0.0268, 0.0139, 0.0246],
        [0.0141, 0.0155, 0.0173, 0.0155, 0.0222, 0.0108, 0.0087, 0.0091, 0.0159,
         0.0146, 0.0073, 0.0089, 0.0159, 0.0082, 0.0146],
        [0.0312, 0.0344, 0.0383, 0.0344, 0.0491, 0.0241, 0.0194, 0.0202, 0.0354,
         0.0323, 0.0164, 0.0199, 0.0353, 0.0183, 0.0323],
        [0.0100, 0.0111, 0.0124, 0.0111, 0.0159, 0.0077, 0.0062, 0.0065, 0.0114,
         0.0104, 0.0052, 0.0064, 0.0114, 0.0059, 0.0104],
        [0.0161, 0.0177, 0.0198, 0.0177, 0.0254, 0.0124, 0.0100, 0.0104, 0.0182,
         0.0167, 0.0084, 0.0102, 0.0182, 0.0094, 0.0167],
        [0.0191, 0.0210, 0.0234, 0.0210, 0.0301, 0.0147, 0.0118, 0.0123, 0.0216,
         0.0198, 0.0100, 0.0121, 0.0216, 0.0111, 0.0198],
        [0.0187, 0.0206, 0.0230, 0.0206, 0.0296, 0.0144, 0.0116, 0.0121, 0.0212,
         0.0194, 0.0098, 0.0119, 0.0212, 0.0109, 0.0194],
        [0.0180, 0.0199, 0.0222, 0.0199, 0.0285, 0.0139, 0.0112, 0.0116, 0.0205,
         0.0187, 0.0094, 0.0115, 0.0204, 0.0105, 0.0187],
        [0.0155, 0.0170, 0.0190, 0.0170, 0.0245, 0.0119, 0.0096, 0.0100, 0.0175,
         0.0160, 0.0081, 0.0098, 0.0175, 0.0090, 0.0160],
        [0.0129, 0.0142, 0.0158, 0.0142, 0.0204, 0.0099, 0.0080, 0.0083, 0.0146,
         0.0133, 0.0067, 0.0082, 0.0146, 0.0075, 0.0133],
        [0.0137, 0.0151, 0.0169, 0.0151, 0.0217, 0.0106, 0.0085, 0.0089, 0.0156,
         0.0142, 0.0072, 0.0087, 0.0155, 0.0080, 0.0142],
        [0.0147, 0.0162, 0.0180, 0.0162, 0.0232, 0.0113, 0.0091, 0.0095, 0.0167,
         0.0152, 0.0077, 0.0093, 0.0166, 0.0086, 0.0152]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(953.2573, grad_fn=<NegBackward>) non event loss:  tensor([13940.7912], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(4.9486, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0024, 0.0024, 0.0012, 0.0015, 0.0016, 0.0015],
        [0.0013, 0.0013, 0.0006, 0.0008, 0.0009, 0.0008],
        [0.0015, 0.0015, 0.0007, 0.0009, 0.0009, 0.0009],
        [0.0013, 0.0013, 0.0006, 0.0008, 0.0008, 0.0008],
        [0.0029, 0.0029, 0.0014, 0.0018, 0.0019, 0.0018],
        [0.0011, 0.0011, 0.0005, 0.0007, 0.0007, 0.0007]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(243.7575, grad_fn=<NegBackward>) non event loss:  tensor([478.1505], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.4569, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.7463e-04, 1.7317e-04, 2.4804e-04, 4.5643e-04, 6.5583e-04, 4.0708e-04,
         4.8655e-04, 4.4836e-04, 2.8017e-04, 3.6776e-04, 2.9500e-04, 2.7988e-04,
         5.6709e-04, 3.9142e-04],
        [3.6191e-04, 1.3204e-04, 1.8913e-04, 3.4804e-04, 5.0009e-04, 3.1040e-04,
         3.7100e-04, 3.4188e-04, 2.1363e-04, 2.8042e-04, 2.2494e-04, 2.1341e-04,
         4.3242e-04, 2.9846e-04],
        [2.8055e-04, 1.0236e-04, 1.4661e-04, 2.6980e-04, 3.8767e-04, 2.4062e-04,
         2.8760e-04, 2.6502e-04, 1.6560e-04, 2.1738e-04, 1.7437e-04, 1.6543e-04,
         3.3521e-04, 2.3136e-04],
        [1.7920e-04, 6.5376e-05, 9.3644e-05, 1.7233e-04, 2.4762e-04, 1.5369e-04,
         1.8370e-04, 1.6928e-04, 1.0577e-04, 1.3885e-04, 1.1137e-04, 1.0566e-04,
         2.1411e-04, 1.4778e-04],
        [1.8478e-04, 6.7411e-05, 9.6558e-05, 1.7769e-04, 2.5533e-04, 1.5847e-04,
         1.8942e-04, 1.7455e-04, 1.0906e-04, 1.4317e-04, 1.1484e-04, 1.0895e-04,
         2.2078e-04, 1.5238e-04],
        [1.2235e-04, 4.4635e-05, 6.3936e-05, 1.1766e-04, 1.6907e-04, 1.0493e-04,
         1.2542e-04, 1.1558e-04, 7.2217e-05, 9.4799e-05, 7.6040e-05, 7.2142e-05,
         1.4619e-04, 1.0090e-04],
        [1.1797e-04, 4.3039e-05, 6.1648e-05, 1.1345e-04, 1.6302e-04, 1.0118e-04,
         1.2094e-04, 1.1144e-04, 6.9633e-05, 9.1408e-05, 7.3320e-05, 6.9562e-05,
         1.4096e-04, 9.7287e-05],
        [1.2762e-04, 4.6559e-05, 6.6691e-05, 1.2273e-04, 1.7636e-04, 1.0946e-04,
         1.3083e-04, 1.2056e-04, 7.5329e-05, 9.8885e-05, 7.9318e-05, 7.5252e-05,
         1.5249e-04, 1.0525e-04],
        [1.2685e-04, 4.6279e-05, 6.6290e-05, 1.2199e-04, 1.7530e-04, 1.0880e-04,
         1.3004e-04, 1.1983e-04, 7.4876e-05, 9.8289e-05, 7.8840e-05, 7.4799e-05,
         1.5157e-04, 1.0461e-04],
        [1.5773e-04, 5.7545e-05, 8.2427e-05, 1.5169e-04, 2.1797e-04, 1.3528e-04,
         1.6170e-04, 1.4900e-04, 9.3103e-05, 1.2222e-04, 9.8032e-05, 9.3007e-05,
         1.8847e-04, 1.3008e-04],
        [2.1493e-04, 7.8414e-05, 1.1232e-04, 2.0669e-04, 2.9700e-04, 1.8434e-04,
         2.2033e-04, 2.0303e-04, 1.2687e-04, 1.6654e-04, 1.3358e-04, 1.2674e-04,
         2.5681e-04, 1.7725e-04],
        [1.1764e-04, 4.2916e-05, 6.1473e-05, 1.1313e-04, 1.6256e-04, 1.0089e-04,
         1.2059e-04, 1.1112e-04, 6.9435e-05, 9.1147e-05, 7.3111e-05, 6.9363e-05,
         1.4056e-04, 9.7010e-05],
        [6.0349e-05, 2.2016e-05, 3.1536e-05, 5.8035e-05, 8.3395e-05, 5.1758e-05,
         6.1865e-05, 5.7008e-05, 3.5620e-05, 4.6759e-05, 3.7506e-05, 3.5584e-05,
         7.2108e-05, 4.9767e-05],
        [1.0147e-04, 3.7018e-05, 5.3025e-05, 9.7580e-05, 1.4022e-04, 8.7026e-05,
         1.0402e-04, 9.5853e-05, 5.9893e-05, 7.8621e-05, 6.3064e-05, 5.9831e-05,
         1.2124e-04, 8.3678e-05]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1756.8633, grad_fn=<NegBackward>) non event loss:  tensor([124.9768], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.3976, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.1350e-04, 1.6716e-04, 7.5964e-05, 7.3616e-05, 7.0897e-05, 2.2166e-04,
         1.2734e-04, 8.7062e-05, 1.0436e-04, 1.3284e-04, 6.1363e-05, 1.2929e-04,
         8.8665e-05, 1.5911e-04, 1.5390e-04, 1.0181e-04, 1.0997e-04],
        [1.5458e-04, 1.2103e-04, 5.4999e-05, 5.3299e-05, 5.1330e-05, 1.6049e-04,
         9.2196e-05, 6.3035e-05, 7.5558e-05, 9.6181e-05, 4.4428e-05, 9.3611e-05,
         6.4195e-05, 1.1520e-04, 1.1143e-04, 7.3715e-05, 7.9620e-05],
        [8.5436e-05, 6.6891e-05, 3.0398e-05, 2.9458e-05, 2.8370e-05, 8.8702e-05,
         5.0956e-05, 3.4838e-05, 4.1760e-05, 5.3159e-05, 2.4555e-05, 5.1738e-05,
         3.5480e-05, 6.3670e-05, 6.1587e-05, 4.0741e-05, 4.4006e-05],
        [5.0692e-05, 3.9689e-05, 1.8036e-05, 1.7478e-05, 1.6833e-05, 5.2630e-05,
         3.0234e-05, 2.0671e-05, 2.4777e-05, 3.1541e-05, 1.4569e-05, 3.0698e-05,
         2.1051e-05, 3.7777e-05, 3.6542e-05, 2.4173e-05, 2.6110e-05],
        [5.5886e-05, 4.3755e-05, 1.9884e-05, 1.9269e-05, 1.8557e-05, 5.8023e-05,
         3.3332e-05, 2.2789e-05, 2.7316e-05, 3.4772e-05, 1.6062e-05, 3.3843e-05,
         2.3208e-05, 4.1648e-05, 4.0286e-05, 2.6650e-05, 2.8785e-05],
        [4.4320e-05, 3.4700e-05, 1.5769e-05, 1.5281e-05, 1.4717e-05, 4.6015e-05,
         2.6434e-05, 1.8072e-05, 2.1663e-05, 2.7576e-05, 1.2738e-05, 2.6839e-05,
         1.8405e-05, 3.3029e-05, 3.1948e-05, 2.1135e-05, 2.2828e-05],
        [3.9956e-05, 3.1283e-05, 1.4216e-05, 1.3777e-05, 1.3268e-05, 4.1484e-05,
         2.3831e-05, 1.6293e-05, 1.9530e-05, 2.4861e-05, 1.1483e-05, 2.4196e-05,
         1.6593e-05, 2.9777e-05, 2.8803e-05, 1.9054e-05, 2.0580e-05],
        [3.8850e-05, 3.0417e-05, 1.3823e-05, 1.3395e-05, 1.2900e-05, 4.0336e-05,
         2.3171e-05, 1.5842e-05, 1.8989e-05, 2.4173e-05, 1.1166e-05, 2.3527e-05,
         1.6134e-05, 2.8953e-05, 2.8005e-05, 1.8526e-05, 2.0011e-05],
        [3.0904e-05, 2.4195e-05, 1.0995e-05, 1.0655e-05, 1.0262e-05, 3.2085e-05,
         1.8432e-05, 1.2601e-05, 1.5105e-05, 1.9228e-05, 8.8817e-06, 1.8714e-05,
         1.2833e-05, 2.3030e-05, 2.2277e-05, 1.4737e-05, 1.5917e-05],
        [2.6871e-05, 2.1038e-05, 9.5603e-06, 9.2648e-06, 8.9225e-06, 2.7898e-05,
         1.6026e-05, 1.0957e-05, 1.3134e-05, 1.6719e-05, 7.7227e-06, 1.6272e-05,
         1.1159e-05, 2.0025e-05, 1.9370e-05, 1.2814e-05, 1.3840e-05],
        [2.5515e-05, 1.9976e-05, 9.0779e-06, 8.7973e-06, 8.4723e-06, 2.6491e-05,
         1.5218e-05, 1.0404e-05, 1.2471e-05, 1.5875e-05, 7.3330e-06, 1.5451e-05,
         1.0596e-05, 1.9015e-05, 1.8392e-05, 1.2167e-05, 1.3142e-05],
        [2.5384e-05, 1.9874e-05, 9.0313e-06, 8.7521e-06, 8.4288e-06, 2.6355e-05,
         1.5139e-05, 1.0351e-05, 1.2407e-05, 1.5794e-05, 7.2954e-06, 1.5372e-05,
         1.0541e-05, 1.8917e-05, 1.8298e-05, 1.2105e-05, 1.3074e-05],
        [1.5195e-05, 1.1897e-05, 5.4063e-06, 5.2392e-06, 5.0456e-06, 1.5776e-05,
         9.0628e-06, 6.1961e-06, 7.4272e-06, 9.4545e-06, 4.3671e-06, 9.2019e-06,
         6.3102e-06, 1.1324e-05, 1.0954e-05, 7.2460e-06, 7.8266e-06],
        [1.4483e-05, 1.1339e-05, 5.1529e-06, 4.9936e-06, 4.8091e-06, 1.5037e-05,
         8.6380e-06, 5.9057e-06, 7.0791e-06, 9.0114e-06, 4.1624e-06, 8.7706e-06,
         6.0144e-06, 1.0793e-05, 1.0440e-05, 6.9064e-06, 7.4597e-06],
        [1.6512e-05, 1.2928e-05, 5.8748e-06, 5.6932e-06, 5.4829e-06, 1.7144e-05,
         9.8482e-06, 6.7331e-06, 8.0708e-06, 1.0274e-05, 4.7456e-06, 9.9993e-06,
         6.8570e-06, 1.2305e-05, 1.1903e-05, 7.8740e-06, 8.5048e-06],
        [2.2462e-05, 1.7587e-05, 7.9918e-06, 7.7448e-06, 7.4587e-06, 2.3321e-05,
         1.3397e-05, 9.1594e-06, 1.0979e-05, 1.3976e-05, 6.4557e-06, 1.3603e-05,
         9.3280e-06, 1.6740e-05, 1.6192e-05, 1.0711e-05, 1.1570e-05],
        [4.9435e-05, 3.8704e-05, 1.7588e-05, 1.7045e-05, 1.6415e-05, 5.1325e-05,
         2.9484e-05, 2.0158e-05, 2.4163e-05, 3.0759e-05, 1.4208e-05, 2.9937e-05,
         2.0529e-05, 3.6841e-05, 3.5635e-05, 2.3574e-05, 2.5462e-05]],
       grad_fn=<SoftplusBackward>)

  3%|████▉                                                                                                                                          | 17/496 [00:03<01:13,  6.48it/s]
##### event loss: tensor(3114.1001, grad_fn=<NegBackward>) non event loss:  tensor([33.7406], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0255, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.9219e-05, 2.1958e-05, 2.8562e-05, 3.5790e-05, 1.3793e-05],
        [2.8521e-05, 2.1433e-05, 2.7880e-05, 3.4935e-05, 1.3463e-05],
        [2.4657e-05, 1.8529e-05, 2.4102e-05, 3.0201e-05, 1.1639e-05],
        [2.7842e-05, 2.0923e-05, 2.7217e-05, 3.4103e-05, 1.3143e-05],
        [1.2258e-05, 9.2118e-06, 1.1982e-05, 1.5014e-05, 5.7863e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(270.8529, grad_fn=<NegBackward>) non event loss:  tensor([3.0111], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0080, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.2995e-05, 1.8558e-05, 1.0183e-05, 1.1450e-05, 1.1803e-05, 1.8554e-05],
        [1.2568e-05, 1.7948e-05, 9.8479e-06, 1.1074e-05, 1.1415e-05, 1.7943e-05],
        [6.4588e-06, 9.2239e-06, 5.0611e-06, 5.6910e-06, 5.8664e-06, 9.2217e-06],
        [5.7923e-06, 8.2720e-06, 4.5388e-06, 5.1037e-06, 5.2610e-06, 8.2701e-06],
        [5.7062e-06, 8.1490e-06, 4.4713e-06, 5.0279e-06, 5.1828e-06, 8.1471e-06],
        [5.9229e-06, 8.4585e-06, 4.6411e-06, 5.2188e-06, 5.3796e-06, 8.4565e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(422.4553, grad_fn=<NegBackward>) non event loss:  tensor([2.9914], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0260, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.5551e-06, 2.2376e-06, 3.4149e-06, 2.5252e-06, 3.8620e-06, 3.6251e-06,
         1.8634e-06, 1.5883e-06],
        [3.3121e-06, 4.7657e-06, 7.2729e-06, 5.3781e-06, 8.2252e-06, 7.7206e-06,
         3.9685e-06, 3.3827e-06],
        [1.7335e-06, 2.4943e-06, 3.8066e-06, 2.8149e-06, 4.3050e-06, 4.0409e-06,
         2.0771e-06, 1.7705e-06],
        [1.3621e-06, 1.9599e-06, 2.9910e-06, 2.2117e-06, 3.3826e-06, 3.1751e-06,
         1.6321e-06, 1.3911e-06],
        [1.3539e-06, 1.9481e-06, 2.9730e-06, 2.1985e-06, 3.3623e-06, 3.1560e-06,
         1.6223e-06, 1.3828e-06],
        [1.0325e-06, 1.4856e-06, 2.2673e-06, 1.6766e-06, 2.5641e-06, 2.4068e-06,
         1.2372e-06, 1.0545e-06],
        [2.4403e-06, 3.5112e-06, 5.3585e-06, 3.9625e-06, 6.0601e-06, 5.6884e-06,
         2.9239e-06, 2.4923e-06],
        [2.5155e-06, 3.6195e-06, 5.5237e-06, 4.0847e-06, 6.2470e-06, 5.8638e-06,
         3.0141e-06, 2.5691e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(818.2651, grad_fn=<NegBackward>) non event loss:  tensor([0.7785], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0614, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.5567e-06, 1.5629e-06, 1.5049e-06, 1.1945e-06, 2.2374e-06],
        [1.5369e-06, 1.5430e-06, 1.4858e-06, 1.1793e-06, 2.2090e-06],
        [2.6430e-07, 2.6536e-07, 2.5552e-07, 2.0282e-07, 3.7989e-07],
        [3.5695e-07, 3.5838e-07, 3.4509e-07, 2.7391e-07, 5.1305e-07],
        [3.0318e-07, 3.0440e-07, 2.9311e-07, 2.3265e-07, 4.3578e-07]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(358.4566, grad_fn=<NegBackward>) non event loss:  tensor([0.0993], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0551, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[6.4025e-06, 5.6472e-06, 7.4946e-06, 6.8301e-06, 7.0944e-06, 1.0579e-05,
         3.6635e-06],
        [4.8789e-07, 4.3033e-07, 5.7110e-07, 5.2047e-07, 5.4061e-07, 8.0616e-07,
         2.7916e-07],
        [3.0058e-07, 2.6512e-07, 3.5185e-07, 3.2066e-07, 3.3307e-07, 4.9667e-07,
         1.7199e-07],
        [1.7234e-07, 1.5201e-07, 2.0173e-07, 1.8385e-07, 1.9096e-07, 2.8476e-07,
         9.8610e-08],
        [2.9860e-07, 2.6338e-07, 3.4953e-07, 3.1854e-07, 3.3087e-07, 4.9340e-07,
         1.7086e-07],
        [1.0591e-07, 9.3415e-08, 1.2398e-07, 1.1298e-07, 1.1736e-07, 1.7500e-07,
         6.0601e-08],
        [1.2383e-07, 1.0922e-07, 1.4495e-07, 1.3210e-07, 1.3721e-07, 2.0461e-07,
         7.0853e-08]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(727.5059, grad_fn=<NegBackward>) non event loss:  tensor([0.2290], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0782, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[6.6900e-07, 1.4624e-06, 2.1597e-06, 1.8568e-06, 1.2917e-06, 8.4714e-07,
         1.4711e-06, 7.3338e-07, 1.5735e-06],
        [5.9108e-07, 1.2920e-06, 1.9082e-06, 1.6405e-06, 1.1413e-06, 7.4847e-07,
         1.2998e-06, 6.4796e-07, 1.3902e-06],
        [4.3773e-07, 9.5682e-07, 1.4131e-06, 1.2149e-06, 8.4518e-07, 5.5429e-07,
         9.6256e-07, 4.7985e-07, 1.0295e-06],
        [3.4046e-07, 7.4421e-07, 1.0991e-06, 9.4495e-07, 6.5737e-07, 4.3112e-07,
         7.4867e-07, 3.7323e-07, 8.0076e-07],
        [1.5208e-07, 3.3244e-07, 4.9097e-07, 4.2211e-07, 2.9365e-07, 1.9258e-07,
         3.3443e-07, 1.6672e-07, 3.5770e-07],
        [1.5198e-07, 3.3221e-07, 4.9063e-07, 4.2182e-07, 2.9344e-07, 1.9245e-07,
         3.3420e-07, 1.6660e-07, 3.5745e-07],
        [2.6302e-07, 5.7493e-07, 8.4910e-07, 7.3001e-07, 5.0784e-07, 3.3306e-07,
         5.7838e-07, 2.8833e-07, 6.1861e-07],
        [2.7201e-07, 5.9458e-07, 8.7812e-07, 7.5496e-07, 5.2520e-07, 3.4444e-07,
         5.9814e-07, 2.9818e-07, 6.3976e-07],
        [2.6205e-07, 5.7281e-07, 8.4597e-07, 7.2732e-07, 5.0597e-07, 3.3183e-07,
         5.7624e-07, 2.8727e-07, 6.1633e-07]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1163.7993, grad_fn=<NegBackward>) non event loss:  tensor([0.3428], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0751, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.6916e-07, 2.4912e-07, 2.1477e-07, 1.2646e-07, 2.5252e-07, 2.2438e-07],
        [2.6865e-07, 2.4865e-07, 2.1436e-07, 1.2622e-07, 2.5204e-07, 2.2396e-07],
        [6.7587e-08, 6.2555e-08, 5.3930e-08, 3.1755e-08, 6.3409e-08, 5.6343e-08],
        [3.1390e-08, 2.9053e-08, 2.5047e-08, 1.4748e-08, 2.9450e-08, 2.6168e-08],
        [3.0377e-08, 2.8115e-08, 2.4239e-08, 1.4272e-08, 2.8499e-08, 2.5323e-08],
        [3.0742e-08, 2.8452e-08, 2.4529e-08, 1.4444e-08, 2.8841e-08, 2.5627e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(599.7355, grad_fn=<NegBackward>) non event loss:  tensor([0.0592], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0740, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.2597e-07, 1.2744e-07, 5.9702e-08, 1.3392e-07, 1.4695e-07, 1.3312e-07],
        [1.0315e-07, 1.0435e-07, 4.8887e-08, 1.0966e-07, 1.2033e-07, 1.0900e-07],
        [1.0386e-07, 1.0508e-07, 4.9225e-08, 1.1042e-07, 1.2116e-07, 1.0976e-07],
        [1.0213e-07, 1.0332e-07, 4.8403e-08, 1.0858e-07, 1.1914e-07, 1.0792e-07],
        [1.0144e-07, 1.0263e-07, 4.8077e-08, 1.0785e-07, 1.1834e-07, 1.0720e-07],
        [4.0330e-08, 4.0801e-08, 1.9114e-08, 4.2877e-08, 4.7047e-08, 4.2618e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(586.4765, grad_fn=<NegBackward>) non event loss:  tensor([0.0338], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0440, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.6177e-07, 9.8340e-08, 1.2577e-07, 1.3283e-07, 1.4571e-07, 1.2723e-07],
        [6.8176e-08, 4.1445e-08, 5.3003e-08, 5.5980e-08, 6.1410e-08, 5.3620e-08],
        [6.8175e-08, 4.1444e-08, 5.3002e-08, 5.5979e-08, 6.1409e-08, 5.3620e-08],
        [3.9240e-08, 2.3855e-08, 3.0507e-08, 3.2221e-08, 3.5346e-08, 3.0862e-08],
        [6.7340e-08, 4.0937e-08, 5.2353e-08, 5.5293e-08, 6.0657e-08, 5.2963e-08],
        [5.1899e-08, 3.1550e-08, 4.0349e-08, 4.2615e-08, 4.6749e-08, 4.0819e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(601.6305, grad_fn=<NegBackward>) non event loss:  tensor([0.0197], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0877, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.4313e-07, 2.4664e-07, 3.0907e-07, 2.3282e-07],
        [2.2264e-07, 1.6003e-07, 2.0054e-07, 1.5106e-07],
        [1.0648e-07, 7.6535e-08, 9.5909e-08, 7.2246e-08],
        [8.5671e-08, 6.1580e-08, 7.7168e-08, 5.8129e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(253.4141, grad_fn=<NegBackward>) non event loss:  tensor([0.0241], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0620, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.5165e-08, 1.3531e-08, 1.3646e-08, 1.3617e-08, 5.1806e-09, 1.3985e-08,
         1.3270e-08, 1.3591e-08, 7.8734e-09, 1.0381e-08, 1.3571e-08, 1.3342e-08,
         5.6397e-09, 8.7206e-09, 1.3653e-08, 1.2786e-08, 1.3651e-08, 1.3541e-08,
         1.5155e-08, 1.3683e-08],
        [2.1042e-08, 1.8775e-08, 1.8935e-08, 1.8895e-08, 7.1883e-09, 1.9405e-08,
         1.8413e-08, 1.8858e-08, 1.0925e-08, 1.4404e-08, 1.8830e-08, 1.8513e-08,
         7.8254e-09, 1.2100e-08, 1.8944e-08, 1.7741e-08, 1.8941e-08, 1.8789e-08,
         2.1029e-08, 1.8985e-08],
        [2.1184e-08, 1.8901e-08, 1.9063e-08, 1.9022e-08, 7.2368e-09, 1.9536e-08,
         1.8538e-08, 1.8985e-08, 1.0998e-08, 1.4501e-08, 1.8957e-08, 1.8638e-08,
         7.8782e-09, 1.2182e-08, 1.9072e-08, 1.7861e-08, 1.9069e-08, 1.8916e-08,
         2.1171e-08, 1.9114e-08],
        [5.9305e-09, 5.2914e-09, 5.3366e-09, 5.3253e-09, 2.0259e-09, 5.4690e-09,
         5.1895e-09, 5.3148e-09, 3.0790e-09, 4.0596e-09, 5.3069e-09, 5.2175e-09,
         2.2055e-09, 3.4103e-09, 5.3390e-09, 5.0000e-09, 5.3382e-09, 5.2954e-09,
         5.9267e-09, 5.3508e-09],
        [2.6621e-09, 2.3753e-09, 2.3956e-09, 2.3905e-09, 9.0942e-10, 2.4550e-09,
         2.3295e-09, 2.3858e-09, 1.3821e-09, 1.8223e-09, 2.3823e-09, 2.3421e-09,
         9.9003e-10, 1.5309e-09, 2.3967e-09, 2.2445e-09, 2.3963e-09, 2.3771e-09,
         2.6604e-09, 2.4019e-09],
        [5.8724e-09, 5.2396e-09, 5.2843e-09, 5.2731e-09, 2.0061e-09, 5.4155e-09,
         5.1387e-09, 5.2628e-09, 3.0488e-09, 4.0199e-09, 5.2550e-09, 5.1664e-09,
         2.1839e-09, 3.3769e-09, 5.2867e-09, 4.9510e-09, 5.2859e-09, 5.2435e-09,
         5.8686e-09, 5.2984e-09],
        [7.9756e-09, 7.1162e-09, 7.1769e-09, 7.1617e-09, 2.7246e-09, 7.3551e-09,
         6.9792e-09, 7.1477e-09, 4.1408e-09, 5.4596e-09, 7.1371e-09, 7.0168e-09,
         2.9661e-09, 4.5864e-09, 7.1802e-09, 6.7243e-09, 7.1791e-09, 7.1215e-09,
         7.9705e-09, 7.1961e-09],
        [8.1325e-09, 7.2561e-09, 7.3181e-09, 7.3026e-09, 2.7782e-09, 7.4997e-09,
         7.1164e-09, 7.2882e-09, 4.2222e-09, 5.5670e-09, 7.2774e-09, 7.1548e-09,
         3.0244e-09, 4.6766e-09, 7.3214e-09, 6.8565e-09, 7.3203e-09, 7.2616e-09,
         8.1273e-09, 7.3376e-09],
        [7.9302e-09, 7.0757e-09, 7.1361e-09, 7.1209e-09, 2.7091e-09, 7.3132e-09,
         6.9394e-09, 7.1070e-09, 4.1172e-09, 5.4285e-09, 7.0964e-09, 6.9769e-09,
         2.9492e-09, 4.5603e-09, 7.1393e-09, 6.6860e-09, 7.1383e-09, 7.0810e-09,
         7.9251e-09, 7.1551e-09],
        [3.8466e-09, 3.4321e-09, 3.4614e-09, 3.4540e-09, 1.3140e-09, 3.5473e-09,
         3.3660e-09, 3.4473e-09, 1.9971e-09, 2.6331e-09, 3.4422e-09, 3.3842e-09,
         1.4305e-09, 2.2120e-09, 3.4630e-09, 3.2431e-09, 3.4624e-09, 3.4346e-09,
         3.8441e-09, 3.4706e-09],
        [5.4824e-09, 4.8917e-09, 4.9334e-09, 4.9230e-09, 1.8729e-09, 5.0559e-09,
         4.7975e-09, 4.9133e-09, 2.8464e-09, 3.7530e-09, 4.9060e-09, 4.8234e-09,
         2.0389e-09, 3.1527e-09, 4.9357e-09, 4.6223e-09, 4.9349e-09, 4.8953e-09,
         5.4789e-09, 4.9466e-09],
        [8.7496e-09, 7.8068e-09, 7.8735e-09, 7.8568e-09, 2.9890e-09, 8.0689e-09,
         7.6565e-09, 7.8413e-09, 4.5427e-09, 5.9895e-09, 7.8297e-09, 7.6978e-09,
         3.2539e-09, 5.0315e-09, 7.8771e-09, 7.3769e-09, 7.8759e-09, 7.8127e-09,
         8.7441e-09, 7.8944e-09],
        [5.7450e-09, 5.1259e-09, 5.1697e-09, 5.1587e-09, 1.9626e-09, 5.2980e-09,
         5.0272e-09, 5.1486e-09, 2.9827e-09, 3.9327e-09, 5.1410e-09, 5.0544e-09,
         2.1365e-09, 3.3037e-09, 5.1721e-09, 4.8436e-09, 5.1713e-09, 5.1298e-09,
         5.7413e-09, 5.1835e-09],
        [4.5687e-09, 4.0764e-09, 4.1112e-09, 4.1025e-09, 1.5607e-09, 4.2132e-09,
         3.9979e-09, 4.0944e-09, 2.3720e-09, 3.1275e-09, 4.0884e-09, 4.0195e-09,
         1.6991e-09, 2.6272e-09, 4.1131e-09, 3.8519e-09, 4.1125e-09, 4.0794e-09,
         4.5658e-09, 4.1221e-09],
        [5.2374e-09, 4.6730e-09, 4.7129e-09, 4.7029e-09, 1.7892e-09, 4.8299e-09,
         4.5830e-09, 4.6937e-09, 2.7191e-09, 3.5852e-09, 4.6867e-09, 4.6078e-09,
         1.9477e-09, 3.0117e-09, 4.7151e-09, 4.4156e-09, 4.7143e-09, 4.6765e-09,
         5.2340e-09, 4.7254e-09],
        [5.1308e-09, 4.5779e-09, 4.6170e-09, 4.6072e-09, 1.7527e-09, 4.7316e-09,
         4.4898e-09, 4.5982e-09, 2.6638e-09, 3.5122e-09, 4.5914e-09, 4.5140e-09,
         1.9081e-09, 2.9505e-09, 4.6191e-09, 4.3258e-09, 4.6184e-09, 4.5813e-09,
         5.1275e-09, 4.6293e-09],
        [7.6718e-09, 6.8451e-09, 6.9035e-09, 6.8889e-09, 2.6208e-09, 7.0749e-09,
         6.7133e-09, 6.8753e-09, 3.9830e-09, 5.2516e-09, 6.8652e-09, 6.7495e-09,
         2.8531e-09, 4.4116e-09, 6.9067e-09, 6.4681e-09, 6.9056e-09, 6.8502e-09,
         7.6669e-09, 6.9219e-09],
        [6.7649e-09, 6.0359e-09, 6.0875e-09, 6.0745e-09, 2.3110e-09, 6.2386e-09,
         5.9197e-09, 6.0626e-09, 3.5122e-09, 4.6308e-09, 6.0537e-09, 5.9517e-09,
         2.5158e-09, 3.8902e-09, 6.0903e-09, 5.7035e-09, 6.0893e-09, 6.0404e-09,
         6.7606e-09, 6.1037e-09],
        [6.8165e-09, 6.0820e-09, 6.1339e-09, 6.1209e-09, 2.3286e-09, 6.2861e-09,
         5.9649e-09, 6.1089e-09, 3.5390e-09, 4.6662e-09, 6.0998e-09, 5.9971e-09,
         2.5350e-09, 3.9198e-09, 6.1367e-09, 5.7470e-09, 6.1358e-09, 6.0865e-09,
         6.8121e-09, 6.1502e-09],
        [5.9135e-09, 5.2763e-09, 5.3213e-09, 5.3100e-09, 2.0201e-09, 5.4534e-09,
         5.1747e-09, 5.2996e-09, 3.0702e-09, 4.0480e-09, 5.2918e-09, 5.2026e-09,
         2.1992e-09, 3.4006e-09, 5.3238e-09, 4.9857e-09, 5.3230e-09, 5.2802e-09,
         5.9097e-09, 5.3355e-09]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(7611.8198, grad_fn=<NegBackward>) non event loss:  tensor([0.0049], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1938, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.4153e-07, 1.1868e-07, 1.1689e-07, 1.3384e-07, 1.1089e-07],
        [1.7299e-07, 8.4996e-08, 8.3716e-08, 9.5855e-08, 7.9419e-08],
        [1.2637e-07, 6.2091e-08, 6.1156e-08, 7.0023e-08, 5.8017e-08],
        [6.4316e-08, 3.1601e-08, 3.1125e-08, 3.5638e-08, 2.9528e-08],
        [6.4335e-08, 3.1610e-08, 3.1135e-08, 3.5649e-08, 2.9537e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(413.0712, grad_fn=<NegBackward>) non event loss:  tensor([0.0210], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0869, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[8.9518e-08, 8.9518e-08, 1.3018e-07, 8.3484e-08, 5.7539e-08, 1.0251e-07,
         4.0897e-08],
        [3.0551e-08, 3.0551e-08, 4.4427e-08, 2.8492e-08, 1.9637e-08, 3.4985e-08,
         1.3957e-08],
        [7.2560e-09, 7.2560e-09, 1.0552e-08, 6.7669e-09, 4.6639e-09, 8.3093e-09,
         3.3150e-09],
        [3.8798e-09, 3.8798e-09, 5.6420e-09, 3.6183e-09, 2.4938e-09, 4.4430e-09,
         1.7725e-09],
        [4.5265e-09, 4.5265e-09, 6.5824e-09, 4.2214e-09, 2.9095e-09, 5.1836e-09,
         2.0680e-09],
        [5.5829e-09, 5.5829e-09, 8.1186e-09, 5.2066e-09, 3.5885e-09, 6.3932e-09,
         2.5506e-09],
        [5.5840e-09, 5.5840e-09, 8.1201e-09, 5.2076e-09, 3.5892e-09, 6.3945e-09,

  6%|████████                                                                                                                                       | 28/496 [00:05<01:19,  5.90it/s]
##### event loss: tensor(907.5292, grad_fn=<NegBackward>) non event loss:  tensor([0.0073], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0897, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.3375e-08, 8.0465e-09, 1.4241e-08, 1.4991e-08],
        [2.0811e-08, 1.2520e-08, 2.2159e-08, 2.3326e-08],
        [4.9109e-09, 2.9545e-09, 5.2290e-09, 5.5043e-09],
        [5.1529e-09, 3.1001e-09, 5.4866e-09, 5.7755e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(297.4579, grad_fn=<NegBackward>) non event loss:  tensor([0.0041], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0872, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[6.9987e-08, 1.0216e-07, 9.1089e-08, 7.2570e-08, 7.5287e-08, 1.1978e-07,
         5.9939e-08, 5.4032e-08, 1.0869e-07],
        [2.9448e-08, 4.2987e-08, 3.8327e-08, 3.0535e-08, 3.1678e-08, 5.0398e-08,
         2.5220e-08, 2.2735e-08, 4.5731e-08],
        [3.0626e-08, 4.4706e-08, 3.9860e-08, 3.1756e-08, 3.2945e-08, 5.2413e-08,
         2.6229e-08, 2.3644e-08, 4.7560e-08],
        [1.0341e-08, 1.5096e-08, 1.3459e-08, 1.0723e-08, 1.1124e-08, 1.7698e-08,
         8.8565e-09, 7.9838e-09, 1.6059e-08],
        [1.0175e-08, 1.4853e-08, 1.3242e-08, 1.0550e-08, 1.0945e-08, 1.7413e-08,
         8.7138e-09, 7.8551e-09, 1.5801e-08],
        [1.9331e-09, 2.8219e-09, 2.5160e-09, 2.0044e-09, 2.0795e-09, 3.3083e-09,
         1.6556e-09, 1.4924e-09, 3.0020e-09],
        [3.8845e-09, 5.6704e-09, 5.0557e-09, 4.0278e-09, 4.1786e-09, 6.6479e-09,
         3.3267e-09, 2.9989e-09, 6.0323e-09],
        [5.9670e-09, 8.7104e-09, 7.7661e-09, 6.1872e-09, 6.4189e-09, 1.0212e-08,
         5.1103e-09, 4.6067e-09, 9.2664e-09],
        [5.9580e-09, 8.6973e-09, 7.7544e-09, 6.1779e-09, 6.4092e-09, 1.0197e-08,
         5.1026e-09, 4.5998e-09, 9.2524e-09]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1475.0883, grad_fn=<NegBackward>) non event loss:  tensor([0.0045], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0950, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.5848e-07, 2.3771e-07, 1.0880e-07, 1.2566e-07, 1.3157e-07, 1.5596e-07,
         7.1438e-08, 1.9095e-07, 1.5106e-07, 1.8768e-07, 2.7622e-07],
        [1.8034e-08, 2.7049e-08, 1.2380e-08, 1.4300e-08, 1.4972e-08, 1.7747e-08,
         8.1291e-09, 2.1728e-08, 1.7189e-08, 2.1357e-08, 3.1432e-08],
        [9.1235e-09, 1.3684e-08, 6.2634e-09, 7.2344e-09, 7.5743e-09, 8.9784e-09,
         4.1126e-09, 1.0993e-08, 8.6963e-09, 1.0805e-08, 1.5902e-08],
        [1.7492e-08, 2.6236e-08, 1.2008e-08, 1.3870e-08, 1.4522e-08, 1.7214e-08,
         7.8848e-09, 2.1075e-08, 1.6673e-08, 2.0715e-08, 3.0487e-08],
        [1.0767e-08, 1.6149e-08, 7.3914e-09, 8.5373e-09, 8.9384e-09, 1.0595e-08,
         4.8533e-09, 1.2972e-08, 1.0262e-08, 1.2750e-08, 1.8766e-08],
        [1.0620e-08, 1.5928e-08, 7.2904e-09, 8.4206e-09, 8.8163e-09, 1.0451e-08,
         4.7870e-09, 1.2795e-08, 1.0122e-08, 1.2576e-08, 1.8509e-08],
        [8.3788e-09, 1.2567e-08, 5.7521e-09, 6.6438e-09, 6.9560e-09, 8.2455e-09,
         3.7769e-09, 1.0095e-08, 7.9864e-09, 9.9226e-09, 1.4604e-08],
        [8.4048e-09, 1.2606e-08, 5.7699e-09, 6.6644e-09, 6.9776e-09, 8.2711e-09,
         3.7886e-09, 1.0127e-08, 8.0112e-09, 9.9534e-09, 1.4649e-08],
        [8.2703e-09, 1.2405e-08, 5.6776e-09, 6.5578e-09, 6.8660e-09, 8.1388e-09,
         3.7280e-09, 9.9647e-09, 7.8830e-09, 9.7942e-09, 1.4415e-08],
        [3.5162e-09, 5.2741e-09, 2.4139e-09, 2.7882e-09, 2.9192e-09, 3.4603e-09,
         1.5850e-09, 4.2366e-09, 3.3516e-09, 4.1641e-09, 6.1286e-09],
        [3.5188e-09, 5.2778e-09, 2.4157e-09, 2.7902e-09, 2.9213e-09, 3.4628e-09,
         1.5862e-09, 4.2396e-09, 3.3540e-09, 4.1671e-09, 6.1330e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(2218.0642, grad_fn=<NegBackward>) non event loss:  tensor([0.0140], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1119, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.9612e-07, 2.4663e-07, 2.3586e-07, 1.4791e-07, 2.6288e-07],
        [1.6817e-07, 1.4006e-07, 1.3395e-07, 8.3999e-08, 1.4929e-07],
        [1.4469e-07, 1.2051e-07, 1.1525e-07, 7.2274e-08, 1.2845e-07],
        [4.9001e-08, 4.0812e-08, 3.9029e-08, 2.4476e-08, 4.3501e-08],
        [4.8675e-09, 4.0540e-09, 3.8769e-09, 2.4313e-09, 4.3211e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(417.8759, grad_fn=<NegBackward>) non event loss:  tensor([0.0090], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0769, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.5341e-08, 4.4245e-08, 6.6772e-08, 3.4976e-08, 2.8327e-08, 2.8620e-08,
         4.8049e-08, 3.8228e-08, 4.8036e-08, 3.6354e-08, 3.9910e-08, 3.7556e-08],
        [2.2936e-08, 4.0046e-08, 6.0435e-08, 3.1657e-08, 2.5639e-08, 2.5904e-08,
         4.3489e-08, 3.4600e-08, 4.3477e-08, 3.2904e-08, 3.6122e-08, 3.3991e-08],
        [1.1466e-08, 2.0019e-08, 3.0212e-08, 1.5826e-08, 1.2817e-08, 1.2950e-08,
         2.1741e-08, 1.7297e-08, 2.1735e-08, 1.6449e-08, 1.8058e-08, 1.6993e-08],
        [6.0753e-09, 1.0607e-08, 1.6008e-08, 8.3853e-09, 6.7911e-09, 6.8614e-09,
         1.1519e-08, 9.1647e-09, 1.1516e-08, 8.7155e-09, 9.5680e-09, 9.0036e-09],
        [5.9755e-09, 1.0433e-08, 1.5745e-08, 8.2475e-09, 6.6796e-09, 6.7487e-09,
         1.1330e-08, 9.0142e-09, 1.1327e-08, 8.5723e-09, 9.4109e-09, 8.8557e-09],
        [5.0235e-09, 8.7709e-09, 1.3237e-08, 6.9336e-09, 5.6154e-09, 5.6735e-09,
         9.5251e-09, 7.5781e-09, 9.5224e-09, 7.2066e-09, 7.9116e-09, 7.4448e-09],
        [4.4805e-09, 7.8228e-09, 1.1806e-08, 6.1840e-09, 5.0084e-09, 5.0602e-09,
         8.4954e-09, 6.7589e-09, 8.4930e-09, 6.4275e-09, 7.0563e-09, 6.6400e-09],
        [4.3878e-09, 7.6610e-09, 1.1562e-08, 6.0561e-09, 4.9048e-09, 4.9555e-09,
         8.3197e-09, 6.6191e-09, 8.3174e-09, 6.2946e-09, 6.9104e-09, 6.5027e-09],
        [3.7811e-09, 6.6017e-09, 9.9630e-09, 5.2188e-09, 4.2266e-09, 4.2703e-09,
         7.1694e-09, 5.7039e-09, 7.1674e-09, 5.4243e-09, 5.9549e-09, 5.6036e-09],
        [3.6599e-09, 6.3901e-09, 9.6437e-09, 5.0515e-09, 4.0912e-09, 4.1335e-09,
         6.9396e-09, 5.5211e-09, 6.9377e-09, 5.2504e-09, 5.7640e-09, 5.4240e-09],
        [3.6830e-09, 6.4305e-09, 9.7046e-09, 5.0834e-09, 4.1170e-09, 4.1596e-09,
         6.9834e-09, 5.5559e-09, 6.9814e-09, 5.2836e-09, 5.8004e-09, 5.4582e-09],
        [3.5282e-09, 6.1601e-09, 9.2966e-09, 4.8697e-09, 3.9439e-09, 3.9847e-09,
         6.6898e-09, 5.3224e-09, 6.6879e-09, 5.0615e-09, 5.5566e-09, 5.2288e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(2658.2124, grad_fn=<NegBackward>) non event loss:  tensor([0.0153], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1623, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.1293e-07, 9.3555e-08, 1.0463e-07, 1.6421e-07, 1.0299e-07, 1.0100e-07,
         8.9020e-08, 1.0085e-07, 7.9355e-08, 1.2651e-07, 1.2349e-07],
        [1.1618e-07, 9.6251e-08, 1.0765e-07, 1.6895e-07, 1.0595e-07, 1.0391e-07,
         9.1585e-08, 1.0375e-07, 8.1642e-08, 1.3016e-07, 1.2705e-07],
        [1.1495e-07, 9.5232e-08, 1.0651e-07, 1.6716e-07, 1.0483e-07, 1.0281e-07,
         9.0616e-08, 1.0266e-07, 8.0777e-08, 1.2878e-07, 1.2570e-07],
        [9.1511e-08, 7.5811e-08, 8.4789e-08, 1.3307e-07, 8.3454e-08, 8.1846e-08,
         7.2136e-08, 8.1722e-08, 6.4305e-08, 1.0252e-07, 1.0007e-07],
        [9.1120e-08, 7.5488e-08, 8.4427e-08, 1.3250e-07, 8.3098e-08, 8.1497e-08,
         7.1829e-08, 8.1373e-08, 6.4030e-08, 1.0208e-07, 9.9639e-08],
        [9.0871e-08, 7.5281e-08, 8.4196e-08, 1.3214e-07, 8.2870e-08, 8.1274e-08,
         7.1632e-08, 8.1151e-08, 6.3855e-08, 1.0180e-07, 9.9367e-08],
        [4.3276e-08, 3.5851e-08, 4.0097e-08, 6.2929e-08, 3.9466e-08, 3.8706e-08,
         3.4114e-08, 3.8647e-08, 3.0410e-08, 4.8481e-08, 4.7322e-08],
        [3.2420e-08, 2.6858e-08, 3.0038e-08, 4.7142e-08, 2.9565e-08, 2.8996e-08,
         2.5556e-08, 2.8952e-08, 2.2781e-08, 3.6319e-08, 3.5450e-08],
        [1.1425e-08, 9.4645e-09, 1.0585e-08, 1.6613e-08, 1.0419e-08, 1.0218e-08,
         9.0058e-09, 1.0202e-08, 8.0280e-09, 1.2799e-08, 1.2493e-08],
        [1.3184e-08, 1.0923e-08, 1.2216e-08, 1.9172e-08, 1.2024e-08, 1.1792e-08,
         1.0393e-08, 1.1774e-08, 9.2647e-09, 1.4770e-08, 1.4417e-08],
        [1.3157e-08, 1.0900e-08, 1.2191e-08, 1.9133e-08, 1.1999e-08, 1.1768e-08,
         1.0372e-08, 1.1750e-08, 9.2457e-09, 1.4740e-08, 1.4387e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(2045.4285, grad_fn=<NegBackward>) non event loss:  tensor([0.0071], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1456, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.0663e-07, 1.6785e-07, 1.7986e-07],
        [7.2208e-08, 1.1366e-07, 1.2180e-07],
        [7.0122e-08, 1.1038e-07, 1.1828e-07]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(143.9824, grad_fn=<NegBackward>) non event loss:  tensor([0.0027], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0133, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.0773e-09, 6.3859e-09, 5.6794e-09, 2.4258e-09, 5.5871e-09],
        [5.1557e-09, 8.0748e-09, 7.1816e-09, 3.0674e-09, 7.0648e-09],
        [1.0971e-08, 1.7183e-08, 1.5282e-08, 6.5273e-09, 1.5034e-08],
        [9.3767e-09, 1.4686e-08, 1.3061e-08, 5.5786e-09, 1.2849e-08],
        [4.1552e-09, 6.5078e-09, 5.7878e-09, 2.4721e-09, 5.6937e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(469.6857, grad_fn=<NegBackward>) non event loss:  tensor([0.0075], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0949, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[7.1513e-08, 6.4366e-08, 8.3679e-08, 1.1651e-07, 1.1079e-07, 5.7703e-08,
         6.5607e-08, 1.1633e-07, 1.3548e-07, 1.0588e-07, 8.7110e-08, 1.2339e-07,
         1.1293e-07, 6.4346e-08, 7.8285e-08, 1.0721e-07],
        [6.5804e-08, 5.9227e-08, 7.6999e-08, 1.0721e-07, 1.0195e-07, 5.3096e-08,
         6.0370e-08, 1.0704e-07, 1.2467e-07, 9.7425e-08, 8.0156e-08, 1.1354e-07,
         1.0392e-07, 5.9209e-08, 7.2035e-08, 9.8652e-08],
        [6.5397e-08, 5.8861e-08, 7.6522e-08, 1.0654e-07, 1.0132e-07, 5.2768e-08,
         5.9996e-08, 1.0638e-07, 1.2389e-07, 9.6822e-08, 7.9660e-08, 1.1284e-07,
         1.0327e-07, 5.8843e-08, 7.1590e-08, 9.8041e-08],
        [4.2198e-08, 3.7980e-08, 4.9376e-08, 6.8748e-08, 6.5376e-08, 3.4049e-08,
         3.8713e-08, 6.8641e-08, 7.9943e-08, 6.2475e-08, 5.1401e-08, 7.2809e-08,
         6.6637e-08, 3.7969e-08, 4.6193e-08, 6.3261e-08],
        [4.2130e-08, 3.7920e-08, 4.9298e-08, 6.8638e-08, 6.5272e-08, 3.3994e-08,
         3.8651e-08, 6.8532e-08, 7.9815e-08, 6.2375e-08, 5.1319e-08, 7.2693e-08,
         6.6531e-08, 3.7908e-08, 4.6120e-08, 6.3161e-08],
        [4.2539e-08, 3.8288e-08, 4.9776e-08, 6.9304e-08, 6.5905e-08, 3.4324e-08,
         3.9026e-08, 6.9197e-08, 8.0590e-08, 6.2981e-08, 5.1817e-08, 7.3398e-08,
         6.7177e-08, 3.8276e-08, 4.6567e-08, 6.3774e-08],
        [1.2339e-08, 1.1106e-08, 1.4438e-08, 2.0103e-08, 1.9117e-08, 9.9562e-09,
         1.1320e-08, 2.0071e-08, 2.3376e-08, 1.8268e-08, 1.5030e-08, 2.1290e-08,
         1.9485e-08, 1.1102e-08, 1.3507e-08, 1.8498e-08],
        [2.3237e-08, 2.0914e-08, 2.7190e-08, 3.7857e-08, 3.6000e-08, 1.8749e-08,
         2.1318e-08, 3.7798e-08, 4.4022e-08, 3.4403e-08, 2.8305e-08, 4.0093e-08,
         3.6695e-08, 2.0908e-08, 2.5437e-08, 3.4836e-08],
        [2.0348e-08, 1.8314e-08, 2.3810e-08, 3.3151e-08, 3.1525e-08, 1.6418e-08,
         1.8668e-08, 3.3099e-08, 3.8549e-08, 3.0126e-08, 2.4786e-08, 3.5109e-08,
         3.2133e-08, 1.8309e-08, 2.2275e-08, 3.0505e-08],
        [2.0383e-08, 1.8346e-08, 2.3850e-08, 3.3207e-08, 3.1579e-08, 1.6447e-08,
         1.8700e-08, 3.3156e-08, 3.8615e-08, 3.0177e-08, 2.4828e-08, 3.5169e-08,
         3.2188e-08, 1.8340e-08, 2.2313e-08, 3.0557e-08],
        [1.7596e-08, 1.5837e-08, 2.0589e-08, 2.8666e-08, 2.7260e-08, 1.4198e-08,
         1.6142e-08, 2.8622e-08, 3.3335e-08, 2.6051e-08, 2.1433e-08, 3.0360e-08,
         2.7786e-08, 1.5832e-08, 1.9262e-08, 2.6379e-08],
        [1.7108e-08, 1.5398e-08, 2.0018e-08, 2.7872e-08, 2.6505e-08, 1.3804e-08,
         1.5695e-08, 2.7829e-08, 3.2411e-08, 2.5329e-08, 2.0839e-08, 2.9518e-08,
         2.7016e-08, 1.5393e-08, 1.8728e-08, 2.5648e-08],
        [1.4880e-08, 1.3393e-08, 1.7411e-08, 2.4242e-08, 2.3053e-08, 1.2006e-08,
         1.3651e-08, 2.4204e-08, 2.8189e-08, 2.2030e-08, 1.8125e-08, 2.5674e-08,
         2.3498e-08, 1.3389e-08, 1.6289e-08, 2.2307e-08],
        [2.0051e-08, 1.8047e-08, 2.3462e-08, 3.2667e-08, 3.1064e-08, 1.6179e-08,
         1.8395e-08, 3.2616e-08, 3.7986e-08, 2.9686e-08, 2.4424e-08, 3.4596e-08,
         3.1664e-08, 1.8041e-08, 2.1950e-08, 3.0060e-08],
        [2.0050e-08, 1.8046e-08, 2.3460e-08, 3.2664e-08, 3.1062e-08, 1.6178e-08,
         1.8394e-08, 3.2614e-08, 3.7984e-08, 2.9684e-08, 2.4422e-08, 3.4594e-08,
         3.1662e-08, 1.8040e-08, 2.1948e-08, 3.0058e-08],
        [2.0091e-08, 1.8083e-08, 2.3509e-08, 3.2732e-08, 3.1126e-08, 1.6211e-08,
         1.8432e-08, 3.2681e-08, 3.8062e-08, 2.9745e-08, 2.4473e-08, 3.4665e-08,
         3.1727e-08, 1.8077e-08, 2.1993e-08, 3.0120e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(4397.2368, grad_fn=<NegBackward>) non event loss:  tensor([0.0365], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1482, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.4588e-07, 5.0971e-07, 2.6961e-07, 4.9077e-07, 2.6421e-07, 2.6954e-07,
         3.2582e-07],
        [6.9484e-08, 7.9431e-08, 4.2015e-08, 7.6479e-08, 4.1173e-08, 4.2003e-08,
         5.0775e-08],
        [2.1063e-08, 2.4079e-08, 1.2736e-08, 2.3184e-08, 1.2481e-08, 1.2733e-08,
         1.5392e-08],
        [1.5316e-08, 1.7509e-08, 9.2613e-09, 1.6858e-08, 9.0757e-09, 9.2587e-09,
         1.1192e-08],
        [1.5233e-08, 1.7413e-08, 9.2108e-09, 1.6766e-08, 9.0262e-09, 9.2082e-09,
         1.1131e-08],
        [6.2394e-09, 7.1325e-09, 3.7727e-09, 6.8675e-09, 3.6971e-09, 3.7717e-09,
         4.5593e-09],
        [5.8938e-09, 6.7375e-09, 3.5638e-09, 6.4871e-09, 3.4924e-09, 3.5628e-09,
         4.3068e-09]], grad_fn=<SoftplusBackward>)

  7%|██████████                                                                                                                                     | 35/496 [00:07<01:56,  3.95it/s]
-1
-1
### event lambdas:  tensor([[1.6393e-08, 2.9515e-08, 1.2017e-08, 1.3880e-08, 2.3563e-08],
        [2.3667e-08, 4.2613e-08, 1.7350e-08, 2.0039e-08, 3.4020e-08],
        [2.4164e-08, 4.3507e-08, 1.7714e-08, 2.0460e-08, 3.4734e-08],
        [7.1078e-09, 1.2797e-08, 5.2106e-09, 6.0183e-09, 1.0217e-08],
        [1.7773e-08, 3.2000e-08, 1.3029e-08, 1.5049e-08, 2.5547e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(445.7882, grad_fn=<NegBackward>) non event loss:  tensor([0.0120], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0981, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.6773e-07, 4.6729e-07, 9.3230e-07, 4.3646e-07, 7.6845e-07],
        [1.1961e-07, 1.1950e-07, 2.3841e-07, 1.1162e-07, 1.9652e-07],
        [1.3519e-07, 1.3506e-07, 2.6946e-07, 1.2615e-07, 2.2210e-07],
        [1.3095e-07, 1.3082e-07, 2.6101e-07, 1.2220e-07, 2.1514e-07],
        [1.3095e-07, 1.3082e-07, 2.6101e-07, 1.2219e-07, 2.1514e-07]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(384.5591, grad_fn=<NegBackward>) non event loss:  tensor([0.1004], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0743, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[6.2942e-07, 4.3535e-07, 6.9806e-07, 7.0123e-07, 4.5832e-07, 5.2400e-07,
         4.3479e-07, 5.5103e-07, 6.7796e-07],
        [7.9345e-08, 5.4880e-08, 8.7997e-08, 8.8398e-08, 5.7776e-08, 6.6056e-08,
         5.4810e-08, 6.9463e-08, 8.5464e-08],
        [4.9852e-08, 3.4481e-08, 5.5288e-08, 5.5539e-08, 3.6300e-08, 4.1502e-08,
         3.4436e-08, 4.3643e-08, 5.3696e-08],
        [1.5722e-07, 1.0874e-07, 1.7436e-07, 1.7515e-07, 1.1448e-07, 1.3088e-07,
         1.0860e-07, 1.3764e-07, 1.6934e-07],
        [1.6502e-07, 1.1414e-07, 1.8302e-07, 1.8385e-07, 1.2016e-07, 1.3738e-07,
         1.1399e-07, 1.4447e-07, 1.7775e-07],
        [1.6423e-07, 1.1359e-07, 1.8213e-07, 1.8296e-07, 1.1958e-07, 1.3672e-07,
         1.1344e-07, 1.4377e-07, 1.7689e-07],
        [6.3528e-08, 4.3940e-08, 7.0455e-08, 7.0776e-08, 4.6258e-08, 5.2888e-08,
         4.3884e-08, 5.5616e-08, 6.8427e-08],
        [1.4897e-07, 1.0304e-07, 1.6521e-07, 1.6596e-07, 1.0847e-07, 1.2402e-07,
         1.0290e-07, 1.3041e-07, 1.6046e-07],
        [1.2849e-07, 8.8873e-08, 1.4250e-07, 1.4315e-07, 9.3561e-08, 1.0697e-07,
         8.8758e-08, 1.1249e-07, 1.3840e-07]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1292.3285, grad_fn=<NegBackward>) non event loss:  tensor([0.1472], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1749, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.0801e-07, 1.3962e-07, 2.6712e-07, 1.5156e-07, 1.0471e-07, 2.7662e-07,
         9.3053e-08, 1.2948e-07, 1.7593e-07, 2.5055e-07, 1.7870e-07, 2.1268e-07,
         2.2064e-07, 2.8987e-07, 1.8069e-07, 2.1209e-07, 2.2930e-07, 2.4899e-07,
         2.3234e-07, 2.8655e-07, 2.1212e-07, 2.4899e-07, 1.2235e-07, 1.6835e-07,
         1.4607e-07, 1.9809e-07, 2.3396e-07, 2.7102e-07],
        [2.4010e-07, 3.1035e-07, 5.9376e-07, 3.3689e-07, 2.3275e-07, 6.1489e-07,
         2.0684e-07, 2.8782e-07, 3.9106e-07, 5.5694e-07, 3.9721e-07, 4.7276e-07,
         4.9044e-07, 6.4434e-07, 4.0163e-07, 4.7145e-07, 5.0969e-07, 5.5347e-07,
         5.1645e-07, 6.3695e-07, 4.7151e-07, 5.5347e-07, 2.7197e-07, 3.7421e-07,
         3.2469e-07, 4.4032e-07, 5.2005e-07, 6.0243e-07],
        [2.4783e-07, 3.2034e-07, 6.1288e-07, 3.4774e-07, 2.4024e-07, 6.3469e-07,
         2.1350e-07, 2.9709e-07, 4.0365e-07, 5.7487e-07, 4.1000e-07, 4.8798e-07,
         5.0623e-07, 6.6509e-07, 4.1456e-07, 4.8663e-07, 5.2611e-07, 5.7129e-07,
         5.3308e-07, 6.5745e-07, 4.8670e-07, 5.7129e-07, 2.8072e-07, 3.8626e-07,
         3.3515e-07, 4.5450e-07, 5.3680e-07, 6.2183e-07],
        [5.3230e-08, 6.8805e-08, 1.3164e-07, 7.4689e-08, 5.1600e-08, 1.3632e-07,
         4.5857e-08, 6.3809e-08, 8.6698e-08, 1.2347e-07, 8.8062e-08, 1.0481e-07,
         1.0873e-07, 1.4285e-07, 8.9042e-08, 1.0452e-07, 1.1300e-07, 1.2270e-07,
         1.1450e-07, 1.4121e-07, 1.0453e-07, 1.2270e-07, 6.0295e-08, 8.2962e-08,
         7.1984e-08, 9.7619e-08, 1.1530e-07, 1.3356e-07],
        [6.3985e-08, 8.2707e-08, 1.5823e-07, 8.9781e-08, 6.2026e-08, 1.6387e-07,
         5.5122e-08, 7.6703e-08, 1.0422e-07, 1.4842e-07, 1.0586e-07, 1.2599e-07,
         1.3070e-07, 1.7171e-07, 1.0703e-07, 1.2564e-07, 1.3583e-07, 1.4750e-07,
         1.3763e-07, 1.6974e-07, 1.2566e-07, 1.4750e-07, 7.2478e-08, 9.9725e-08,
         8.6529e-08, 1.1734e-07, 1.3859e-07, 1.6055e-07],
        [6.5751e-08, 8.4989e-08, 1.6260e-07, 9.2258e-08, 6.3738e-08, 1.6839e-07,
         5.6643e-08, 7.8819e-08, 1.0709e-07, 1.5252e-07, 1.0878e-07, 1.2946e-07,
         1.3431e-07, 1.7645e-07, 1.0999e-07, 1.2910e-07, 1.3958e-07, 1.5157e-07,
         1.4143e-07, 1.7443e-07, 1.2912e-07, 1.5157e-07, 7.4478e-08, 1.0248e-07,
         8.8916e-08, 1.2058e-07, 1.4242e-07, 1.6497e-07],
        [5.7288e-08, 7.4051e-08, 1.4167e-07, 8.0384e-08, 5.5534e-08, 1.4671e-07,
         4.9353e-08, 6.8674e-08, 9.3308e-08, 1.3289e-07, 9.4777e-08, 1.1280e-07,
         1.1702e-07, 1.5374e-07, 9.5831e-08, 1.1249e-07, 1.2161e-07, 1.3206e-07,
         1.2323e-07, 1.5198e-07, 1.1250e-07, 1.3206e-07, 6.4892e-08, 8.9287e-08,
         7.7473e-08, 1.0506e-07, 1.2409e-07, 1.4374e-07],
        [5.3740e-08, 6.9464e-08, 1.3290e-07, 7.5405e-08, 5.2094e-08, 1.3763e-07,
         4.6296e-08, 6.4421e-08, 8.7529e-08, 1.2466e-07, 8.8906e-08, 1.0581e-07,
         1.0977e-07, 1.4422e-07, 8.9895e-08, 1.0552e-07, 1.1408e-07, 1.2388e-07,
         1.1559e-07, 1.4256e-07, 1.0554e-07, 1.2388e-07, 6.0873e-08, 8.3757e-08,
         7.2674e-08, 9.8555e-08, 1.1640e-07, 1.3484e-07],
        [5.5621e-08, 7.1896e-08, 1.3755e-07, 7.8045e-08, 5.3918e-08, 1.4245e-07,
         4.7917e-08, 6.6676e-08, 9.0593e-08, 1.2902e-07, 9.2019e-08, 1.0952e-07,
         1.1362e-07, 1.4927e-07, 9.3042e-08, 1.0922e-07, 1.1808e-07, 1.2822e-07,
         1.1964e-07, 1.4756e-07, 1.0923e-07, 1.2822e-07, 6.3004e-08, 8.6689e-08,
         7.5218e-08, 1.0201e-07, 1.2048e-07, 1.3956e-07],
        [5.5769e-08, 7.2087e-08, 1.3792e-07, 7.8252e-08, 5.4061e-08, 1.4282e-07,
         4.8044e-08, 6.6853e-08, 9.0833e-08, 1.2936e-07, 9.2263e-08, 1.0981e-07,
         1.1392e-07, 1.4966e-07, 9.3289e-08, 1.0951e-07, 1.1839e-07, 1.2856e-07,
         1.1996e-07, 1.4795e-07, 1.0952e-07, 1.2856e-07, 6.3171e-08, 8.6919e-08,
         7.5418e-08, 1.0228e-07, 1.2080e-07, 1.3993e-07],
        [5.5212e-08, 7.1367e-08, 1.3654e-07, 7.7470e-08, 5.3522e-08, 1.4140e-07,
         4.7564e-08, 6.6185e-08, 8.9926e-08, 1.2807e-07, 9.1342e-08, 1.0871e-07,
         1.1278e-07, 1.4817e-07, 9.2357e-08, 1.0841e-07, 1.1721e-07, 1.2727e-07,
         1.1876e-07, 1.4647e-07, 1.0843e-07, 1.2727e-07, 6.2540e-08, 8.6051e-08,
         7.4665e-08, 1.0125e-07, 1.1959e-07, 1.3853e-07],
        [5.6086e-08, 7.2497e-08, 1.3870e-07, 7.8697e-08, 5.4369e-08, 1.4364e-07,
         4.8318e-08, 6.7234e-08, 9.1351e-08, 1.3010e-07, 9.2788e-08, 1.1043e-07,
         1.1457e-07, 1.5052e-07, 9.3820e-08, 1.1013e-07, 1.1906e-07, 1.2929e-07,
         1.2064e-07, 1.4879e-07, 1.1014e-07, 1.2929e-07, 6.3531e-08, 8.7414e-08,
         7.5847e-08, 1.0286e-07, 1.2148e-07, 1.4073e-07],
        [5.4967e-08, 7.1050e-08, 1.3593e-07, 7.7127e-08, 5.3284e-08, 1.4077e-07,
         4.7353e-08, 6.5892e-08, 8.9527e-08, 1.2750e-07, 9.0936e-08, 1.0823e-07,
         1.1228e-07, 1.4751e-07, 9.1948e-08, 1.0793e-07, 1.1669e-07, 1.2671e-07,
         1.1823e-07, 1.4582e-07, 1.0795e-07, 1.2671e-07, 6.2263e-08, 8.5669e-08,
         7.4333e-08, 1.0081e-07, 1.1906e-07, 1.3792e-07],
        [5.1112e-08, 6.6067e-08, 1.2640e-07, 7.1718e-08, 4.9547e-08, 1.3090e-07,
         4.4032e-08, 6.1270e-08, 8.3249e-08, 1.1856e-07, 8.4559e-08, 1.0064e-07,
         1.0440e-07, 1.3717e-07, 8.5499e-08, 1.0036e-07, 1.0850e-07, 1.1782e-07,
         1.0994e-07, 1.3559e-07, 1.0038e-07, 1.1782e-07, 5.7896e-08, 7.9661e-08,
         6.9120e-08, 9.3735e-08, 1.1071e-07, 1.2824e-07],
        [4.7409e-08, 6.1281e-08, 1.1724e-07, 6.6522e-08, 4.5958e-08, 1.2141e-07,
         4.0842e-08, 5.6832e-08, 7.7218e-08, 1.0997e-07, 7.8433e-08, 9.3349e-08,
         9.6841e-08, 1.2723e-07, 7.9305e-08, 9.3091e-08, 1.0064e-07, 1.0929e-07,
         1.0198e-07, 1.2577e-07, 9.3104e-08, 1.0929e-07, 5.3702e-08, 7.3890e-08,
         6.4113e-08, 8.6945e-08, 1.0269e-07, 1.1895e-07],
        [4.6856e-08, 6.0566e-08, 1.1587e-07, 6.5746e-08, 4.5422e-08, 1.2000e-07,
         4.0366e-08, 5.6169e-08, 7.6317e-08, 1.0869e-07, 7.7518e-08, 9.2260e-08,
         9.5711e-08, 1.2575e-07, 7.8380e-08, 9.2005e-08, 9.9469e-08, 1.0801e-07,
         1.0079e-07, 1.2430e-07, 9.2017e-08, 1.0801e-07, 5.3075e-08, 7.3028e-08,
         6.3365e-08, 8.5930e-08, 1.0149e-07, 1.1757e-07],
        [4.6511e-08, 6.0120e-08, 1.1502e-07, 6.5262e-08, 4.5087e-08, 1.1911e-07,
         4.0069e-08, 5.5756e-08, 7.5755e-08, 1.0789e-07, 7.6948e-08, 9.1581e-08,
         9.5007e-08, 1.2482e-07, 7.7803e-08, 9.1328e-08, 9.8737e-08, 1.0722e-07,
         1.0005e-07, 1.2339e-07, 9.1340e-08, 1.0722e-07, 5.2685e-08, 7.2491e-08,
         6.2899e-08, 8.5298e-08, 1.0074e-07, 1.1670e-07],
        [4.6998e-08, 6.0749e-08, 1.1622e-07, 6.5945e-08, 4.5559e-08, 1.2036e-07,
         4.0488e-08, 5.6339e-08, 7.6548e-08, 1.0902e-07, 7.7752e-08, 9.2539e-08,
         9.6001e-08, 1.2613e-07, 7.8617e-08, 9.2283e-08, 9.9769e-08, 1.0834e-07,
         1.0109e-07, 1.2468e-07, 9.2296e-08, 1.0834e-07, 5.3236e-08, 7.3249e-08,
         6.3556e-08, 8.6190e-08, 1.0180e-07, 1.1792e-07],
        [1.3403e-07, 1.7325e-07, 3.3145e-07, 1.8806e-07, 1.2993e-07, 3.4325e-07,
         1.1546e-07, 1.6067e-07, 2.1830e-07, 3.1090e-07, 2.2174e-07, 2.6391e-07,
         2.7378e-07, 3.5969e-07, 2.2420e-07, 2.6317e-07, 2.8452e-07, 3.0896e-07,
         2.8830e-07, 3.5556e-07, 2.6321e-07, 3.0896e-07, 1.5182e-07, 2.0889e-07,
         1.8125e-07, 2.4580e-07, 2.9031e-07, 3.3629e-07],
        [8.9830e-08, 1.1611e-07, 2.2215e-07, 1.2604e-07, 8.7080e-08, 2.3005e-07,
         7.7387e-08, 1.0768e-07, 1.4631e-07, 2.0837e-07, 1.4861e-07, 1.7688e-07,
         1.8349e-07, 2.4107e-07, 1.5027e-07, 1.7639e-07, 1.9070e-07, 2.0708e-07,
         1.9322e-07, 2.3831e-07, 1.7641e-07, 2.0708e-07, 1.0175e-07, 1.4001e-07,
         1.2148e-07, 1.6474e-07, 1.9457e-07, 2.2539e-07],
        [5.3392e-08, 6.9014e-08, 1.3204e-07, 7.4916e-08, 5.1757e-08, 1.3674e-07,
         4.5996e-08, 6.4004e-08, 8.6962e-08, 1.2385e-07, 8.8330e-08, 1.0513e-07,
         1.0906e-07, 1.4328e-07, 8.9313e-08, 1.0484e-07, 1.1334e-07, 1.2308e-07,
         1.1485e-07, 1.4164e-07, 1.0485e-07, 1.2308e-07, 6.0479e-08, 8.3215e-08,
         7.2203e-08, 9.7917e-08, 1.1565e-07, 1.3397e-07],
        [5.3117e-08, 6.8659e-08, 1.3136e-07, 7.4531e-08, 5.1491e-08, 1.3603e-07,
         4.5760e-08, 6.3675e-08, 8.6515e-08, 1.2321e-07, 8.7876e-08, 1.0459e-07,
         1.0850e-07, 1.4255e-07, 8.8854e-08, 1.0430e-07, 1.1276e-07, 1.2245e-07,
         1.1426e-07, 1.4091e-07, 1.0431e-07, 1.2245e-07, 6.0168e-08, 8.2787e-08,
         7.1832e-08, 9.7413e-08, 1.1505e-07, 1.3328e-07],
        [3.8653e-08, 4.9963e-08, 9.5589e-08, 5.4236e-08, 3.7470e-08, 9.8991e-08,
         3.3299e-08, 4.6336e-08, 6.2957e-08, 8.9661e-08, 6.3948e-08, 7.6109e-08,
         7.8956e-08, 1.0373e-07, 6.4659e-08, 7.5898e-08, 8.2055e-08, 8.9103e-08,
         8.3143e-08, 1.0254e-07, 7.5909e-08, 8.9103e-08, 4.3784e-08, 6.0244e-08,
         5.2272e-08, 7.0887e-08, 8.3724e-08, 9.6985e-08],
        [4.6746e-08, 6.0424e-08, 1.1560e-07, 6.5591e-08, 4.5315e-08, 1.1972e-07,
         4.0271e-08, 5.6037e-08, 7.6137e-08, 1.0843e-07, 7.7336e-08, 9.2043e-08,
         9.5486e-08, 1.2545e-07, 7.8196e-08, 9.1788e-08, 9.9235e-08, 1.0776e-07,
         1.0055e-07, 1.2401e-07, 9.1801e-08, 1.0776e-07, 5.2951e-08, 7.2856e-08,
         6.3216e-08, 8.5728e-08, 1.0125e-07, 1.1729e-07],
        [4.6901e-08, 6.0624e-08, 1.1599e-07, 6.5809e-08, 4.5465e-08, 1.2011e-07,
         4.0405e-08, 5.6223e-08, 7.6390e-08, 1.0879e-07, 7.7592e-08, 9.2349e-08,
         9.5803e-08, 1.2587e-07, 7.8455e-08, 9.2093e-08, 9.9564e-08, 1.0812e-07,
         1.0088e-07, 1.2442e-07, 9.2106e-08, 1.0812e-07, 5.3126e-08, 7.3098e-08,
         6.3426e-08, 8.6013e-08, 1.0159e-07, 1.1768e-07],
        [2.6769e-08, 3.4602e-08, 6.6200e-08, 3.7561e-08, 2.5950e-08, 6.8556e-08,
         2.3062e-08, 3.2090e-08, 4.3601e-08, 6.2095e-08, 4.4287e-08, 5.2709e-08,
         5.4681e-08, 7.1840e-08, 4.4780e-08, 5.2563e-08, 5.6828e-08, 6.1709e-08,
         5.7581e-08, 7.1016e-08, 5.2571e-08, 6.1709e-08, 3.0323e-08, 4.1722e-08,
         3.6201e-08, 4.9093e-08, 5.7983e-08, 6.7167e-08],
        [3.6297e-08, 4.6917e-08, 8.9761e-08, 5.0930e-08, 3.5186e-08, 9.2955e-08,
         3.1269e-08, 4.3511e-08, 5.9118e-08, 8.4195e-08, 6.0049e-08, 7.1469e-08,
         7.4142e-08, 9.7408e-08, 6.0717e-08, 7.1271e-08, 7.7053e-08, 8.3671e-08,
         7.8074e-08, 9.6290e-08, 7.1281e-08, 8.3671e-08, 4.1114e-08, 5.6571e-08,
         4.9085e-08, 6.6565e-08, 7.8619e-08, 9.1072e-08],
        [3.7199e-08, 4.8083e-08, 9.1992e-08, 5.2196e-08, 3.6060e-08, 9.5266e-08,
         3.2046e-08, 4.4592e-08, 6.0588e-08, 8.6288e-08, 6.1542e-08, 7.3245e-08,
         7.5985e-08, 9.9829e-08, 6.2226e-08, 7.3043e-08, 7.8968e-08, 8.5751e-08,
         8.0015e-08, 9.8684e-08, 7.3053e-08, 8.5751e-08, 4.2137e-08, 5.7977e-08,
         5.0305e-08, 6.8220e-08, 8.0573e-08, 9.3336e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(12593.5234, grad_fn=<NegBackward>) non event loss:  tensor([0.3515], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.3060, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.4869e-06, 4.9592e-06, 2.0061e-06, 3.1088e-06, 3.8130e-06, 4.6028e-06,
         3.0168e-06, 3.9470e-06, 4.7111e-06, 3.4302e-06, 3.7890e-06, 2.4785e-06,
         2.8424e-06, 4.0086e-06, 4.4116e-06, 1.8202e-06, 2.0945e-06, 3.5999e-06,
         1.7000e-06, 4.9067e-06, 3.8535e-06, 2.1250e-06, 4.8100e-06, 4.0621e-06,
         2.8054e-06, 3.9884e-06, 2.5583e-06],
        [1.1162e-06, 1.2337e-06, 4.9908e-07, 7.7339e-07, 9.4859e-07, 1.1451e-06,
         7.5050e-07, 9.8192e-07, 1.1720e-06, 8.5334e-07, 9.4261e-07, 6.1659e-07,
         7.0712e-07, 9.9724e-07, 1.0975e-06, 4.5282e-07, 5.2107e-07, 8.9556e-07,
         4.2292e-07, 1.2207e-06, 9.5865e-07, 5.2866e-07, 1.1966e-06, 1.0105e-06,
         6.9791e-07, 9.9221e-07, 6.3644e-07],
        [8.6630e-07, 9.5750e-07, 3.8733e-07, 6.0022e-07, 7.3620e-07, 8.8867e-07,
         5.8246e-07, 7.6206e-07, 9.0959e-07, 6.6227e-07, 7.3155e-07, 4.7853e-07,
         5.4879e-07, 7.7395e-07, 8.5176e-07, 3.5143e-07, 4.0440e-07, 6.9504e-07,
         3.2822e-07, 9.4736e-07, 7.4400e-07, 4.1029e-07, 9.2869e-07, 7.8428e-07,
         5.4164e-07, 7.7005e-07, 4.9394e-07],
        [8.4240e-07, 9.3108e-07, 3.7665e-07, 5.8366e-07, 7.1589e-07, 8.6416e-07,
         5.6639e-07, 7.4104e-07, 8.8450e-07, 6.4400e-07, 7.1137e-07, 4.6533e-07,
         5.3365e-07, 7.5260e-07, 8.2827e-07, 3.4174e-07, 3.9324e-07, 6.7586e-07,
         3.1917e-07, 9.2123e-07, 7.2348e-07, 3.9897e-07, 9.0307e-07, 7.6264e-07,
         5.2670e-07, 7.4881e-07, 4.8031e-07],
        [1.2377e-07, 1.3680e-07, 5.5337e-08, 8.5752e-08, 1.0518e-07, 1.2696e-07,
         8.3215e-08, 1.0887e-07, 1.2995e-07, 9.4617e-08, 1.0451e-07, 6.8366e-08,
         7.8404e-08, 1.1057e-07, 1.2169e-07, 5.0208e-08, 5.7775e-08, 9.9298e-08,
         4.6893e-08, 1.3535e-07, 1.0629e-07, 5.8617e-08, 1.3268e-07, 1.1205e-07,
         7.7383e-08, 1.1002e-07, 7.0568e-08],
        [2.5997e-07, 2.8734e-07, 1.1624e-07, 1.8012e-07, 2.2093e-07, 2.6669e-07,
         1.7479e-07, 2.2869e-07, 2.7296e-07, 1.9874e-07, 2.1953e-07, 1.4360e-07,
         1.6469e-07, 2.3226e-07, 2.5561e-07, 1.0546e-07, 1.2136e-07, 2.0858e-07,
         9.8498e-08, 2.8430e-07, 2.2327e-07, 1.2312e-07, 2.7869e-07, 2.3536e-07,
         1.6254e-07, 2.3109e-07, 1.4823e-07],
        [2.6926e-07, 2.9760e-07, 1.2039e-07, 1.8656e-07, 2.2882e-07, 2.7621e-07,
         1.8104e-07, 2.3686e-07, 2.8271e-07, 2.0584e-07, 2.2738e-07, 1.4873e-07,
         1.7057e-07, 2.4055e-07, 2.6474e-07, 1.0923e-07, 1.2569e-07, 2.1603e-07,
         1.0202e-07, 2.9445e-07, 2.3125e-07, 1.2752e-07, 2.8865e-07, 2.4377e-07,
         1.6835e-07, 2.3934e-07, 1.5352e-07],
        [2.6193e-07, 2.8950e-07, 1.1711e-07, 1.8148e-07, 2.2259e-07, 2.6869e-07,
         1.7611e-07, 2.3041e-07, 2.7502e-07, 2.0024e-07, 2.2119e-07, 1.4468e-07,
         1.6593e-07, 2.3401e-07, 2.5753e-07, 1.0626e-07, 1.2227e-07, 2.1015e-07,
         9.9240e-08, 2.8644e-07, 2.2495e-07, 1.2405e-07, 2.8079e-07, 2.3713e-07,
         1.6377e-07, 2.3283e-07, 1.4934e-07],
        [2.6135e-07, 2.8886e-07, 1.1685e-07, 1.8108e-07, 2.2210e-07, 2.6810e-07,
         1.7572e-07, 2.2990e-07, 2.7441e-07, 1.9980e-07, 2.2070e-07, 1.4437e-07,
         1.6556e-07, 2.3349e-07, 2.5697e-07, 1.0602e-07, 1.2200e-07, 2.0968e-07,
         9.9021e-08, 2.8581e-07, 2.2446e-07, 1.2378e-07, 2.8017e-07, 2.3661e-07,
         1.6341e-07, 2.3231e-07, 1.4901e-07],
        [2.6362e-07, 2.9138e-07, 1.1787e-07, 1.8265e-07, 2.2403e-07, 2.7043e-07,
         1.7725e-07, 2.3190e-07, 2.7680e-07, 2.0154e-07, 2.2262e-07, 1.4562e-07,
         1.6700e-07, 2.3552e-07, 2.5920e-07, 1.0694e-07, 1.2306e-07, 2.1151e-07,
         9.9882e-08, 2.8829e-07, 2.2641e-07, 1.2485e-07, 2.8261e-07, 2.3866e-07,
         1.6483e-07, 2.3433e-07, 1.5031e-07],
        [2.6213e-07, 2.8972e-07, 1.1720e-07, 1.8162e-07, 2.2276e-07, 2.6890e-07,
         1.7624e-07, 2.3059e-07, 2.7523e-07, 2.0039e-07, 2.2136e-07, 1.4480e-07,
         1.6606e-07, 2.3419e-07, 2.5773e-07, 1.0634e-07, 1.2236e-07, 2.1031e-07,
         9.9316e-08, 2.8666e-07, 2.2512e-07, 1.2415e-07, 2.8101e-07, 2.3731e-07,
         1.6389e-07, 2.3301e-07, 1.4946e-07],
        [2.5557e-07, 2.8247e-07, 1.1427e-07, 1.7707e-07, 2.1719e-07, 2.6217e-07,
         1.7183e-07, 2.2482e-07, 2.6834e-07, 1.9538e-07, 2.1581e-07, 1.4117e-07,
         1.6190e-07, 2.2832e-07, 2.5128e-07, 1.0368e-07, 1.1930e-07, 2.0504e-07,
         9.6829e-08, 2.7948e-07, 2.1949e-07, 1.2104e-07, 2.7397e-07, 2.3137e-07,
         1.5979e-07, 2.2717e-07, 1.4572e-07],
        [2.4531e-07, 2.7114e-07, 1.0968e-07, 1.6997e-07, 2.0847e-07, 2.5165e-07,
         1.6494e-07, 2.1579e-07, 2.5757e-07, 1.8754e-07, 2.0716e-07, 1.3551e-07,
         1.5540e-07, 2.1916e-07, 2.4120e-07, 9.9516e-08, 1.1451e-07, 1.9682e-07,
         9.2944e-08, 2.6827e-07, 2.1068e-07, 1.1618e-07, 2.6298e-07, 2.2209e-07,
         1.5338e-07, 2.1806e-07, 1.3987e-07],
        [2.6556e-07, 2.9352e-07, 1.1874e-07, 1.8400e-07, 2.2568e-07, 2.7242e-07,
         1.7855e-07, 2.3361e-07, 2.7883e-07, 2.0302e-07, 2.2426e-07, 1.4669e-07,
         1.6823e-07, 2.3725e-07, 2.6111e-07, 1.0773e-07, 1.2397e-07, 2.1306e-07,
         1.0062e-07, 2.9041e-07, 2.2807e-07, 1.2577e-07, 2.8469e-07, 2.4042e-07,
         1.6604e-07, 2.3606e-07, 1.5142e-07],
        [2.9066e-07, 3.2126e-07, 1.2996e-07, 2.0138e-07, 2.4701e-07, 2.9817e-07,
         1.9543e-07, 2.5568e-07, 3.0518e-07, 2.2220e-07, 2.4545e-07, 1.6055e-07,
         1.8413e-07, 2.5967e-07, 2.8578e-07, 1.1791e-07, 1.3568e-07, 2.3320e-07,
         1.1013e-07, 3.1786e-07, 2.4963e-07, 1.3766e-07, 3.1159e-07, 2.6314e-07,
         1.8173e-07, 2.5837e-07, 1.6573e-07],
        [3.8326e-07, 4.2361e-07, 1.7136e-07, 2.6554e-07, 3.2570e-07, 3.9316e-07,
         2.5769e-07, 3.3715e-07, 4.0242e-07, 2.9300e-07, 3.2365e-07, 2.1171e-07,
         2.4279e-07, 3.4241e-07, 3.7683e-07, 1.5548e-07, 1.7891e-07, 3.0749e-07,
         1.4521e-07, 4.1913e-07, 3.2916e-07, 1.8152e-07, 4.1086e-07, 3.4698e-07,
         2.3963e-07, 3.4068e-07, 2.1852e-07],
        [3.7319e-07, 4.1248e-07, 1.6686e-07, 2.5857e-07, 3.1714e-07, 3.8283e-07,
         2.5092e-07, 3.2829e-07, 3.9184e-07, 2.8530e-07, 3.1514e-07, 2.0614e-07,
         2.3641e-07, 3.3341e-07, 3.6693e-07, 1.5139e-07, 1.7421e-07, 2.9941e-07,
         1.4139e-07, 4.0811e-07, 3.2051e-07, 1.7675e-07, 4.0007e-07, 3.3786e-07,
         2.3333e-07, 3.3173e-07, 2.1278e-07],
        [2.7003e-07, 2.9846e-07, 1.2073e-07, 1.8709e-07, 2.2948e-07, 2.7701e-07,
         1.8156e-07, 2.3754e-07, 2.8353e-07, 2.0644e-07, 2.2803e-07, 1.4916e-07,
         1.7106e-07, 2.4125e-07, 2.6550e-07, 1.0954e-07, 1.2605e-07, 2.1665e-07,
         1.0231e-07, 2.9530e-07, 2.3191e-07, 1.2789e-07, 2.8948e-07, 2.4447e-07,
         1.6884e-07, 2.4003e-07, 1.5397e-07],
        [2.7416e-07, 3.0302e-07, 1.2258e-07, 1.8995e-07, 2.3299e-07, 2.8124e-07,
         1.8433e-07, 2.4117e-07, 2.8786e-07, 2.0959e-07, 2.3152e-07, 1.5144e-07,
         1.7368e-07, 2.4493e-07, 2.6956e-07, 1.1122e-07, 1.2798e-07, 2.1996e-07,
         1.0387e-07, 2.9981e-07, 2.3546e-07, 1.2984e-07, 2.9391e-07, 2.4820e-07,
         1.7142e-07, 2.4370e-07, 1.5632e-07],
        [9.8547e-08, 1.0892e-07, 4.4061e-08, 6.8279e-08, 8.3747e-08, 1.0109e-07,
         6.6258e-08, 8.6689e-08, 1.0347e-07, 7.5337e-08, 8.3218e-08, 5.4435e-08,
         6.2428e-08, 8.8041e-08, 9.6893e-08, 3.9977e-08, 4.6003e-08, 7.9065e-08,
         3.7338e-08, 1.0777e-07, 8.4635e-08, 4.6673e-08, 1.0564e-07, 8.9216e-08,
         6.1615e-08, 8.7598e-08, 5.6189e-08],
        [3.5121e-07, 3.8818e-07, 1.5703e-07, 2.4334e-07, 2.9847e-07, 3.6028e-07,
         2.3614e-07, 3.0895e-07, 3.6876e-07, 2.6850e-07, 2.9658e-07, 1.9400e-07,
         2.2249e-07, 3.1377e-07, 3.4532e-07, 1.4248e-07, 1.6395e-07, 2.8178e-07,
         1.3307e-07, 3.8408e-07, 3.0163e-07, 1.6634e-07, 3.7651e-07, 3.1796e-07,
         2.1959e-07, 3.1219e-07, 2.0025e-07],
        [3.6456e-07, 4.0294e-07, 1.6300e-07, 2.5259e-07, 3.0981e-07, 3.7398e-07,
         2.4511e-07, 3.2069e-07, 3.8278e-07, 2.7870e-07, 3.0785e-07, 2.0138e-07,
         2.3094e-07, 3.2570e-07, 3.5844e-07, 1.4789e-07, 1.7018e-07, 2.9249e-07,
         1.3812e-07, 3.9867e-07, 3.1310e-07, 1.7266e-07, 3.9082e-07, 3.3004e-07,
         2.2794e-07, 3.2406e-07, 2.0786e-07],
        [1.9569e-07, 2.1629e-07, 8.7495e-08, 1.3558e-07, 1.6630e-07, 2.0074e-07,
         1.3157e-07, 1.7214e-07, 2.0547e-07, 1.4960e-07, 1.6525e-07, 1.0810e-07,
         1.2397e-07, 1.7483e-07, 1.9241e-07, 7.9385e-08, 9.1350e-08, 1.5700e-07,
         7.4143e-08, 2.1400e-07, 1.6806e-07, 9.2680e-08, 2.0978e-07, 1.7716e-07,
         1.2235e-07, 1.7395e-07, 1.1158e-07],
        [3.7078e-07, 4.0981e-07, 1.6578e-07, 2.5689e-07, 3.1509e-07, 3.8035e-07,
         2.4929e-07, 3.2616e-07, 3.8931e-07, 2.8345e-07, 3.1310e-07, 2.0481e-07,
         2.3488e-07, 3.3125e-07, 3.6456e-07, 1.5041e-07, 1.7308e-07, 2.9748e-07,
         1.4048e-07, 4.0547e-07, 3.1843e-07, 1.7560e-07, 3.9748e-07, 3.3567e-07,
         2.3182e-07, 3.2958e-07, 2.1141e-07],
        [1.7017e-07, 1.8809e-07, 7.6086e-08, 1.1791e-07, 1.4462e-07, 1.7457e-07,
         1.1442e-07, 1.4970e-07, 1.7868e-07, 1.3009e-07, 1.4370e-07, 9.4000e-08,
         1.0780e-07, 1.5203e-07, 1.6732e-07, 6.9034e-08, 7.9439e-08, 1.3653e-07,
         6.4475e-08, 1.8610e-07, 1.4615e-07, 8.0595e-08, 1.8243e-07, 1.5406e-07,
         1.0640e-07, 1.5127e-07, 9.7028e-08],
        [1.7580e-07, 1.9431e-07, 7.8604e-08, 1.2181e-07, 1.4940e-07, 1.8035e-07,
         1.1820e-07, 1.5465e-07, 1.8459e-07, 1.3440e-07, 1.4846e-07, 9.7111e-08,
         1.1137e-07, 1.5706e-07, 1.7285e-07, 7.1318e-08, 8.2067e-08, 1.4105e-07,
         6.6609e-08, 1.9225e-07, 1.5099e-07, 8.3262e-08, 1.8847e-07, 1.5916e-07,
         1.0992e-07, 1.5627e-07, 1.0024e-07],
        [1.4498e-07, 1.6024e-07, 6.4822e-08, 1.0045e-07, 1.2321e-07, 1.4873e-07,
         9.7478e-08, 1.2754e-07, 1.5223e-07, 1.1084e-07, 1.2243e-07, 8.0085e-08,
         9.1844e-08, 1.2953e-07, 1.4255e-07, 5.8814e-08, 6.7679e-08, 1.1632e-07,
         5.4931e-08, 1.5855e-07, 1.2451e-07, 6.8664e-08, 1.5542e-07, 1.3125e-07,
         9.0648e-08, 1.2887e-07, 8.2664e-08]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(11131.7588, grad_fn=<NegBackward>) non event loss:  tensor([0.5294], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.2982, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.1800e-06, 1.3283e-06, 1.3297e-06, 1.2994e-06, 1.4450e-06],
        [2.8007e-07, 3.1528e-07, 3.1560e-07, 3.0843e-07, 3.4297e-07],
        [1.3424e-06, 1.5111e-06, 1.5127e-06, 1.4783e-06, 1.6439e-06],
        [1.4164e-06, 1.5944e-06, 1.5961e-06, 1.5598e-06, 1.7345e-06],
        [6.5667e-07, 7.3921e-07, 7.3998e-07, 7.2315e-07, 8.0415e-07]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(347.1295, grad_fn=<NegBackward>) non event loss:  tensor([0.1232], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0765, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.7425e-05, 2.2120e-05, 1.1486e-05, 2.7581e-05, 1.6479e-05, 1.2298e-05,
         2.3063e-05],
        [1.8499e-06, 2.3484e-06, 1.2194e-06, 2.9282e-06, 1.7496e-06, 1.3056e-06,
         2.4486e-06],
        [3.3167e-06, 4.2104e-06, 2.1863e-06, 5.2499e-06, 3.1368e-06, 2.3408e-06,
         4.3901e-06],
        [3.3067e-06, 4.1977e-06, 2.1797e-06, 5.2341e-06, 3.1273e-06, 2.3338e-06,
         4.3768e-06],
        [2.1836e-06, 2.7720e-06, 1.4394e-06, 3.4564e-06, 2.0652e-06, 1.5412e-06,
         2.8903e-06],
        [2.0757e-06, 2.6350e-06, 1.3683e-06, 3.2856e-06, 1.9631e-06, 1.4650e-06,
         2.7474e-06],
        [2.3851e-06, 3.0278e-06, 1.5722e-06, 3.7753e-06, 2.2557e-06, 1.6833e-06,
         3.1570e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(618.1202, grad_fn=<NegBackward>) non event loss:  tensor([1.7748], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0497, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.9296e-05, 4.9620e-05, 4.8963e-05, 3.8423e-05, 3.1843e-05, 2.5908e-05,
         3.9270e-05, 5.0586e-05, 4.5269e-05, 4.2215e-05, 4.1763e-05, 7.0228e-05,
         2.2418e-05, 3.7112e-05, 5.7529e-05, 2.7342e-05, 4.0224e-05, 3.7089e-05,
         2.2860e-05, 6.1333e-05, 6.0819e-05, 6.1828e-05, 2.9091e-05, 2.7062e-05,
         4.2296e-05, 3.0374e-05],
        [2.4638e-05, 3.1110e-05, 3.0698e-05, 2.4090e-05, 1.9965e-05, 1.6244e-05,
         2.4621e-05, 3.1717e-05, 2.8383e-05, 2.6468e-05, 2.6184e-05, 4.4031e-05,
         1.4055e-05, 2.3268e-05, 3.6070e-05, 1.7143e-05, 2.5220e-05, 2.3254e-05,
         1.4333e-05, 3.8455e-05, 3.8132e-05, 3.8765e-05, 1.8239e-05, 1.6967e-05,
         2.6519e-05, 1.9044e-05],
        [1.9730e-05, 2.4914e-05, 2.4584e-05, 1.9292e-05, 1.5988e-05, 1.3008e-05,
         1.9717e-05, 2.5399e-05, 2.2729e-05, 2.1196e-05, 2.0969e-05, 3.5261e-05,
         1.1256e-05, 1.8634e-05, 2.8885e-05, 1.3728e-05, 2.0196e-05, 1.8622e-05,
         1.1478e-05, 3.0795e-05, 3.0537e-05, 3.1044e-05, 1.4606e-05, 1.3588e-05,
         2.1237e-05, 1.5251e-05],
        [2.7807e-05, 3.5112e-05, 3.4647e-05, 2.7189e-05, 2.2533e-05, 1.8333e-05,
         2.7789e-05, 3.5796e-05, 3.2033e-05, 2.9872e-05, 2.9553e-05, 4.9695e-05,
         1.5863e-05, 2.6261e-05, 4.0709e-05, 1.9348e-05, 2.8464e-05, 2.6246e-05,
         1.6176e-05, 4.3401e-05, 4.3037e-05, 4.3751e-05, 2.0586e-05, 1.9150e-05,
         2.9930e-05, 2.1493e-05],
        [1.1383e-05, 1.4374e-05, 1.4183e-05, 1.1130e-05, 9.2240e-06, 7.5048e-06,
         1.1375e-05, 1.4654e-05, 1.3113e-05, 1.2228e-05, 1.2098e-05, 2.0343e-05,
         6.4938e-06, 1.0750e-05, 1.6665e-05, 7.9203e-06, 1.1652e-05, 1.0744e-05,
         6.6219e-06, 1.7767e-05, 1.7618e-05, 1.7910e-05, 8.4269e-06, 7.8392e-06,
         1.2252e-05, 8.7985e-06],
        [7.6820e-06, 9.7002e-06, 9.5718e-06, 7.5113e-06, 6.2250e-06, 5.0648e-06,
         7.6769e-06, 9.8892e-06, 8.8496e-06, 8.2526e-06, 8.1643e-06, 1.3729e-05,
         4.3825e-06, 7.2551e-06, 1.1247e-05, 5.3452e-06, 7.8635e-06, 7.2506e-06,
         4.4689e-06, 1.1990e-05, 1.1890e-05, 1.2087e-05, 5.6870e-06, 5.2904e-06,
         8.2685e-06, 5.9378e-06],
        [8.9046e-06, 1.1244e-05, 1.1095e-05, 8.7068e-06, 7.2157e-06, 5.8709e-06,
         8.8988e-06, 1.1463e-05, 1.0258e-05, 9.5660e-06, 9.4637e-06, 1.5914e-05,
         5.0800e-06, 8.4098e-06, 1.3036e-05, 6.1959e-06, 9.1150e-06, 8.4046e-06,
         5.1802e-06, 1.3898e-05, 1.3782e-05, 1.4011e-05, 6.5922e-06, 6.1324e-06,
         9.5845e-06, 6.8828e-06],
        [9.3793e-06, 1.1844e-05, 1.1687e-05, 9.1710e-06, 7.6004e-06, 6.1838e-06,
         9.3732e-06, 1.2074e-05, 1.0805e-05, 1.0076e-05, 9.9682e-06, 1.6763e-05,
         5.3508e-06, 8.8581e-06, 1.3731e-05, 6.5262e-06, 9.6010e-06, 8.8527e-06,
         5.4563e-06, 1.4639e-05, 1.4517e-05, 1.4758e-05, 6.9436e-06, 6.4594e-06,
         1.0095e-05, 7.2498e-06],
        [9.3203e-06, 1.1769e-05, 1.1613e-05, 9.1133e-06, 7.5526e-06, 6.1449e-06,
         9.3142e-06, 1.1998e-05, 1.0737e-05, 1.0013e-05, 9.9054e-06, 1.6657e-05,
         5.3171e-06, 8.8023e-06, 1.3645e-05, 6.4851e-06, 9.5406e-06, 8.7970e-06,
         5.4220e-06, 1.4547e-05, 1.4425e-05, 1.4665e-05, 6.8999e-06, 6.4187e-06,
         1.0032e-05, 7.2042e-06],
        [4.3924e-06, 5.5465e-06, 5.4730e-06, 4.2949e-06, 3.5594e-06, 2.8960e-06,
         4.3896e-06, 5.6545e-06, 5.0601e-06, 4.7187e-06, 4.6682e-06, 7.8501e-06,
         2.5058e-06, 4.1484e-06, 6.4306e-06, 3.0563e-06, 4.4962e-06, 4.1458e-06,
         2.5553e-06, 6.8558e-06, 6.7983e-06, 6.9112e-06, 3.2518e-06, 3.0250e-06,
         4.7278e-06, 3.3952e-06],
        [4.3273e-06, 5.4642e-06, 5.3918e-06, 4.2312e-06, 3.5065e-06, 2.8530e-06,
         4.3244e-06, 5.5706e-06, 4.9850e-06, 4.6487e-06, 4.5989e-06, 7.7336e-06,
         2.4687e-06, 4.0868e-06, 6.3352e-06, 3.0110e-06, 4.4295e-06, 4.0843e-06,
         2.5173e-06, 6.7541e-06, 6.6974e-06, 6.8086e-06, 3.2035e-06, 2.9801e-06,
         4.6577e-06, 3.3448e-06],
        [4.3015e-06, 5.4316e-06, 5.3597e-06, 4.2059e-06, 3.4856e-06, 2.8360e-06,
         4.2987e-06, 5.5374e-06, 4.9553e-06, 4.6210e-06, 4.5715e-06, 7.6875e-06,
         2.4539e-06, 4.0624e-06, 6.2974e-06, 2.9930e-06, 4.4031e-06, 4.0600e-06,
         2.5023e-06, 6.7138e-06, 6.6575e-06, 6.7680e-06, 3.1844e-06, 2.9623e-06,
         4.6299e-06, 3.3248e-06],
        [1.5127e-05, 1.9101e-05, 1.8848e-05, 1.4791e-05, 1.2258e-05, 9.9731e-06,
         1.5117e-05, 1.9473e-05, 1.7426e-05, 1.6250e-05, 1.6076e-05, 2.7034e-05,
         8.6296e-06, 1.4286e-05, 2.2146e-05, 1.0525e-05, 1.5484e-05, 1.4277e-05,
         8.7998e-06, 2.3610e-05, 2.3412e-05, 2.3801e-05, 1.1198e-05, 1.0417e-05,
         1.6282e-05, 1.1692e-05],
        [1.5201e-05, 1.9194e-05, 1.8940e-05, 1.4863e-05, 1.2318e-05, 1.0022e-05,
         1.5191e-05, 1.9568e-05, 1.7511e-05, 1.6330e-05, 1.6155e-05, 2.7166e-05,
         8.6719e-06, 1.4356e-05, 2.2254e-05, 1.0577e-05, 1.5560e-05, 1.4347e-05,
         8.8429e-06, 2.3726e-05, 2.3527e-05, 2.3917e-05, 1.1253e-05, 1.0469e-05,
         1.6361e-05, 1.1750e-05],
        [1.2147e-05, 1.5339e-05, 1.5136e-05, 1.1878e-05, 9.8435e-06, 8.0089e-06,
         1.2140e-05, 1.5638e-05, 1.3994e-05, 1.3050e-05, 1.2910e-05, 2.1710e-05,
         6.9300e-06, 1.1472e-05, 1.7784e-05, 8.4523e-06, 1.2435e-05, 1.1465e-05,
         7.0667e-06, 1.8960e-05, 1.8801e-05, 1.9113e-05, 8.9929e-06, 8.3657e-06,
         1.3075e-05, 9.3895e-06],
        [1.2010e-05, 1.5165e-05, 1.4964e-05, 1.1743e-05, 9.7318e-06, 7.9180e-06,
         1.2002e-05, 1.5460e-05, 1.3835e-05, 1.2902e-05, 1.2764e-05, 2.1463e-05,
         6.8513e-06, 1.1342e-05, 1.7582e-05, 8.3564e-06, 1.2293e-05, 1.1335e-05,
         6.9865e-06, 1.8745e-05, 1.8588e-05, 1.8896e-05, 8.8908e-06, 8.2708e-06,
         1.2927e-05, 9.2829e-06],
        [1.1716e-05, 1.4794e-05, 1.4598e-05, 1.1455e-05, 9.4935e-06, 7.7241e-06,
         1.1708e-05, 1.5082e-05, 1.3496e-05, 1.2586e-05, 1.2451e-05, 2.0938e-05,
         6.6836e-06, 1.1064e-05, 1.7152e-05, 8.1518e-06, 1.1992e-05, 1.1058e-05,
         6.8154e-06, 1.8286e-05, 1.8132e-05, 1.8433e-05, 8.6731e-06, 8.0683e-06,
         1.2610e-05, 9.0556e-06],
        [1.2225e-05, 1.5437e-05, 1.5232e-05, 1.1953e-05, 9.9064e-06, 8.0600e-06,
         1.2217e-05, 1.5738e-05, 1.4083e-05, 1.3133e-05, 1.2993e-05, 2.1848e-05,
         6.9742e-06, 1.1546e-05, 1.7898e-05, 8.5063e-06, 1.2514e-05, 1.1539e-05,
         7.1118e-06, 1.9081e-05, 1.8921e-05, 1.9235e-05, 9.0503e-06, 8.4191e-06,
         1.3158e-05, 9.4494e-06],
        [1.0460e-05, 1.3208e-05, 1.3033e-05, 1.0228e-05, 8.4761e-06, 6.8964e-06,
         1.0453e-05, 1.3466e-05, 1.2050e-05, 1.1237e-05, 1.1117e-05, 1.8694e-05,
         5.9673e-06, 9.8787e-06, 1.5314e-05, 7.2782e-06, 1.0707e-05, 9.8727e-06,
         6.0850e-06, 1.6326e-05, 1.6189e-05, 1.6458e-05, 7.7437e-06, 7.2036e-06,
         1.1259e-05, 8.0851e-06],
        [3.2029e-06, 4.0445e-06, 3.9909e-06, 3.1318e-06, 2.5955e-06, 2.1117e-06,
         3.2009e-06, 4.1233e-06, 3.6898e-06, 3.4409e-06, 3.4040e-06, 5.7242e-06,
         1.8272e-06, 3.0250e-06, 4.6892e-06, 2.2286e-06, 3.2786e-06, 3.0231e-06,
         1.8633e-06, 4.9992e-06, 4.9573e-06, 5.0396e-06, 2.3712e-06, 2.2058e-06,
         3.4475e-06, 2.4757e-06],
        [9.8111e-06, 1.2389e-05, 1.2225e-05, 9.5932e-06, 7.9503e-06, 6.4685e-06,
         9.8047e-06, 1.2630e-05, 1.1302e-05, 1.0540e-05, 1.0427e-05, 1.7534e-05,
         5.5971e-06, 9.2659e-06, 1.4364e-05, 6.8267e-06, 1.0043e-05, 9.2603e-06,
         5.7075e-06, 1.5313e-05, 1.5185e-05, 1.5437e-05, 7.2633e-06, 6.7567e-06,
         1.0560e-05, 7.5836e-06],
        [9.9136e-06, 1.2518e-05, 1.2352e-05, 9.6934e-06, 8.0334e-06, 6.5361e-06,
         9.9072e-06, 1.2762e-05, 1.1421e-05, 1.0650e-05, 1.0536e-05, 1.7717e-05,
         5.6556e-06, 9.3627e-06, 1.4514e-05, 6.8980e-06, 1.0148e-05, 9.3570e-06,
         5.7672e-06, 1.5473e-05, 1.5344e-05, 1.5598e-05, 7.3392e-06, 6.8273e-06,
         1.0671e-05, 7.6628e-06],
        [2.7619e-06, 3.4875e-06, 3.4413e-06, 2.7005e-06, 2.2380e-06, 1.8209e-06,
         2.7601e-06, 3.5555e-06, 3.1817e-06, 2.9670e-06, 2.9353e-06, 4.9360e-06,
         1.5756e-06, 2.6084e-06, 4.0434e-06, 1.9217e-06, 2.8271e-06, 2.6068e-06,
         1.6067e-06, 4.3108e-06, 4.2746e-06, 4.3456e-06, 2.0446e-06, 1.9021e-06,
         2.9727e-06, 2.1348e-06],
        [2.6640e-06, 3.3640e-06, 3.3194e-06, 2.6049e-06, 2.1588e-06, 1.7564e-06,
         2.6623e-06, 3.4295e-06, 3.0690e-06, 2.8619e-06, 2.8313e-06, 4.7611e-06,
         1.5198e-06, 2.5160e-06, 3.9002e-06, 1.8537e-06, 2.7270e-06, 2.5145e-06,
         1.5498e-06, 4.1581e-06, 4.1232e-06, 4.1917e-06, 1.9722e-06, 1.8347e-06,
         2.8674e-06, 2.0592e-06],
        [2.6246e-06, 3.3142e-06, 3.2703e-06, 2.5663e-06, 2.1268e-06, 1.7304e-06,
         2.6229e-06, 3.3788e-06, 3.0236e-06, 2.8196e-06, 2.7894e-06, 4.6907e-06,
         1.4973e-06, 2.4788e-06, 3.8425e-06, 1.8262e-06, 2.6866e-06, 2.4773e-06,
         1.5268e-06, 4.0966e-06, 4.0622e-06, 4.1296e-06, 1.9430e-06, 1.8075e-06,
         2.8250e-06, 2.0287e-06],
        [1.1188e-06, 1.4127e-06, 1.3940e-06, 1.0940e-06, 9.0661e-07, 7.3763e-07,
         1.1181e-06, 1.4403e-06, 1.2889e-06, 1.2019e-06, 1.1890e-06, 1.9995e-06,
         6.3826e-07, 1.0566e-06, 1.6380e-06, 7.7847e-07, 1.1452e-06, 1.0560e-06,
         6.5085e-07, 1.7463e-06, 1.7316e-06, 1.7604e-06, 8.2826e-07, 7.7050e-07,
         1.2042e-06, 8.6478e-07]], grad_fn=<SoftplusBackward>)

  9%|████████████▉                                                                                                                                  | 45/496 [00:09<01:37,  4.62it/s]
-1
-1
### event lambdas:  tensor([[6.2793e-05, 3.9024e-05, 4.7027e-05, 7.5047e-05, 1.0793e-04, 7.2018e-05],
        [6.8097e-05, 4.2320e-05, 5.1000e-05, 8.1385e-05, 1.1705e-04, 7.8101e-05],
        [3.7465e-05, 2.3283e-05, 2.8058e-05, 4.4776e-05, 6.4398e-05, 4.2969e-05],
        [4.0221e-05, 2.4996e-05, 3.0122e-05, 4.8070e-05, 6.9136e-05, 4.6130e-05],
        [5.6471e-05, 3.5095e-05, 4.2293e-05, 6.7491e-05, 9.7067e-05, 6.4767e-05],
        [2.2317e-05, 1.3869e-05, 1.6713e-05, 2.6672e-05, 3.8360e-05, 2.5595e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(359.8992, grad_fn=<NegBackward>) non event loss:  tensor([33.7166], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0986, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0008, 0.0007, 0.0006, 0.0007, 0.0006, 0.0006],
        [0.0008, 0.0007, 0.0007, 0.0007, 0.0006, 0.0006],
        [0.0009, 0.0008, 0.0008, 0.0008, 0.0007, 0.0007],
        [0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],
        [0.0004, 0.0004, 0.0004, 0.0004, 0.0003, 0.0003],
        [0.0004, 0.0004, 0.0004, 0.0004, 0.0003, 0.0003]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(271.0704, grad_fn=<NegBackward>) non event loss:  tensor([222.1876], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1000, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0144, 0.0176, 0.0083, 0.0192, 0.0176, 0.0153, 0.0159, 0.0117, 0.0156],
        [0.0062, 0.0075, 0.0036, 0.0082, 0.0075, 0.0066, 0.0068, 0.0050, 0.0067],
        [0.0025, 0.0031, 0.0015, 0.0034, 0.0031, 0.0027, 0.0028, 0.0020, 0.0027],
        [0.0025, 0.0031, 0.0015, 0.0034, 0.0031, 0.0027, 0.0028, 0.0020, 0.0027],
        [0.0025, 0.0030, 0.0014, 0.0033, 0.0030, 0.0026, 0.0027, 0.0020, 0.0027],
        [0.0036, 0.0044, 0.0021, 0.0048, 0.0044, 0.0038, 0.0040, 0.0029, 0.0039],
        [0.0017, 0.0021, 0.0010, 0.0022, 0.0021, 0.0018, 0.0019, 0.0014, 0.0018],
        [0.0016, 0.0020, 0.0009, 0.0021, 0.0020, 0.0017, 0.0018, 0.0013, 0.0017],
        [0.0013, 0.0016, 0.0008, 0.0018, 0.0016, 0.0014, 0.0015, 0.0011, 0.0014]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(469.7300, grad_fn=<NegBackward>) non event loss:  tensor([534.9026], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0973, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0014, 0.0024, 0.0011, 0.0019, 0.0012],
        [0.0120, 0.0202, 0.0095, 0.0158, 0.0101],
        [0.0015, 0.0025, 0.0012, 0.0019, 0.0012],
        [0.0015, 0.0025, 0.0012, 0.0019, 0.0012],
        [0.0011, 0.0019, 0.0009, 0.0015, 0.0010]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(151.9870, grad_fn=<NegBackward>) non event loss:  tensor([1800.0456], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1058, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0005, 0.0004, 0.0005, 0.0005, 0.0004],
        [0.0005, 0.0004, 0.0005, 0.0005, 0.0004],
        [0.0007, 0.0007, 0.0008, 0.0007, 0.0006],
        [0.0007, 0.0007, 0.0008, 0.0007, 0.0006],
        [0.0007, 0.0007, 0.0008, 0.0007, 0.0006]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(185.5203, grad_fn=<NegBackward>) non event loss:  tensor([299.5849], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0303, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.1921e-04, 1.7933e-04, 1.5101e-04, 2.9261e-04, 1.9793e-04, 1.4917e-04,
         1.6371e-04, 1.6367e-04, 1.8267e-04],
        [1.1657e-04, 1.7536e-04, 1.4766e-04, 2.8613e-04, 1.9355e-04, 1.4586e-04,
         1.6009e-04, 1.6005e-04, 1.7863e-04],
        [8.7683e-05, 1.3191e-04, 1.1107e-04, 2.1523e-04, 1.4559e-04, 1.0972e-04,
         1.2042e-04, 1.2039e-04, 1.3437e-04],
        [1.1853e-04, 1.7831e-04, 1.5015e-04, 2.9094e-04, 1.9681e-04, 1.4832e-04,
         1.6278e-04, 1.6274e-04, 1.8164e-04],
        [7.2333e-05, 1.0882e-04, 9.1630e-05, 1.7756e-04, 1.2010e-04, 9.0512e-05,
         9.9339e-05, 9.9316e-05, 1.1085e-04],
        [3.6181e-05, 5.4431e-05, 4.5833e-05, 8.8814e-05, 6.0076e-05, 4.5274e-05,
         4.9689e-05, 4.9678e-05, 5.5445e-05],
        [2.0016e-04, 3.0111e-04, 2.5356e-04, 4.9129e-04, 3.3234e-04, 2.5046e-04,
         2.7488e-04, 2.7482e-04, 3.0672e-04],
        [1.6028e-04, 2.4112e-04, 2.0304e-04, 3.9342e-04, 2.6613e-04, 2.0056e-04,
         2.2012e-04, 2.2007e-04, 2.4562e-04],
        [3.3896e-05, 5.0994e-05, 4.2939e-05, 8.3208e-05, 5.6284e-05, 4.2416e-05,
         4.6552e-05, 4.6541e-05, 5.1944e-05]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(724.0228, grad_fn=<NegBackward>) non event loss:  tensor([103.7423], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1582, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0013, 0.0023, 0.0019],
        [0.0006, 0.0011, 0.0009],
        [0.0002, 0.0003, 0.0003]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(65.1301, grad_fn=<NegBackward>) non event loss:  tensor([9.8339], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0381, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.2943e-05, 1.3588e-05, 1.2083e-05, 1.2458e-05, 1.5352e-05, 1.8674e-05,
         1.7460e-05, 1.9013e-05],
        [2.6522e-05, 2.7843e-05, 2.4760e-05, 2.5527e-05, 3.1458e-05, 3.8264e-05,
         3.5778e-05, 3.8960e-05],
        [1.4196e-05, 1.4903e-05, 1.3252e-05, 1.3663e-05, 1.6838e-05, 2.0480e-05,
         1.9150e-05, 2.0853e-05],
        [9.0785e-06, 9.5307e-06, 8.4753e-06, 8.7379e-06, 1.0768e-05, 1.3098e-05,
         1.2247e-05, 1.3336e-05],
        [9.2407e-05, 9.7010e-05, 8.6267e-05, 8.8940e-05, 1.0960e-04, 1.3331e-04,
         1.2465e-04, 1.3574e-04],
        [2.5062e-05, 2.6310e-05, 2.3396e-05, 2.4121e-05, 2.9726e-05, 3.6157e-05,
         3.3808e-05, 3.6814e-05],
        [9.4919e-06, 9.9647e-06, 8.8612e-06, 9.1358e-06, 1.1258e-05, 1.3694e-05,
         1.2804e-05, 1.3943e-05],
        [7.8833e-06, 8.2760e-06, 7.3595e-06, 7.5876e-06, 9.3505e-06, 1.1373e-05,
         1.0634e-05, 1.1580e-05]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(692.8099, grad_fn=<NegBackward>) non event loss:  tensor([13.0673], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1549, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.5166e-05, 5.1469e-05, 2.8394e-05, 3.5696e-05, 1.4047e-05],
        [9.4927e-06, 1.3893e-05, 7.6645e-06, 9.6358e-06, 3.7919e-06],
        [5.2720e-06, 7.7160e-06, 4.2566e-06, 5.3514e-06, 2.1059e-06],
        [4.9611e-06, 7.2610e-06, 4.0056e-06, 5.0358e-06, 1.9817e-06],
        [1.1089e-05, 1.6230e-05, 8.9535e-06, 1.1256e-05, 4.4296e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(291.6634, grad_fn=<NegBackward>) non event loss:  tensor([3.6169], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0909, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.1597e-04, 1.5663e-04, 5.6357e-05, 1.1485e-04, 8.8745e-05, 7.0871e-05],
        [1.1015e-04, 1.4877e-04, 5.3529e-05, 1.0909e-04, 8.4291e-05, 6.7314e-05],
        [1.8757e-04, 2.5333e-04, 9.1153e-05, 1.8576e-04, 1.4354e-04, 1.1463e-04],
        [2.2828e-04, 3.0830e-04, 1.1093e-04, 2.2607e-04, 1.7468e-04, 1.3950e-04],
        [7.9376e-05, 1.0720e-04, 3.8572e-05, 7.8608e-05, 6.0740e-05, 4.8506e-05],
        [7.8240e-05, 1.0567e-04, 3.8020e-05, 7.7483e-05, 5.9870e-05, 4.7811e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(331.3803, grad_fn=<NegBackward>) non event loss:  tensor([28.7400], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1302, grad_fn=<SumBackward0>)
-1

 11%|████████████████▍                                                                                                                              | 57/496 [00:11<01:04,  6.78it/s]
### event lambdas:  tensor([[6.1025e-05, 6.7109e-05, 2.7559e-05, 5.5709e-05, 4.1693e-05, 3.2616e-05,
         3.6812e-05, 6.3965e-05, 4.1751e-05, 3.8390e-05, 3.7295e-05, 2.7771e-05,
         2.8352e-05, 3.9287e-05, 2.8803e-05, 4.8921e-05, 6.2329e-05, 3.5522e-05],
        [1.1159e-05, 1.2272e-05, 5.0394e-06, 1.0187e-05, 7.6240e-06, 5.9643e-06,
         6.7315e-06, 1.1697e-05, 7.6346e-06, 7.0201e-06, 6.8197e-06, 5.0782e-06,
         5.1845e-06, 7.1840e-06, 5.2669e-06, 8.9457e-06, 1.1398e-05, 6.4955e-06],
        [1.1274e-05, 1.2398e-05, 5.0911e-06, 1.0292e-05, 7.7022e-06, 6.0255e-06,
         6.8005e-06, 1.1817e-05, 7.7129e-06, 7.0921e-06, 6.8897e-06, 5.1303e-06,
         5.2377e-06, 7.2577e-06, 5.3209e-06, 9.0375e-06, 1.1515e-05, 6.5622e-06],
        [1.2233e-05, 1.3453e-05, 5.5245e-06, 1.1168e-05, 8.3579e-06, 6.5384e-06,
         7.3794e-06, 1.2823e-05, 8.3695e-06, 7.6959e-06, 7.4762e-06, 5.5670e-06,
         5.6836e-06, 7.8755e-06, 5.7739e-06, 9.8069e-06, 1.2495e-05, 7.1208e-06],
        [6.8717e-06, 7.5568e-06, 3.1032e-06, 6.2731e-06, 4.6948e-06, 3.6727e-06,
         4.1452e-06, 7.2028e-06, 4.7013e-06, 4.3229e-06, 4.1995e-06, 3.1271e-06,
         3.1926e-06, 4.4238e-06, 3.2433e-06, 5.5087e-06, 7.0185e-06, 3.9999e-06],
        [5.7415e-06, 6.3140e-06, 2.5929e-06, 5.2414e-06, 3.9227e-06, 3.0687e-06,
         3.4634e-06, 6.0182e-06, 3.9281e-06, 3.6119e-06, 3.5088e-06, 2.6128e-06,
         2.6675e-06, 3.6963e-06, 2.7099e-06, 4.6027e-06, 5.8642e-06, 3.3421e-06],
        [5.2508e-06, 5.7744e-06, 2.3712e-06, 4.7934e-06, 3.5874e-06, 2.8064e-06,
         3.1674e-06, 5.5039e-06, 3.5924e-06, 3.3032e-06, 3.2089e-06, 2.3895e-06,
         2.4395e-06, 3.3804e-06, 2.4783e-06, 4.2093e-06, 5.3630e-06, 3.0564e-06],
        [9.0760e-06, 9.9809e-06, 4.0987e-06, 8.2854e-06, 6.2008e-06, 4.8509e-06,
         5.4748e-06, 9.5133e-06, 6.2094e-06, 5.7096e-06, 5.5466e-06, 4.1302e-06,
         4.2167e-06, 5.8429e-06, 4.2837e-06, 7.2758e-06, 9.2699e-06, 5.2830e-06],
        [8.7547e-06, 9.6276e-06, 3.9536e-06, 7.9920e-06, 5.9812e-06, 4.6791e-06,
         5.2810e-06, 9.1765e-06, 5.9896e-06, 5.5075e-06, 5.3503e-06, 3.9840e-06,
         4.0674e-06, 5.6360e-06, 4.1320e-06, 7.0182e-06, 8.9417e-06, 5.0959e-06],
        [3.1224e-06, 3.4337e-06, 1.4100e-06, 2.8504e-06, 2.1332e-06, 1.6688e-06,
         1.8835e-06, 3.2728e-06, 2.1362e-06, 1.9642e-06, 1.9082e-06, 1.4209e-06,
         1.4507e-06, 2.0101e-06, 1.4737e-06, 2.5031e-06, 3.1891e-06, 1.8175e-06],
        [4.7495e-06, 5.2231e-06, 2.1449e-06, 4.3358e-06, 3.2449e-06, 2.5385e-06,
         2.8650e-06, 4.9784e-06, 3.2494e-06, 2.9879e-06, 2.9026e-06, 2.1614e-06,
         2.2066e-06, 3.0576e-06, 2.2417e-06, 3.8075e-06, 4.8510e-06, 2.7646e-06],
        [1.2970e-05, 1.4263e-05, 5.8570e-06, 1.1840e-05, 8.8609e-06, 6.9319e-06,
         7.8236e-06, 1.3595e-05, 8.8733e-06, 8.1590e-06, 7.9261e-06, 5.9021e-06,
         6.0257e-06, 8.3495e-06, 6.1214e-06, 1.0397e-05, 1.3247e-05, 7.5494e-06],
        [2.9090e-06, 3.1991e-06, 1.3137e-06, 2.6556e-06, 1.9874e-06, 1.5548e-06,
         1.7548e-06, 3.0492e-06, 1.9902e-06, 1.8300e-06, 1.7778e-06, 1.3238e-06,
         1.3515e-06, 1.8727e-06, 1.3730e-06, 2.3320e-06, 2.9712e-06, 1.6933e-06],
        [3.1319e-06, 3.4442e-06, 1.4144e-06, 2.8591e-06, 2.1398e-06, 1.6739e-06,
         1.8893e-06, 3.2829e-06, 2.1427e-06, 1.9703e-06, 1.9140e-06, 1.4253e-06,
         1.4551e-06, 2.0163e-06, 1.4782e-06, 2.5107e-06, 3.1989e-06, 1.8230e-06],
        [2.7265e-06, 2.9984e-06, 1.2313e-06, 2.4890e-06, 1.8628e-06, 1.4573e-06,
         1.6447e-06, 2.8579e-06, 1.8654e-06, 1.7152e-06, 1.6663e-06, 1.2408e-06,
         1.2667e-06, 1.7553e-06, 1.2869e-06, 2.1857e-06, 2.7848e-06, 1.5871e-06],
        [1.9632e-06, 2.1589e-06, 8.8655e-07, 1.7921e-06, 1.3412e-06, 1.0493e-06,
         1.1842e-06, 2.0578e-06, 1.3431e-06, 1.2350e-06, 1.1997e-06, 8.9337e-07,
         9.1208e-07, 1.2638e-06, 9.2656e-07, 1.5738e-06, 2.0051e-06, 1.1427e-06],
        [1.9934e-06, 2.1921e-06, 9.0019e-07, 1.8197e-06, 1.3619e-06, 1.0654e-06,
         1.2024e-06, 2.0894e-06, 1.3638e-06, 1.2540e-06, 1.2182e-06, 9.0712e-07,
         9.2611e-07, 1.2833e-06, 9.4082e-07, 1.5980e-06, 2.0360e-06, 1.1603e-06],
        [1.9686e-06, 2.1648e-06, 8.8899e-07, 1.7971e-06, 1.3449e-06, 1.0521e-06,
         1.1875e-06, 2.0634e-06, 1.3468e-06, 1.2384e-06, 1.2031e-06, 8.9583e-07,
         9.1459e-07, 1.2673e-06, 9.2912e-07, 1.5781e-06, 2.0106e-06, 1.1459e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(4033.5383, grad_fn=<NegBackward>) non event loss:  tensor([7.6916], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.2665, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[5.4689e-05, 4.0615e-05, 6.3339e-05, 4.3728e-05],
        [6.1134e-05, 4.5401e-05, 7.0804e-05, 4.8881e-05],
        [3.7751e-05, 2.8036e-05, 4.3722e-05, 3.0185e-05],
        [1.2015e-05, 8.9232e-06, 1.3916e-05, 9.6072e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(165.6179, grad_fn=<NegBackward>) non event loss:  tensor([3.6986], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0526, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.2907e-05, 1.3248e-05, 1.0452e-05, 9.9058e-06, 8.6741e-06],
        [1.9420e-05, 1.9934e-05, 1.5726e-05, 1.4905e-05, 1.3051e-05],
        [6.4965e-06, 6.6685e-06, 5.2608e-06, 4.9860e-06, 4.3661e-06],
        [4.2618e-06, 4.3747e-06, 3.4512e-06, 3.2709e-06, 2.8642e-06],
        [2.6549e-06, 2.7252e-06, 2.1499e-06, 2.0376e-06, 1.7843e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(300.5150, grad_fn=<NegBackward>) non event loss:  tensor([1.2429], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0756, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.6825e-05, 1.4011e-05, 2.5309e-05, 1.8253e-05, 1.3927e-05, 2.2218e-05,
         1.4564e-05],
        [1.6850e-05, 1.4031e-05, 2.5346e-05, 1.8279e-05, 1.3947e-05, 2.2250e-05,
         1.4585e-05],
        [2.0899e-05, 1.7403e-05, 3.1436e-05, 2.2672e-05, 1.7298e-05, 2.7597e-05,
         1.8090e-05],
        [5.2599e-06, 4.3801e-06, 7.9121e-06, 5.7062e-06, 4.3537e-06, 6.9458e-06,
         4.5530e-06],
        [1.7160e-06, 1.4290e-06, 2.5813e-06, 1.8616e-06, 1.4204e-06, 2.2661e-06,
         1.4854e-06],
        [1.6986e-06, 1.4145e-06, 2.5551e-06, 1.8427e-06, 1.4060e-06, 2.2431e-06,
         1.4704e-06],
        [1.6984e-06, 1.4143e-06, 2.5548e-06, 1.8425e-06, 1.4058e-06, 2.2428e-06,
         1.4701e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(591.5753, grad_fn=<NegBackward>) non event loss:  tensor([1.2554], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1020, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.5846e-06, 6.2333e-06, 4.1320e-06, 4.1939e-06, 6.1646e-06, 4.7143e-06,
         4.9642e-06],
        [1.4616e-06, 3.5249e-06, 2.3366e-06, 2.3716e-06, 3.4860e-06, 2.6658e-06,
         2.8072e-06],
        [1.4718e-06, 3.5495e-06, 2.3529e-06, 2.3882e-06, 3.5104e-06, 2.6845e-06,
         2.8268e-06],
        [1.2047e-06, 2.9055e-06, 1.9260e-06, 1.9549e-06, 2.8735e-06, 2.1974e-06,
         2.3139e-06],
        [2.2197e-06, 5.3534e-06, 3.5487e-06, 3.6019e-06, 5.2944e-06, 4.0487e-06,
         4.2634e-06],
        [7.2952e-07, 1.7594e-06, 1.1663e-06, 1.1838e-06, 1.7400e-06, 1.3306e-06,
         1.4012e-06],
        [1.0649e-06, 2.5683e-06, 1.7025e-06, 1.7280e-06, 2.5400e-06, 1.9424e-06,
         2.0454e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(632.1371, grad_fn=<NegBackward>) non event loss:  tensor([1.0894], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1169, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.3724e-06, 2.4072e-06, 4.0071e-06, 4.4549e-06],
        [1.2238e-06, 8.7353e-07, 1.4541e-06, 1.6166e-06],
        [1.2568e-06, 8.9710e-07, 1.4934e-06, 1.6602e-06],
        [8.0075e-07, 5.7157e-07, 9.5146e-07, 1.0578e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(214.8977, grad_fn=<NegBackward>) non event loss:  tensor([0.3420], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0962, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.8692e-06, 2.3926e-06, 3.6296e-06, 2.6151e-06, 2.3440e-06, 3.7056e-06,
         2.4652e-06, 2.8562e-06, 3.6196e-06, 2.4174e-06],
        [1.6992e-06, 2.1750e-06, 3.2994e-06, 2.3772e-06, 2.1308e-06, 3.3685e-06,
         2.2410e-06, 2.5964e-06, 3.2903e-06, 2.1975e-06],
        [9.2323e-07, 1.1817e-06, 1.7927e-06, 1.2916e-06, 1.1577e-06, 1.8302e-06,
         1.2176e-06, 1.4107e-06, 1.7877e-06, 1.1940e-06],
        [7.7738e-07, 9.9504e-07, 1.5095e-06, 1.0876e-06, 9.7484e-07, 1.5411e-06,
         1.0252e-06, 1.1879e-06, 1.5053e-06, 1.0054e-06],
        [7.4809e-07, 9.5755e-07, 1.4526e-06, 1.0466e-06, 9.3811e-07, 1.4830e-06,
         9.8660e-07, 1.1431e-06, 1.4486e-06, 9.6749e-07],
        [6.8305e-07, 8.7430e-07, 1.3263e-06, 9.5560e-07, 8.5655e-07, 1.3541e-06,
         9.0083e-07, 1.0437e-06, 1.3227e-06, 8.8338e-07],
        [1.0102e-06, 1.2930e-06, 1.9615e-06, 1.4132e-06, 1.2668e-06, 2.0026e-06,
         1.3322e-06, 1.5436e-06, 1.9561e-06, 1.3064e-06],
        [1.5995e-06, 2.0473e-06, 3.1057e-06, 2.2377e-06, 2.0057e-06, 3.1708e-06,
         2.1094e-06, 2.4440e-06, 3.0972e-06, 2.0685e-06],
        [7.6793e-07, 9.8294e-07, 1.4911e-06, 1.0743e-06, 9.6299e-07, 1.5223e-06,
         1.0128e-06, 1.1734e-06, 1.4870e-06, 9.9315e-07],
        [6.1718e-07, 7.8999e-07, 1.1984e-06, 8.6345e-07, 7.7395e-07, 1.2235e-06,
         8.1396e-07, 9.4306e-07, 1.1951e-06, 7.9819e-07]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(1344.9382, grad_fn=<NegBackward>) non event loss:  tensor([1.2260], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1708, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.7724e-05, 5.7246e-05, 4.8020e-05, 5.7087e-05, 7.9341e-05],
        [3.8721e-06, 4.6447e-06, 3.8961e-06, 4.6318e-06, 6.4374e-06],
        [3.6949e-06, 4.4321e-06, 3.7178e-06, 4.4198e-06, 6.1429e-06],
        [2.2259e-06, 2.6700e-06, 2.2397e-06, 2.6626e-06, 3.7006e-06],
        [1.0648e-06, 1.2773e-06, 1.0714e-06, 1.2737e-06, 1.7703e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(304.0641, grad_fn=<NegBackward>) non event loss:  tensor([0.4115], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0419, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.7125e-05, 1.8657e-05, 1.6278e-05, 1.3624e-05, 1.3621e-05],
        [1.5409e-05, 1.6788e-05, 1.4647e-05, 1.2259e-05, 1.2256e-05],
        [9.8997e-06, 1.0786e-05, 9.4101e-06, 7.8758e-06, 7.8741e-06],
        [9.8926e-06, 1.0778e-05, 9.4033e-06, 7.8701e-06, 7.8685e-06],
        [1.2234e-05, 1.3329e-05, 1.1629e-05, 9.7331e-06, 9.7310e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(284.1810, grad_fn=<NegBackward>) non event loss:  tensor([2.6304], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0765, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.7017e-05, 1.2443e-05, 2.2376e-05, 2.3807e-05, 1.1430e-05, 1.0144e-05,
         1.4625e-05, 1.4679e-05, 2.1701e-05, 1.4858e-05, 2.4432e-05, 1.2244e-05],
        [1.6185e-05, 1.1835e-05, 2.1282e-05, 2.2643e-05, 1.0871e-05, 9.6476e-06,
         1.3910e-05, 1.3962e-05, 2.0640e-05, 1.4132e-05, 2.3238e-05, 1.1646e-05],
        [1.5445e-05, 1.1293e-05, 2.0309e-05, 2.1607e-05, 1.0374e-05, 9.2062e-06,
         1.3274e-05, 1.3323e-05, 1.9696e-05, 1.3485e-05, 2.2175e-05, 1.1113e-05],
        [1.1672e-05, 8.5349e-06, 1.5348e-05, 1.6330e-05, 7.8400e-06, 6.9577e-06,
         1.0032e-05, 1.0069e-05, 1.4885e-05, 1.0192e-05, 1.6759e-05, 8.3987e-06],
        [8.7863e-06, 6.4245e-06, 1.1553e-05, 1.2292e-05, 5.9014e-06, 5.2373e-06,
         7.5512e-06, 7.5793e-06, 1.1205e-05, 7.6716e-06, 1.2615e-05, 6.3220e-06],
        [8.2392e-06, 6.0245e-06, 1.0834e-05, 1.1527e-05, 5.5340e-06, 4.9112e-06,
         7.0810e-06, 7.1074e-06, 1.0507e-05, 7.1940e-06, 1.1829e-05, 5.9284e-06],
        [8.1605e-06, 5.9669e-06, 1.0730e-05, 1.1416e-05, 5.4811e-06, 4.8643e-06,
         7.0134e-06, 7.0395e-06, 1.0407e-05, 7.1252e-06, 1.1716e-05, 5.8717e-06],
        [8.0306e-06, 5.8720e-06, 1.0560e-05, 1.1235e-05, 5.3939e-06, 4.7869e-06,
         6.9018e-06, 6.9274e-06, 1.0241e-05, 7.0118e-06, 1.1530e-05, 5.7783e-06],
        [6.2924e-06, 4.6010e-06, 8.2741e-06, 8.8031e-06, 4.2264e-06, 3.7508e-06,
         5.4079e-06, 5.4280e-06, 8.0244e-06, 5.4941e-06, 9.0344e-06, 4.5276e-06],
        [5.5266e-06, 4.0410e-06, 7.2671e-06, 7.7317e-06, 3.7120e-06, 3.2943e-06,
         4.7497e-06, 4.7674e-06, 7.0477e-06, 4.8255e-06, 7.9348e-06, 3.9766e-06],
        [4.6846e-06, 3.4254e-06, 6.1600e-06, 6.5538e-06, 3.1465e-06, 2.7924e-06,
         4.0261e-06, 4.0411e-06, 5.9740e-06, 4.0903e-06, 6.7260e-06, 3.3707e-06],
        [4.9003e-06, 3.5831e-06, 6.4435e-06, 6.8555e-06, 3.2913e-06, 2.9209e-06,
         4.2114e-06, 4.2271e-06, 6.2490e-06, 4.2786e-06, 7.0356e-06, 3.5259e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(1687.2518, grad_fn=<NegBackward>) non event loss:  tensor([3.3179], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0996, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[5.8592e-06, 3.7786e-06, 4.0815e-06, 3.2455e-06, 3.4787e-06, 3.4787e-06,
         3.8032e-06, 3.8185e-06, 3.4807e-06],
        [6.4655e-06, 4.1696e-06, 4.5038e-06, 3.5813e-06, 3.8386e-06, 3.8386e-06,
         4.1968e-06, 4.2136e-06, 3.8409e-06],
        [6.3643e-06, 4.1044e-06, 4.4333e-06, 3.5253e-06, 3.7786e-06, 3.7786e-06,
         4.1311e-06, 4.1477e-06, 3.7808e-06],
        [5.7228e-06, 3.6906e-06, 3.9864e-06, 3.1699e-06, 3.3977e-06, 3.3977e-06,
         3.7147e-06, 3.7296e-06, 3.3997e-06],
        [5.6788e-06, 3.6623e-06, 3.9558e-06, 3.1456e-06, 3.3716e-06, 3.3716e-06,
         3.6862e-06, 3.7009e-06, 3.3736e-06],
        [3.3573e-05, 2.1651e-05, 2.3387e-05, 1.8596e-05, 1.9933e-05, 1.9933e-05,
         2.1792e-05, 2.1880e-05, 1.9944e-05],
        [2.8250e-06, 1.8218e-06, 1.9679e-06, 1.5648e-06, 1.6772e-06, 1.6772e-06,
         1.8337e-06, 1.8411e-06, 1.6782e-06],
        [2.9964e-06, 1.9324e-06, 2.0872e-06, 1.6597e-06, 1.7790e-06, 1.7790e-06,
         1.9450e-06, 1.9528e-06, 1.7800e-06],
        [2.9872e-06, 1.9264e-06, 2.0808e-06, 1.6546e-06, 1.7735e-06, 1.7735e-06,
         1.9390e-06, 1.9468e-06, 1.7746e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1011.9963, grad_fn=<NegBackward>) non event loss:  tensor([1.5068], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0979, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[7.1529e-06, 9.3688e-06, 1.1708e-05, 4.4255e-06, 8.2426e-06, 7.8697e-06,
         8.8582e-06, 5.4917e-06, 4.8300e-06, 7.2380e-06, 1.2912e-05, 6.2753e-06,
         6.4604e-06],
        [6.7060e-06, 8.7835e-06, 1.0977e-05, 4.1491e-06, 7.7276e-06, 7.3780e-06,
         8.3048e-06, 5.1486e-06, 4.5282e-06, 6.7858e-06, 1.2105e-05, 5.8833e-06,
         6.0568e-06],
        [1.0295e-05, 1.3485e-05, 1.6852e-05, 6.3699e-06, 1.1864e-05, 1.1327e-05,
         1.2750e-05, 7.9044e-06, 6.9520e-06, 1.0418e-05, 1.8584e-05, 9.0323e-06,
         9.2987e-06],
        [1.3422e-06, 1.7580e-06, 2.1970e-06, 8.3042e-07, 1.5467e-06, 1.4767e-06,
         1.6622e-06, 1.0305e-06, 9.0630e-07, 1.3582e-06, 2.4228e-06, 1.1775e-06,
         1.2122e-06],
        [9.7488e-07, 1.2769e-06, 1.5958e-06, 6.0316e-07, 1.1234e-06, 1.0726e-06,
         1.2073e-06, 7.4847e-07, 6.5828e-07, 9.8648e-07, 1.7598e-06, 8.5527e-07,
         8.8050e-07],
        [1.5258e-06, 1.9985e-06, 2.4976e-06, 9.4402e-07, 1.7582e-06, 1.6787e-06,
         1.8896e-06, 1.1714e-06, 1.0303e-06, 1.5440e-06, 2.7542e-06, 1.3386e-06,
         1.3781e-06],
        [1.7600e-06, 2.3053e-06, 2.8809e-06, 1.0889e-06, 2.0281e-06, 1.9364e-06,
         2.1796e-06, 1.3513e-06, 1.1884e-06, 1.7810e-06, 3.1770e-06, 1.5441e-06,
         1.5896e-06],
        [1.3557e-06, 1.7756e-06, 2.2190e-06, 8.3875e-07, 1.5622e-06, 1.4915e-06,
         1.6789e-06, 1.0408e-06, 9.1540e-07, 1.3718e-06, 2.4471e-06, 1.1893e-06,
         1.2244e-06],
        [8.6582e-07, 1.1341e-06, 1.4172e-06, 5.3569e-07, 9.9772e-07, 9.5259e-07,
         1.0722e-06, 6.6474e-07, 5.8464e-07, 8.7612e-07, 1.5629e-06, 7.5960e-07,
         7.8200e-07],
        [4.9447e-06, 6.4765e-06, 8.0939e-06, 3.0593e-06, 5.6980e-06, 5.4402e-06,
         6.1236e-06, 3.7963e-06, 3.3389e-06, 5.0035e-06, 8.9257e-06, 4.3380e-06,
         4.4660e-06],
        [1.2045e-06, 1.5777e-06, 1.9716e-06, 7.4524e-07, 1.3880e-06, 1.3252e-06,
         1.4917e-06, 9.2477e-07, 8.1335e-07, 1.2188e-06, 2.1743e-06, 1.0567e-06,
         1.0879e-06],
        [1.0335e-06, 1.3536e-06, 1.6917e-06, 6.3941e-07, 1.1909e-06, 1.1370e-06,
         1.2799e-06, 7.9345e-07, 6.9784e-07, 1.0458e-06, 1.8655e-06, 9.0667e-07,
         9.3341e-07],
        [9.3702e-07, 1.2273e-06, 1.5338e-06, 5.7974e-07, 1.0798e-06, 1.0309e-06,
         1.1604e-06, 7.1940e-07, 6.3272e-07, 9.4817e-07, 1.6914e-06, 8.2206e-07,
         8.4630e-07]], grad_fn=<SoftplusBackward>)

 14%|███████████████████▌                                                                                                                           | 68/496 [00:13<01:20,  5.32it/s]
-1
-1
### event lambdas:  tensor([[1.5974e-06, 3.4140e-06, 1.8271e-06, 1.9925e-06, 3.5154e-06],
        [1.3815e-06, 2.9525e-06, 1.5801e-06, 1.7232e-06, 3.0402e-06],
        [1.1220e-06, 2.3979e-06, 1.2833e-06, 1.3995e-06, 2.4691e-06],
        [6.9936e-07, 1.4947e-06, 7.9992e-07, 8.7235e-07, 1.5391e-06],
        [8.9473e-07, 1.9122e-06, 1.0234e-06, 1.1160e-06, 1.9690e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(333.6806, grad_fn=<NegBackward>) non event loss:  tensor([1.0482], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1033, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.6581e-05, 1.3922e-05, 1.8206e-05, 2.3199e-05, 2.5183e-05],
        [4.7647e-06, 1.8133e-06, 2.3713e-06, 3.0216e-06, 3.2800e-06],
        [1.7377e-06, 6.6134e-07, 8.6485e-07, 1.1020e-06, 1.1963e-06],
        [1.1706e-05, 4.4549e-06, 5.8258e-06, 7.4234e-06, 8.0583e-06],
        [2.0297e-06, 7.7245e-07, 1.0102e-06, 1.2872e-06, 1.3973e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(313.4445, grad_fn=<NegBackward>) non event loss:  tensor([1.1239], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0672, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.0544e-05, 2.1961e-05, 2.2550e-05],
        [4.3946e-06, 4.6979e-06, 4.8238e-06],
        [3.1415e-06, 3.3583e-06, 3.4483e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(106.9170, grad_fn=<NegBackward>) non event loss:  tensor([2.3677], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0594, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.0783e-05, 3.1347e-05, 2.3957e-05, 2.6015e-05],
        [6.9353e-05, 1.0460e-04, 7.9945e-05, 8.6811e-05],
        [6.5210e-06, 9.8357e-06, 7.5169e-06, 8.1625e-06],
        [6.8544e-06, 1.0339e-05, 7.9012e-06, 8.5798e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(173.6446, grad_fn=<NegBackward>) non event loss:  tensor([2.9670], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0723, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.3325e-05, 4.2821e-05, 4.1347e-05, 3.6548e-05, 2.8263e-05],
        [2.2949e-05, 4.2130e-05, 4.0681e-05, 3.5959e-05, 2.7807e-05],
        [2.2438e-05, 4.1193e-05, 3.9775e-05, 3.5158e-05, 2.7188e-05],
        [2.7807e-05, 5.1049e-05, 4.9292e-05, 4.3571e-05, 3.3694e-05],
        [2.0995e-05, 3.8543e-05, 3.7217e-05, 3.2897e-05, 2.5440e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(257.4666, grad_fn=<NegBackward>) non event loss:  tensor([9.9413], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1056, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.6955e-05, 3.0175e-05, 2.2417e-05, 1.9991e-05, 1.6954e-05, 2.9124e-05,
         2.5170e-05, 3.3224e-05, 1.3629e-05],
        [9.8122e-06, 1.7463e-05, 1.2973e-05, 1.1569e-05, 9.8117e-06, 1.6855e-05,
         1.4567e-05, 1.9228e-05, 7.8875e-06],
        [2.8836e-06, 5.1321e-06, 3.8126e-06, 3.4000e-06, 2.8835e-06, 4.9533e-06,
         4.2809e-06, 5.6507e-06, 2.3180e-06],
        [2.0200e-06, 3.5951e-06, 2.6707e-06, 2.3817e-06, 2.0199e-06, 3.4698e-06,
         2.9988e-06, 3.9583e-06, 1.6238e-06],
        [1.9756e-06, 3.5160e-06, 2.6120e-06, 2.3293e-06, 1.9755e-06, 3.3935e-06,
         2.9328e-06, 3.8713e-06, 1.5881e-06],
        [1.9737e-06, 3.5128e-06, 2.6096e-06, 2.3272e-06, 1.9737e-06, 3.3904e-06,
         2.9301e-06, 3.8677e-06, 1.5866e-06],
        [1.9782e-06, 3.5207e-06, 2.6155e-06, 2.3324e-06, 1.9781e-06, 3.3980e-06,
         2.9367e-06, 3.8764e-06, 1.5902e-06],
        [2.5201e-06, 4.4852e-06, 3.3320e-06, 2.9714e-06, 2.5200e-06, 4.3289e-06,
         3.7413e-06, 4.9384e-06, 2.0258e-06],
        [4.6068e-06, 8.1990e-06, 6.0909e-06, 5.4317e-06, 4.6066e-06, 7.9133e-06,
         6.8390e-06, 9.0274e-06, 3.7032e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(995.0179, grad_fn=<NegBackward>) non event loss:  tensor([2.2879], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1696, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[5.7159e-05, 4.0482e-05, 5.4876e-05, 4.5793e-05, 5.5949e-05, 4.2074e-05,
         6.8947e-05, 4.3870e-05],
        [5.7247e-05, 4.0544e-05, 5.4960e-05, 4.5864e-05, 5.6035e-05, 4.2138e-05,
         6.9053e-05, 4.3937e-05],
        [5.6411e-05, 3.9952e-05, 5.4157e-05, 4.5194e-05, 5.5216e-05, 4.1523e-05,
         6.8044e-05, 4.3295e-05],
        [2.0054e-04, 1.4203e-04, 1.9252e-04, 1.6066e-04, 1.9629e-04, 1.4761e-04,
         2.4189e-04, 1.5391e-04],
        [2.7797e-05, 1.9687e-05, 2.6687e-05, 2.2270e-05, 2.7209e-05, 2.0461e-05,
         3.3530e-05, 2.1334e-05],
        [2.4847e-05, 1.7597e-05, 2.3854e-05, 1.9906e-05, 2.4320e-05, 1.8289e-05,
         2.9971e-05, 1.9070e-05],
        [9.8594e-05, 6.9828e-05, 9.4655e-05, 7.8989e-05, 9.6506e-05, 7.2574e-05,
         1.1893e-04, 7.5671e-05],
        [7.8044e-06, 5.5272e-06, 7.4926e-06, 6.2524e-06, 7.6391e-06, 5.7446e-06,
         9.4138e-06, 5.9898e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(647.4100, grad_fn=<NegBackward>) non event loss:  tensor([8.0356], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1074, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.1413e-05, 1.0154e-05, 1.9691e-05, 1.1280e-05, 2.1055e-05, 1.9651e-05,
         1.2090e-05],
        [1.3204e-04, 6.2613e-05, 1.2142e-04, 6.9559e-05, 1.2983e-04, 1.2117e-04,
         7.4552e-05],
        [1.6775e-05, 7.9543e-06, 1.5426e-05, 8.8368e-06, 1.6494e-05, 1.5394e-05,
         9.4712e-06],
        [1.1629e-05, 5.5143e-06, 1.0694e-05, 6.1261e-06, 1.1434e-05, 1.0672e-05,
         6.5658e-06],
        [1.0789e-05, 5.1159e-06, 9.9215e-06, 5.6835e-06, 1.0608e-05, 9.9011e-06,
         6.0915e-06],
        [1.1735e-05, 5.5646e-06, 1.0792e-05, 6.1820e-06, 1.1539e-05, 1.0770e-05,
         6.6258e-06],
        [5.7738e-05, 2.7379e-05, 5.3097e-05, 3.0417e-05, 5.6773e-05, 5.2988e-05,
         3.2600e-05]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(537.1544, grad_fn=<NegBackward>) non event loss:  tensor([3.8102], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0782, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.1345e-05, 5.5406e-05, 5.8753e-05, 4.3951e-05, 7.1217e-05, 6.0805e-05,
         3.4761e-05, 3.0981e-05, 6.0356e-05, 5.2177e-05],
        [1.7780e-04, 2.3827e-04, 2.5266e-04, 1.8901e-04, 3.0625e-04, 2.6148e-04,
         1.4949e-04, 1.3323e-04, 2.5955e-04, 2.2438e-04],
        [4.6392e-05, 6.2169e-05, 6.5925e-05, 4.9316e-05, 7.9910e-05, 6.8227e-05,
         3.9004e-05, 3.4762e-05, 6.7723e-05, 5.8546e-05],
        [4.6536e-05, 6.2362e-05, 6.6129e-05, 4.9469e-05, 8.0157e-05, 6.8438e-05,
         3.9125e-05, 3.4870e-05, 6.7932e-05, 5.8728e-05],
        [5.9085e-05, 7.9178e-05, 8.3962e-05, 6.2809e-05, 1.0177e-04, 8.6893e-05,
         4.9675e-05, 4.4274e-05, 8.6251e-05, 7.4564e-05],
        [5.0078e-05, 6.7109e-05, 7.1163e-05, 5.3234e-05, 8.6259e-05, 7.3648e-05,
         4.2103e-05, 3.7525e-05, 7.3104e-05, 6.3198e-05],
        [1.9204e-04, 2.5735e-04, 2.7289e-04, 2.0415e-04, 3.3077e-04, 2.8242e-04,
         1.6146e-04, 1.4390e-04, 2.8033e-04, 2.4235e-04],
        [1.3035e-05, 1.7467e-05, 1.8523e-05, 1.3856e-05, 2.2452e-05, 1.9169e-05,
         1.0959e-05, 9.7670e-06, 1.9028e-05, 1.6449e-05],
        [2.6207e-05, 3.5119e-05, 3.7241e-05, 2.7858e-05, 4.5141e-05, 3.8541e-05,
         2.2033e-05, 1.9637e-05, 3.8256e-05, 3.3073e-05],
        [7.0113e-05, 9.3957e-05, 9.9633e-05, 7.4532e-05, 1.2077e-04, 1.0311e-04,
         5.8947e-05, 5.2537e-05, 1.0235e-04, 8.8481e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(964.5751, grad_fn=<NegBackward>) non event loss:  tensor([41.0416], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1352, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[7.7470e-05, 7.1274e-05, 1.3086e-04, 9.9864e-05, 9.3169e-05, 7.8716e-05,
         1.0053e-04, 1.4927e-04, 1.5917e-04, 8.2087e-05],
        [1.9765e-04, 1.8185e-04, 3.3386e-04, 2.5478e-04, 2.3770e-04, 2.0083e-04,
         2.5649e-04, 3.8081e-04, 4.0607e-04, 2.0943e-04],
        [1.3158e-04, 1.2106e-04, 2.2226e-04, 1.6962e-04, 1.5825e-04, 1.3370e-04,
         1.7075e-04, 2.5352e-04, 2.7034e-04, 1.3942e-04],
        [7.9189e-05, 7.2856e-05, 1.3376e-04, 1.0208e-04, 9.5235e-05, 8.0462e-05,
         1.0276e-04, 1.5258e-04, 1.6270e-04, 8.3907e-05],
        [9.8541e-05, 9.0660e-05, 1.6645e-04, 1.2702e-04, 1.1851e-04, 1.0013e-04,
         1.2788e-04, 1.8986e-04, 2.0246e-04, 1.0441e-04],
        [9.1083e-05, 8.3799e-05, 1.5385e-04, 1.1741e-04, 1.0954e-04, 9.2548e-05,
         1.1820e-04, 1.7550e-04, 1.8714e-04, 9.6511e-05],
        [3.1829e-05, 2.9284e-05, 5.3766e-05, 4.1030e-05, 3.8279e-05, 3.2341e-05,
         4.1305e-05, 6.1329e-05, 6.5397e-05, 3.3726e-05],
        [2.5933e-06, 2.3859e-06, 4.3807e-06, 3.3430e-06, 3.1188e-06, 2.6350e-06,
         3.3654e-06, 4.9969e-06, 5.3284e-06, 2.7479e-06],
        [2.5731e-05, 2.3674e-05, 4.3466e-05, 3.3170e-05, 3.0946e-05, 2.6145e-05,
         3.3392e-05, 4.9580e-05, 5.2869e-05, 2.7265e-05],
        [4.0918e-05, 3.7646e-05, 6.9119e-05, 5.2746e-05, 4.9210e-05, 4.1576e-05,
         5.3100e-05, 7.8841e-05, 8.4071e-05, 4.3356e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(962.0272, grad_fn=<NegBackward>) non event loss:  tensor([26.8837], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1090, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0013, 0.0030, 0.0024, 0.0012, 0.0022, 0.0026],
        [0.0012, 0.0027, 0.0022, 0.0011, 0.0020, 0.0023],
        [0.0018, 0.0040, 0.0032, 0.0016, 0.0030, 0.0035],
        [0.0017, 0.0039, 0.0031, 0.0016, 0.0029, 0.0034],
        [0.0017, 0.0039, 0.0032, 0.0016, 0.0029, 0.0034],
        [0.0018, 0.0041, 0.0033, 0.0016, 0.0030, 0.0035]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(217.4160, grad_fn=<NegBackward>) non event loss:  tensor([559.9762], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1031, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.5253e-04, 3.2255e-04, 3.9212e-04, 2.3490e-04, 1.8114e-04],
        [3.3637e-04, 2.3975e-04, 2.9147e-04, 1.7460e-04, 1.3464e-04],
        [2.0434e-04, 1.4565e-04, 1.7707e-04, 1.0607e-04, 8.1792e-05],
        [2.8676e-04, 2.0439e-04, 2.4848e-04, 1.4885e-04, 1.1478e-04],
        [1.9701e-04, 1.4042e-04, 1.7071e-04, 1.0226e-04, 7.8855e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(214.6801, grad_fn=<NegBackward>) non event loss:  tensor([46.0749], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0919, grad_fn=<SumBackward0>)

 16%|███████████████████████▎                                                                                                                       | 81/496 [00:15<01:00,  6.84it/s]
-1
### event lambdas:  tensor([[1.1441e-03, 9.3364e-04, 1.0692e-03, 6.8137e-04, 8.1456e-04, 6.9952e-04,
         1.5184e-03, 7.8455e-04, 5.4558e-04, 9.3244e-04, 1.3104e-03],
        [1.1158e-03, 9.1055e-04, 1.0428e-03, 6.6452e-04, 7.9441e-04, 6.8222e-04,
         1.4809e-03, 7.6514e-04, 5.3208e-04, 9.0938e-04, 1.2780e-03],
        [1.0771e-03, 8.7899e-04, 1.0066e-03, 6.4148e-04, 7.6688e-04, 6.5857e-04,
         1.4296e-03, 7.3862e-04, 5.1364e-04, 8.7786e-04, 1.2337e-03],
        [5.7671e-04, 4.7060e-04, 5.3896e-04, 3.4343e-04, 4.1057e-04, 3.5258e-04,
         7.6547e-04, 3.9544e-04, 2.7497e-04, 4.7000e-04, 6.6057e-04],
        [2.7799e-04, 2.2684e-04, 2.5979e-04, 1.6553e-04, 1.9790e-04, 1.6994e-04,
         3.6900e-04, 1.9060e-04, 1.3253e-04, 2.2655e-04, 3.1842e-04],
        [3.1008e-04, 2.5302e-04, 2.8978e-04, 1.8464e-04, 2.2074e-04, 1.8956e-04,
         4.1159e-04, 2.1261e-04, 1.4783e-04, 2.5270e-04, 3.5518e-04],
        [2.2968e-04, 1.8742e-04, 2.1465e-04, 1.3677e-04, 1.6351e-04, 1.4041e-04,
         3.0488e-04, 1.5748e-04, 1.0950e-04, 1.8718e-04, 2.6309e-04],
        [2.5169e-04, 2.0538e-04, 2.3522e-04, 1.4987e-04, 1.7918e-04, 1.5387e-04,
         3.3409e-04, 1.7257e-04, 1.2000e-04, 2.0512e-04, 2.8830e-04],
        [2.5243e-04, 2.0598e-04, 2.3591e-04, 1.5031e-04, 1.7970e-04, 1.5432e-04,
         3.3508e-04, 1.7308e-04, 1.2035e-04, 2.0572e-04, 2.8915e-04],
        [2.1976e-04, 1.7932e-04, 2.0538e-04, 1.3086e-04, 1.5644e-04, 1.3434e-04,
         2.9171e-04, 1.5068e-04, 1.0477e-04, 1.7909e-04, 2.5173e-04],
        [1.4750e-04, 1.2036e-04, 1.3785e-04, 8.7829e-05, 1.0500e-04, 9.0169e-05,
         1.9580e-04, 1.0113e-04, 7.0321e-05, 1.2020e-04, 1.6896e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(977.3464, grad_fn=<NegBackward>) non event loss:  tensor([86.3566], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1108, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0012, 0.0009, 0.0015, 0.0010, 0.0012],
        [0.0011, 0.0009, 0.0015, 0.0010, 0.0011],
        [0.0004, 0.0003, 0.0006, 0.0004, 0.0004],
        [0.0005, 0.0004, 0.0006, 0.0004, 0.0005],
        [0.0003, 0.0003, 0.0004, 0.0003, 0.0003]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(185.2380, grad_fn=<NegBackward>) non event loss:  tensor([237.5213], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0986, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.5645e-04, 4.6099e-04, 6.0417e-04, 6.0417e-04, 4.8990e-04, 3.6197e-04],
        [1.1523e-04, 1.1638e-04, 1.5253e-04, 1.5253e-04, 1.2368e-04, 9.1377e-05],
        [4.1629e-04, 4.2042e-04, 5.5101e-04, 5.5101e-04, 4.4679e-04, 3.3012e-04],
        [1.2472e-04, 1.2596e-04, 1.6509e-04, 1.6509e-04, 1.3386e-04, 9.8901e-05],
        [2.3434e-04, 2.3667e-04, 3.1019e-04, 3.1019e-04, 2.5151e-04, 1.8583e-04],
        [6.1488e-05, 6.2098e-05, 8.1392e-05, 8.1392e-05, 6.5994e-05, 4.8758e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(307.0806, grad_fn=<NegBackward>) non event loss:  tensor([46.9178], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0799, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0008, 0.0010, 0.0007, 0.0007],
        [0.0007, 0.0010, 0.0006, 0.0006],
        [0.0007, 0.0009, 0.0006, 0.0006],
        [0.0003, 0.0004, 0.0003, 0.0002]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(119.3013, grad_fn=<NegBackward>) non event loss:  tensor([216.5860], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0859, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0006, 0.0007, 0.0008, 0.0004, 0.0008],
        [0.0006, 0.0007, 0.0008, 0.0004, 0.0007],
        [0.0002, 0.0002, 0.0003, 0.0001, 0.0003],
        [0.0002, 0.0002, 0.0003, 0.0001, 0.0002],
        [0.0002, 0.0002, 0.0003, 0.0001, 0.0002]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(200.5636, grad_fn=<NegBackward>) non event loss:  tensor([140.1342], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1050, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[5.0605e-04, 5.2734e-04, 2.1486e-04, 2.3075e-04, 2.7279e-04],
        [3.5492e-04, 3.6984e-04, 1.5069e-04, 1.6182e-04, 1.9131e-04],
        [3.5131e-04, 3.6608e-04, 1.4915e-04, 1.6018e-04, 1.8937e-04],
        [3.5008e-04, 3.6481e-04, 1.4863e-04, 1.5962e-04, 1.8871e-04],
        [1.5450e-04, 1.6100e-04, 6.5593e-05, 7.0441e-05, 8.3278e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(212.1891, grad_fn=<NegBackward>) non event loss:  tensor([33.4602], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0433, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.2400e-04, 8.0149e-05, 8.2627e-05, 1.2113e-04, 7.7845e-05, 8.0342e-05,
         7.1128e-05],
        [1.1796e-04, 7.6242e-05, 7.8599e-05, 1.1523e-04, 7.4050e-05, 7.6426e-05,
         6.7661e-05],
        [2.8758e-05, 1.8587e-05, 1.9162e-05, 2.8092e-05, 1.8053e-05, 1.8632e-05,
         1.6495e-05],
        [2.5256e-05, 1.6323e-05, 1.6828e-05, 2.4670e-05, 1.5854e-05, 1.6363e-05,
         1.4486e-05],
        [2.4916e-05, 1.6104e-05, 1.6602e-05, 2.4339e-05, 1.5641e-05, 1.6143e-05,
         1.4292e-05],
        [2.5944e-05, 1.6768e-05, 1.7287e-05, 2.5343e-05, 1.6286e-05, 1.6809e-05,
         1.4881e-05],
        [3.4748e-05, 2.2459e-05, 2.3153e-05, 3.3943e-05, 2.1813e-05, 2.2513e-05,
         1.9931e-05]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(509.8221, grad_fn=<NegBackward>) non event loss:  tensor([27.5927], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1052, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.2253e-04, 2.8321e-04, 2.8771e-04,  ..., 2.9642e-04, 4.2862e-04,
         2.2028e-04],
        [2.7925e-04, 2.4521e-04, 2.4911e-04,  ..., 2.5665e-04, 3.7111e-04,
         1.9073e-04],
        [2.7547e-04, 2.4189e-04, 2.4574e-04,  ..., 2.5317e-04, 3.6609e-04,
         1.8815e-04],
        ...,
        [2.1551e-05, 1.8923e-05, 1.9224e-05,  ..., 1.9806e-05, 2.8641e-05,
         1.4718e-05],
        [1.5863e-05, 1.3929e-05, 1.4150e-05,  ..., 1.4579e-05, 2.1082e-05,
         1.0834e-05],
        [1.6123e-05, 1.4157e-05, 1.4383e-05,  ..., 1.4818e-05, 2.1428e-05,
         1.1012e-05]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(13457.3740, grad_fn=<NegBackward>) non event loss:  tensor([67.2349], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.3332, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.7080e-04, 4.6503e-04, 3.5717e-04, 4.5474e-04, 4.2471e-04],
        [3.6346e-04, 4.5582e-04, 3.5010e-04, 4.4573e-04, 4.1631e-04],
        [1.7249e-04, 2.1633e-04, 1.6615e-04, 2.1154e-04, 1.9757e-04],
        [1.1751e-04, 1.4738e-04, 1.1319e-04, 1.4411e-04, 1.3460e-04],
        [4.0147e-05, 5.0351e-05, 3.8671e-05, 4.9237e-05, 4.5985e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(215.6399, grad_fn=<NegBackward>) non event loss:  tensor([33.3330], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0536, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.5816e-04, 4.8942e-04, 2.6520e-04, 4.0812e-04, 4.3279e-04],
        [7.6711e-05, 8.1947e-05, 4.4400e-05, 6.8332e-05, 7.2464e-05],
        [1.6829e-04, 1.7977e-04, 9.7404e-05, 1.4990e-04, 1.5897e-04],
        [7.6488e-05, 8.1709e-05, 4.4270e-05, 6.8134e-05, 7.2253e-05],
        [6.2673e-05, 6.6950e-05, 3.6274e-05, 5.5827e-05, 5.9202e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(228.3148, grad_fn=<NegBackward>) non event loss:  tensor([17.6538], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0270, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[7.9338e-04, 5.2818e-04, 5.4923e-04, 1.5940e-03],
        [3.8322e-04, 2.5510e-04, 2.6527e-04, 7.7010e-04],
        [1.4388e-04, 9.5775e-05, 9.9594e-05, 2.8917e-04],
        [8.0421e-05, 5.3532e-05, 5.5666e-05, 1.6164e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(133.4310, grad_fn=<NegBackward>) non event loss:  tensor([27.5287], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0632, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.3118e-03, 1.4400e-03, 1.3708e-03, 1.3469e-03, 8.4192e-04, 1.6762e-03,
         9.9900e-04, 1.4795e-03, 1.2569e-03, 1.4429e-03],
        [1.2476e-03, 1.3696e-03, 1.3037e-03, 1.2810e-03, 8.0071e-04, 1.5942e-03,
         9.5010e-04, 1.4071e-03, 1.1954e-03, 1.3722e-03],
        [1.1743e-03, 1.2891e-03, 1.2271e-03, 1.2057e-03, 7.5366e-04, 1.5005e-03,
         8.9427e-04, 1.3244e-03, 1.1251e-03, 1.2916e-03],
        [3.9193e-04, 4.3026e-04, 4.0957e-04, 4.0242e-04, 2.5151e-04, 5.0086e-04,
         2.9844e-04, 4.4206e-04, 3.7552e-04, 4.3111e-04],
        [1.6306e-04, 1.7902e-04, 1.7041e-04, 1.6743e-04, 1.0464e-04, 2.0840e-04,
         1.2417e-04, 1.8393e-04, 1.5624e-04, 1.7937e-04],
        [1.5645e-04, 1.7175e-04, 1.6349e-04, 1.6064e-04, 1.0039e-04, 1.9994e-04,
         1.1913e-04, 1.7646e-04, 1.4990e-04, 1.7209e-04],
        [1.4119e-04, 1.5500e-04, 1.4755e-04, 1.4497e-04, 9.0600e-05, 1.8044e-04,
         1.0751e-04, 1.5926e-04, 1.3528e-04, 1.5531e-04],
        [1.0467e-04, 1.1491e-04, 1.0939e-04, 1.0748e-04, 6.7167e-05, 1.3377e-04,
         7.9703e-05, 1.1806e-04, 1.0029e-04, 1.1514e-04],
        [9.5381e-05, 1.0471e-04, 9.9676e-05, 9.7935e-05, 6.1204e-05, 1.2190e-04,
         7.2628e-05, 1.0758e-04, 9.1387e-05, 1.0492e-04],
        [1.2575e-04, 1.3805e-04, 1.3141e-04, 1.2912e-04, 8.0692e-05, 1.6071e-04,
         9.5753e-05, 1.4184e-04, 1.2048e-04, 1.3832e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(818.0171, grad_fn=<NegBackward>) non event loss:  tensor([101.7260], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1612, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[5.0601e-04, 4.5508e-04, 3.3475e-04, 1.9847e-04, 2.1396e-04, 3.5316e-04,
         3.7853e-04, 3.1724e-04, 2.7661e-04, 4.1248e-04],
        [4.0810e-04, 3.6702e-04, 2.6997e-04, 1.6006e-04, 1.7255e-04, 2.8482e-04,
         3.0528e-04, 2.5585e-04, 2.2308e-04, 3.3266e-04],
        [2.3217e-04, 2.0880e-04, 1.5359e-04, 9.1055e-05, 9.8162e-05, 1.6204e-04,
         1.7367e-04, 1.4555e-04, 1.2691e-04, 1.8925e-04],
        [1.6927e-04, 1.5223e-04, 1.1197e-04, 6.6385e-05, 7.1567e-05, 1.1814e-04,
         1.2662e-04, 1.0612e-04, 9.2525e-05, 1.3798e-04],
        [1.7093e-04, 1.5373e-04, 1.1307e-04, 6.7036e-05, 7.2269e-05, 1.1930e-04,
         1.2786e-04, 1.0716e-04, 9.3433e-05, 1.3933e-04],
        [1.6618e-04, 1.4945e-04, 1.0993e-04, 6.5171e-05, 7.0258e-05, 1.1598e-04,
         1.2431e-04, 1.0418e-04, 9.0833e-05, 1.3546e-04],
        [1.5950e-04, 1.4344e-04, 1.0551e-04, 6.2552e-05, 6.7435e-05, 1.1132e-04,
         1.1931e-04, 9.9993e-05, 8.7183e-05, 1.3001e-04],
        [1.1252e-04, 1.0119e-04, 7.4429e-05, 4.4126e-05, 4.7570e-05, 7.8525e-05,
         8.4165e-05, 7.0537e-05, 6.1501e-05, 9.1715e-05],
        [1.1983e-04, 1.0776e-04, 7.9265e-05, 4.6993e-05, 5.0661e-05, 8.3627e-05,
         8.9634e-05, 7.5120e-05, 6.5497e-05, 9.7675e-05],
        [1.0938e-04, 9.8371e-05, 7.2356e-05, 4.2896e-05, 4.6245e-05, 7.6338e-05,
         8.1820e-05, 6.8572e-05, 5.9787e-05, 8.9160e-05]],
       grad_fn=<SoftplusBackward>)

 19%|███████████████████████████                                                                                                                    | 94/496 [00:17<00:54,  7.33it/s]
-1
-1
### event lambdas:  tensor([[0.0026, 0.0028, 0.0017, 0.0017, 0.0016, 0.0020],
        [0.0007, 0.0008, 0.0005, 0.0005, 0.0004, 0.0006],
        [0.0006, 0.0007, 0.0004, 0.0004, 0.0004, 0.0005],
        [0.0006, 0.0006, 0.0004, 0.0004, 0.0004, 0.0005],
        [0.0005, 0.0006, 0.0004, 0.0003, 0.0003, 0.0004],
        [0.0005, 0.0006, 0.0003, 0.0003, 0.0003, 0.0004]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(267.6938, grad_fn=<NegBackward>) non event loss:  tensor([216.7518], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1268, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0006, 0.0006, 0.0004, 0.0010, 0.0006, 0.0006, 0.0005],
        [0.0005, 0.0005, 0.0004, 0.0009, 0.0006, 0.0005, 0.0004],
        [0.0006, 0.0006, 0.0004, 0.0010, 0.0006, 0.0006, 0.0005],
        [0.0005, 0.0006, 0.0004, 0.0009, 0.0006, 0.0005, 0.0005],
        [0.0005, 0.0006, 0.0004, 0.0009, 0.0006, 0.0005, 0.0005],
        [0.0004, 0.0004, 0.0002, 0.0006, 0.0004, 0.0003, 0.0003],
        [0.0002, 0.0002, 0.0001, 0.0003, 0.0002, 0.0002, 0.0002]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(376.9199, grad_fn=<NegBackward>) non event loss:  tensor([242.9042], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0873, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0028, 0.0017, 0.0016],
        [0.0016, 0.0010, 0.0009],
        [0.0016, 0.0010, 0.0009]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(59.4921, grad_fn=<NegBackward>) non event loss:  tensor([38.9211], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0479, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.8936e-03, 2.0687e-03, 2.7063e-03, 1.6259e-03, 2.3062e-03, 2.5548e-03,
         1.5987e-03],
        [7.5951e-04, 8.2978e-04, 1.0858e-03, 6.5207e-04, 9.2513e-04, 1.0249e-03,
         6.4117e-04],
        [7.1201e-04, 7.7789e-04, 1.0179e-03, 6.1129e-04, 8.6728e-04, 9.6085e-04,
         6.0107e-04],
        [2.1687e-04, 2.3694e-04, 3.1007e-04, 1.8619e-04, 2.6418e-04, 2.9269e-04,
         1.8307e-04],
        [1.4360e-04, 1.5690e-04, 2.0532e-04, 1.2329e-04, 1.7493e-04, 1.9381e-04,
         1.2122e-04],
        [1.4339e-04, 1.5666e-04, 2.0501e-04, 1.2310e-04, 1.7466e-04, 1.9352e-04,
         1.2104e-04],
        [1.0315e-04, 1.1269e-04, 1.4748e-04, 8.8552e-05, 1.2565e-04, 1.3921e-04,
         8.7071e-05]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(387.7476, grad_fn=<NegBackward>) non event loss:  tensor([69.3391], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1096, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.0768e-03, 8.5917e-04, 1.3810e-03, 1.2990e-03, 8.6769e-04, 1.1176e-03,
         7.5497e-04, 9.0812e-04, 1.0176e-03, 8.4791e-04, 1.3850e-03, 8.6531e-04,
         1.1298e-03, 1.4461e-03, 1.4482e-03],
        [1.0366e-03, 8.2710e-04, 1.3295e-03, 1.2505e-03, 8.3530e-04, 1.0759e-03,
         7.2679e-04, 8.7423e-04, 9.7961e-04, 8.1626e-04, 1.3334e-03, 8.3301e-04,
         1.0876e-03, 1.3921e-03, 1.3942e-03],
        [9.7525e-04, 7.7811e-04, 1.2508e-03, 1.1764e-03, 7.8583e-04, 1.0122e-03,
         6.8374e-04, 8.2245e-04, 9.2159e-04, 7.6791e-04, 1.2544e-03, 7.8368e-04,
         1.0232e-03, 1.3097e-03, 1.3116e-03],
        [6.7909e-04, 5.4180e-04, 8.7097e-04, 8.1921e-04, 5.4717e-04, 7.0479e-04,
         4.7608e-04, 5.7267e-04, 6.4172e-04, 5.3470e-04, 8.7350e-04, 5.4567e-04,
         7.1250e-04, 9.1200e-04, 9.1335e-04],
        [2.0749e-04, 1.6553e-04, 2.6613e-04, 2.5031e-04, 1.6718e-04, 2.1534e-04,
         1.4545e-04, 1.7497e-04, 1.9607e-04, 1.6336e-04, 2.6691e-04, 1.6672e-04,
         2.1770e-04, 2.7867e-04, 2.7909e-04],
        [1.7908e-04, 1.4287e-04, 2.2969e-04, 2.1604e-04, 1.4428e-04, 1.8586e-04,
         1.2554e-04, 1.5101e-04, 1.6922e-04, 1.4099e-04, 2.3036e-04, 1.4389e-04,
         1.8789e-04, 2.4052e-04, 2.4088e-04],
        [9.5310e-05, 7.6037e-05, 1.2225e-04, 1.1498e-04, 7.6791e-05, 9.8918e-05,
         6.6812e-05, 8.0371e-05, 9.0064e-05, 7.5040e-05, 1.2261e-04, 7.6581e-05,
         1.0000e-04, 1.2801e-04, 1.2820e-04],
        [9.8767e-05, 7.8795e-05, 1.2668e-04, 1.1915e-04, 7.9577e-05, 1.0251e-04,
         6.9236e-05, 8.3287e-05, 9.3331e-05, 7.7762e-05, 1.2705e-04, 7.9359e-05,
         1.0363e-04, 1.3266e-04, 1.3285e-04],
        [1.0978e-04, 8.7584e-05, 1.4081e-04, 1.3244e-04, 8.8453e-05, 1.1394e-04,
         7.6958e-05, 9.2576e-05, 1.0374e-04, 8.6436e-05, 1.4123e-04, 8.8210e-05,
         1.1519e-04, 1.4745e-04, 1.4767e-04],
        [2.6329e-04, 2.1005e-04, 3.3770e-04, 3.1763e-04, 2.1213e-04, 2.7325e-04,
         1.8457e-04, 2.2202e-04, 2.4880e-04, 2.0730e-04, 3.3868e-04, 2.1155e-04,
         2.7624e-04, 3.5361e-04, 3.5414e-04],
        [1.3753e-04, 1.0972e-04, 1.7640e-04, 1.6591e-04, 1.1080e-04, 1.4273e-04,
         9.6406e-05, 1.1597e-04, 1.2996e-04, 1.0828e-04, 1.7691e-04, 1.1050e-04,
         1.4429e-04, 1.8471e-04, 1.8498e-04],
        [1.1885e-04, 9.4814e-05, 1.5244e-04, 1.4338e-04, 9.5754e-05, 1.2334e-04,
         8.3311e-05, 1.0022e-04, 1.1230e-04, 9.3571e-05, 1.5288e-04, 9.5492e-05,
         1.2469e-04, 1.5962e-04, 1.5986e-04],
        [1.3193e-04, 1.0526e-04, 1.6923e-04, 1.5917e-04, 1.0630e-04, 1.3693e-04,
         9.2486e-05, 1.1125e-04, 1.2467e-04, 1.0388e-04, 1.6972e-04, 1.0601e-04,
         1.3843e-04, 1.7720e-04, 1.7746e-04],
        [1.2452e-04, 9.9338e-05, 1.5971e-04, 1.5022e-04, 1.0032e-04, 1.2923e-04,
         8.7286e-05, 1.0500e-04, 1.1766e-04, 9.8035e-05, 1.6018e-04, 1.0005e-04,
         1.3064e-04, 1.6724e-04, 1.6749e-04],
        [1.3544e-04, 1.0806e-04, 1.7373e-04, 1.6340e-04, 1.0913e-04, 1.4057e-04,
         9.4947e-05, 1.1421e-04, 1.2799e-04, 1.0664e-04, 1.7423e-04, 1.0883e-04,
         1.4211e-04, 1.8191e-04, 1.8218e-04]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1886.3901, grad_fn=<NegBackward>) non event loss:  tensor([407.8590], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1882, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[5.8163e-04, 3.4340e-04, 7.1773e-04, 3.4669e-04, 6.4235e-04, 6.6960e-04,
         8.0543e-04, 6.3608e-04],
        [5.8816e-04, 3.4726e-04, 7.2579e-04, 3.5059e-04, 6.4956e-04, 6.7712e-04,
         8.1447e-04, 6.4322e-04],
        [6.2657e-04, 3.6994e-04, 7.7319e-04, 3.7349e-04, 6.9199e-04, 7.2135e-04,
         8.6767e-04, 6.8523e-04],
        [6.1179e-04, 3.6121e-04, 7.5494e-04, 3.6468e-04, 6.7566e-04, 7.0433e-04,
         8.4720e-04, 6.6906e-04],
        [6.6084e-04, 3.9018e-04, 8.1547e-04, 3.9392e-04, 7.2983e-04, 7.6080e-04,
         9.1512e-04, 7.2271e-04],
        [1.2674e-04, 7.4825e-05, 1.5641e-04, 7.5543e-05, 1.3998e-04, 1.4592e-04,
         1.7553e-04, 1.3861e-04],
        [1.1855e-04, 6.9988e-05, 1.4630e-04, 7.0660e-05, 1.3093e-04, 1.3649e-04,
         1.6419e-04, 1.2965e-04],
        [9.4621e-05, 5.5860e-05, 1.1677e-04, 5.6396e-05, 1.0450e-04, 1.0894e-04,
         1.3104e-04, 1.0348e-04]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(515.5402, grad_fn=<NegBackward>) non event loss:  tensor([125.5082], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0724, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[6.1542e-04, 4.2007e-04, 4.2194e-04, 9.3154e-04],
        [4.4936e-04, 3.0671e-04, 3.0808e-04, 6.8020e-04],
        [3.8338e-04, 2.6168e-04, 2.6284e-04, 5.8035e-04],
        [1.3286e-04, 9.0683e-05, 9.1086e-05, 2.0114e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(128.9535, grad_fn=<NegBackward>) non event loss:  tensor([43.9322], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0513, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.5740e-04, 1.1444e-04, 2.0512e-04, 1.0469e-04, 1.7877e-04, 1.4516e-04,
         1.5278e-04, 1.2007e-04, 1.5741e-04, 1.5612e-04, 1.2675e-04, 2.1897e-04,
         1.7220e-04],
        [1.1618e-04, 8.4469e-05, 1.5140e-04, 7.7273e-05, 1.3196e-04, 1.0715e-04,
         1.1277e-04, 8.8625e-05, 1.1619e-04, 1.1524e-04, 9.3553e-05, 1.6162e-04,
         1.2710e-04],
        [9.3563e-05, 6.8024e-05, 1.2193e-04, 6.2228e-05, 1.0627e-04, 8.6286e-05,
         9.0814e-05, 7.1370e-05, 9.3568e-05, 9.2801e-05, 7.5339e-05, 1.3016e-04,
         1.0236e-04],
        [1.4364e-04, 1.0443e-04, 1.8718e-04, 9.5532e-05, 1.6314e-04, 1.3246e-04,
         1.3941e-04, 1.0957e-04, 1.4364e-04, 1.4247e-04, 1.1566e-04, 1.9981e-04,
         1.5714e-04],
        [1.5753e-04, 1.1453e-04, 2.0528e-04, 1.0477e-04, 1.7891e-04, 1.4528e-04,
         1.5290e-04, 1.2016e-04, 1.5753e-04, 1.5624e-04, 1.2684e-04, 2.1914e-04,
         1.7233e-04],
        [2.3992e-04, 1.7443e-04, 3.1264e-04, 1.5957e-04, 2.7249e-04, 2.2126e-04,
         2.3287e-04, 1.8301e-04, 2.3993e-04, 2.3796e-04, 1.9319e-04, 3.3375e-04,
         2.6247e-04],
        [1.0080e-04, 7.3285e-05, 1.3136e-04, 6.7042e-05, 1.1448e-04, 9.2960e-05,
         9.7838e-05, 7.6890e-05, 1.0080e-04, 9.9979e-05, 8.1166e-05, 1.4023e-04,
         1.1028e-04],
        [1.0930e-04, 7.9467e-05, 1.4244e-04, 7.2697e-05, 1.2414e-04, 1.0080e-04,
         1.0609e-04, 8.3376e-05, 1.0931e-04, 1.0841e-04, 8.8013e-05, 1.5205e-04,
         1.1958e-04],
        [9.1499e-05, 6.6523e-05, 1.1924e-04, 6.0856e-05, 1.0392e-04, 8.4383e-05,
         8.8810e-05, 6.9795e-05, 9.1504e-05, 9.0754e-05, 7.3677e-05, 1.2729e-04,
         1.0010e-04],
        [7.0072e-05, 5.0945e-05, 9.1314e-05, 4.6604e-05, 7.9585e-05, 6.4622e-05,
         6.8013e-05, 5.3451e-05, 7.0075e-05, 6.9501e-05, 5.6423e-05, 9.7479e-05,
         7.6659e-05],
        [6.6917e-05, 4.8651e-05, 8.7203e-05, 4.4507e-05, 7.6003e-05, 6.1713e-05,
         6.4951e-05, 5.1045e-05, 6.6921e-05, 6.6373e-05, 5.3883e-05, 9.3091e-05,
         7.3208e-05],
        [1.3887e-04, 1.0096e-04, 1.8097e-04, 9.2363e-05, 1.5772e-04, 1.2807e-04,
         1.3479e-04, 1.0593e-04, 1.3888e-04, 1.3774e-04, 1.1182e-04, 1.9318e-04,
         1.5192e-04],
        [3.5238e-04, 2.5620e-04, 4.5919e-04, 2.3438e-04, 4.0022e-04, 3.2498e-04,
         3.4203e-04, 2.6881e-04, 3.5240e-04, 3.4951e-04, 2.8375e-04, 4.9019e-04,
         3.8551e-04]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1523.4426, grad_fn=<NegBackward>) non event loss:  tensor([116.4532], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1635, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0005, 0.0006, 0.0004, 0.0010, 0.0006],
        [0.0002, 0.0003, 0.0002, 0.0005, 0.0003],
        [0.0002, 0.0003, 0.0002, 0.0005, 0.0003],
        [0.0002, 0.0003, 0.0002, 0.0004, 0.0003],
        [0.0002, 0.0002, 0.0002, 0.0004, 0.0002]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(202.1935, grad_fn=<NegBackward>) non event loss:  tensor([99.1087], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1015, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[9.3843e-04, 1.1523e-03, 1.1154e-03, 7.9168e-04],
        [8.7596e-04, 1.0756e-03, 1.0411e-03, 7.3897e-04],
        [1.2302e-04, 1.5107e-04, 1.4622e-04, 1.0377e-04],
        [7.5163e-05, 9.2304e-05, 8.9342e-05, 6.3405e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(129.2099, grad_fn=<NegBackward>) non event loss:  tensor([46.9125], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0574, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0005, 0.0007, 0.0003],
        [0.0004, 0.0006, 0.0003],
        [0.0002, 0.0003, 0.0001]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(72.0902, grad_fn=<NegBackward>) non event loss:  tensor([4.0346], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0390, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.0574e-04, 3.1498e-04, 5.3374e-04, 3.5732e-04, 3.1984e-04, 2.9716e-04,
         3.5269e-04],
        [2.7565e-05, 4.2203e-05, 7.1520e-05, 4.7877e-05, 4.2854e-05, 3.9815e-05,
         4.7257e-05],
        [4.1096e-05, 6.2918e-05, 1.0663e-04, 7.1377e-05, 6.3888e-05, 5.9357e-05,
         7.0452e-05],
        [3.1390e-05, 4.8059e-05, 8.1444e-05, 5.4520e-05, 4.8800e-05, 4.5339e-05,
         5.3814e-05],
        [4.9245e-05, 7.5395e-05, 1.2777e-04, 8.5531e-05, 7.6558e-05, 7.1128e-05,
         8.4423e-05],
        [3.2591e-05, 4.9897e-05, 8.4559e-05, 5.6605e-05, 5.0666e-05, 4.7073e-05,
         5.5872e-05],
        [3.5349e-05, 5.4120e-05, 9.1716e-05, 6.1396e-05, 5.4955e-05, 5.1057e-05,
         6.0601e-05]], grad_fn=<SoftplusBackward>)

 21%|█████████████████████████████▊                                                                                                                | 104/496 [00:19<01:10,  5.60it/s]
-1
-1
### event lambdas:  tensor([[9.4105e-04, 9.0919e-04, 8.7874e-04, 9.2912e-04, 4.4657e-04, 5.4306e-04,
         7.5345e-04],
        [1.6466e-04, 1.5909e-04, 1.5376e-04, 1.6258e-04, 7.8124e-05, 9.5009e-05,
         1.3183e-04],
        [1.4490e-04, 1.3999e-04, 1.3530e-04, 1.4306e-04, 6.8745e-05, 8.3603e-05,
         1.1600e-04],
        [7.6924e-05, 7.4318e-05, 7.1829e-05, 7.5948e-05, 3.6495e-05, 4.4383e-05,
         6.1584e-05],
        [6.5520e-05, 6.3300e-05, 6.1180e-05, 6.4689e-05, 3.1085e-05, 3.7803e-05,
         5.2454e-05],
        [7.4100e-05, 7.1590e-05, 6.9192e-05, 7.3160e-05, 3.5156e-05, 4.2754e-05,
         5.9323e-05],
        [7.2699e-05, 7.0236e-05, 6.7883e-05, 7.1777e-05, 3.4491e-05, 4.1945e-05,
         5.8201e-05]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(450.0890, grad_fn=<NegBackward>) non event loss:  tensor([34.5991], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1194, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.7294e-04, 3.0060e-04, 4.1822e-04, 2.7537e-04, 4.5016e-04, 3.2470e-04],
        [1.4520e-04, 2.5238e-04, 3.5114e-04, 2.3120e-04, 3.7796e-04, 2.7262e-04],
        [1.5719e-04, 2.7322e-04, 3.8014e-04, 2.5029e-04, 4.0917e-04, 2.9513e-04],
        [1.3040e-04, 2.2666e-04, 3.1535e-04, 2.0764e-04, 3.3944e-04, 2.4483e-04],
        [7.3503e-05, 1.2777e-04, 1.7777e-04, 1.1704e-04, 1.9135e-04, 1.3801e-04],
        [1.0409e-04, 1.8094e-04, 2.5174e-04, 1.6575e-04, 2.7097e-04, 1.9545e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(302.4213, grad_fn=<NegBackward>) non event loss:  tensor([83.8169], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1131, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0006, 0.0007, 0.0005, 0.0005, 0.0005, 0.0004, 0.0006],
        [0.0005, 0.0006, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005],
        [0.0005, 0.0006, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005],
        [0.0005, 0.0006, 0.0004, 0.0004, 0.0004, 0.0003, 0.0005],
        [0.0005, 0.0006, 0.0004, 0.0004, 0.0004, 0.0003, 0.0005],
        [0.0004, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004],
        [0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0001, 0.0002]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(384.9814, grad_fn=<NegBackward>) non event loss:  tensor([164.3277], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0898, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[5.3026e-04, 3.6750e-04, 2.4050e-04, 2.9741e-04],
        [2.4128e-04, 1.6721e-04, 1.0942e-04, 1.3532e-04],
        [5.4866e-04, 3.8025e-04, 2.4884e-04, 3.0773e-04],
        [1.2296e-04, 8.5210e-05, 5.5760e-05, 6.8958e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(136.4763, grad_fn=<NegBackward>) non event loss:  tensor([27.1247], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0796, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.9011e-04, 5.2778e-04, 5.9765e-04, 2.6173e-04, 4.7610e-04, 5.1674e-04,
         7.1884e-04, 5.2797e-04, 7.3657e-04, 4.0779e-04, 4.6156e-04, 5.9266e-04,
         7.2232e-04, 3.6101e-04, 2.9868e-04, 3.6909e-04, 6.7098e-04],
        [2.0765e-04, 3.7778e-04, 4.2780e-04, 1.8733e-04, 3.4078e-04, 3.6988e-04,
         5.1455e-04, 3.7792e-04, 5.2724e-04, 2.9189e-04, 3.3038e-04, 4.2423e-04,
         5.1704e-04, 2.5840e-04, 2.1379e-04, 2.6418e-04, 4.8029e-04],
        [1.8203e-04, 3.3117e-04, 3.7502e-04, 1.6422e-04, 2.9874e-04, 3.2424e-04,
         4.5107e-04, 3.3129e-04, 4.6220e-04, 2.5587e-04, 2.8961e-04, 3.7189e-04,
         4.5325e-04, 2.2652e-04, 1.8741e-04, 2.3159e-04, 4.2104e-04],
        [1.2965e-04, 2.3588e-04, 2.6711e-04, 1.1696e-04, 2.1278e-04, 2.3094e-04,
         3.2129e-04, 2.3596e-04, 3.2921e-04, 1.8225e-04, 2.0628e-04, 2.6488e-04,
         3.2284e-04, 1.6134e-04, 1.3348e-04, 1.6495e-04, 2.9989e-04],
        [5.9313e-05, 1.0791e-04, 1.2220e-04, 5.3508e-05, 9.7344e-05, 1.0566e-04,
         1.4699e-04, 1.0795e-04, 1.5062e-04, 8.3375e-05, 9.4371e-05, 1.2118e-04,
         1.4770e-04, 7.3809e-05, 6.1066e-05, 7.5461e-05, 1.3720e-04],
        [5.3596e-05, 9.7513e-05, 1.1043e-04, 4.8351e-05, 8.7962e-05, 9.5473e-05,
         1.3282e-04, 9.7548e-05, 1.3610e-04, 7.5340e-05, 8.5276e-05, 1.0950e-04,
         1.3347e-04, 6.6695e-05, 5.5180e-05, 6.8188e-05, 1.2398e-04],
        [4.4766e-05, 8.1449e-05, 9.2234e-05, 4.0386e-05, 7.3471e-05, 7.9744e-05,
         1.1094e-04, 8.1478e-05, 1.1368e-04, 6.2928e-05, 7.1227e-05, 9.1464e-05,
         1.1148e-04, 5.5708e-05, 4.6089e-05, 5.6955e-05, 1.0355e-04],
        [3.7088e-05, 6.7478e-05, 7.6414e-05, 3.3458e-05, 6.0869e-05, 6.6066e-05,
         9.1913e-05, 6.7503e-05, 9.4180e-05, 5.2134e-05, 5.9010e-05, 7.5776e-05,
         9.2358e-05, 4.6152e-05, 3.8184e-05, 4.7185e-05, 8.5792e-05],
        [1.7780e-04, 3.2348e-04, 3.6631e-04, 1.6040e-04, 2.9180e-04, 3.1671e-04,
         4.4059e-04, 3.2359e-04, 4.5146e-04, 2.4993e-04, 2.8289e-04, 3.6325e-04,
         4.4273e-04, 2.2125e-04, 1.8306e-04, 2.2621e-04, 4.1126e-04],
        [2.6073e-05, 4.7438e-05, 5.3720e-05, 2.3522e-05, 4.2792e-05, 4.6445e-05,
         6.4616e-05, 4.7455e-05, 6.6210e-05, 3.6651e-05, 4.1485e-05, 5.3271e-05,
         6.4929e-05, 3.2446e-05, 2.6844e-05, 3.3172e-05, 6.0313e-05],
        [2.1324e-05, 3.8798e-05, 4.3935e-05, 1.9237e-05, 3.4997e-05, 3.7986e-05,
         5.2847e-05, 3.8812e-05, 5.4151e-05, 2.9975e-05, 3.3929e-05, 4.3568e-05,
         5.3103e-05, 2.6536e-05, 2.1954e-05, 2.7130e-05, 4.9328e-05],
        [1.8507e-05, 3.3672e-05, 3.8131e-05, 1.6696e-05, 3.0374e-05, 3.2967e-05,
         4.5865e-05, 3.3684e-05, 4.6997e-05, 2.6015e-05, 2.9446e-05, 3.7812e-05,
         4.6087e-05, 2.3030e-05, 1.9054e-05, 2.3546e-05, 4.2811e-05],
        [1.5803e-05, 2.8752e-05, 3.2559e-05, 1.4256e-05, 2.5936e-05, 2.8150e-05,
         3.9164e-05, 2.8762e-05, 4.0130e-05, 2.2214e-05, 2.5144e-05, 3.2287e-05,
         3.9353e-05, 1.9665e-05, 1.6270e-05, 2.0105e-05, 3.6556e-05],
        [1.8768e-05, 3.4148e-05, 3.8670e-05, 1.6932e-05, 3.0803e-05, 3.3434e-05,
         4.6514e-05, 3.4160e-05, 4.7661e-05, 2.6383e-05, 2.9863e-05, 3.8347e-05,
         4.6739e-05, 2.3356e-05, 1.9323e-05, 2.3879e-05, 4.3416e-05],
        [9.5455e-06, 1.7368e-05, 1.9667e-05, 8.6114e-06, 1.5666e-05, 1.7004e-05,
         2.3657e-05, 1.7374e-05, 2.4240e-05, 1.3418e-05, 1.5188e-05, 1.9503e-05,
         2.3771e-05, 1.1879e-05, 9.8277e-06, 1.2144e-05, 2.2081e-05],
        [1.0271e-05, 1.8688e-05, 2.1163e-05, 9.2662e-06, 1.6858e-05, 1.8297e-05,
         2.5456e-05, 1.8695e-05, 2.6084e-05, 1.4439e-05, 1.6343e-05, 2.0986e-05,
         2.5579e-05, 1.2782e-05, 1.0575e-05, 1.3068e-05, 2.3760e-05],
        [1.2549e-05, 2.2833e-05, 2.5856e-05, 1.1321e-05, 2.0596e-05, 2.2355e-05,
         3.1101e-05, 2.2841e-05, 3.1868e-05, 1.7641e-05, 1.9967e-05, 2.5640e-05,
         3.1251e-05, 1.5616e-05, 1.2920e-05, 1.5966e-05, 2.9030e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(2764.0999, grad_fn=<NegBackward>) non event loss:  tensor([69.1071], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1628, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.6143e-04, 3.0778e-04, 5.3663e-04, 4.8434e-04, 3.3333e-04],
        [1.3603e-04, 1.1584e-04, 2.0198e-04, 1.8230e-04, 1.2546e-04],
        [5.7531e-05, 4.8990e-05, 8.5424e-05, 7.7099e-05, 5.3058e-05],
        [4.4879e-05, 3.8216e-05, 6.6638e-05, 6.0143e-05, 4.1389e-05],
        [5.9457e-05, 5.0630e-05, 8.8283e-05, 7.9679e-05, 5.4833e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(229.4332, grad_fn=<NegBackward>) non event loss:  tensor([5.3730], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0820, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.7394e-04, 2.7537e-04, 2.7610e-04, 1.8910e-04, 2.0190e-04, 4.5546e-04,
         2.5149e-04, 3.0144e-04, 4.0392e-04, 3.3551e-04, 1.7778e-04],
        [6.3927e-05, 6.4261e-05, 6.4430e-05, 4.4127e-05, 4.7113e-05, 1.0629e-04,
         5.8686e-05, 7.0344e-05, 9.4262e-05, 7.8296e-05, 4.1486e-05],
        [3.7363e-05, 3.7558e-05, 3.7657e-05, 2.5790e-05, 2.7536e-05, 6.2124e-05,
         3.4300e-05, 4.1114e-05, 5.5093e-05, 4.5761e-05, 2.4247e-05],
        [3.1531e-05, 3.1695e-05, 3.1779e-05, 2.1765e-05, 2.3238e-05, 5.2427e-05,
         2.8946e-05, 3.4696e-05, 4.6494e-05, 3.8618e-05, 2.0462e-05],
        [1.0808e-04, 1.0864e-04, 1.0893e-04, 7.4603e-05, 7.9653e-05, 1.7970e-04,
         9.9217e-05, 1.1893e-04, 1.5936e-04, 1.3237e-04, 7.0138e-05],
        [2.8653e-05, 2.8803e-05, 2.8878e-05, 1.9778e-05, 2.1117e-05, 4.7642e-05,
         2.6304e-05, 3.1529e-05, 4.2250e-05, 3.5094e-05, 1.8594e-05],
        [2.3560e-05, 2.3683e-05, 2.3745e-05, 1.6263e-05, 1.7363e-05, 3.9174e-05,
         2.1628e-05, 2.5925e-05, 3.4740e-05, 2.8856e-05, 1.5289e-05],
        [1.8331e-05, 1.8426e-05, 1.8475e-05, 1.2653e-05, 1.3509e-05, 3.0479e-05,
         1.6828e-05, 2.0171e-05, 2.7029e-05, 2.2451e-05, 1.1896e-05],
        [1.6439e-05, 1.6525e-05, 1.6569e-05, 1.1348e-05, 1.2116e-05, 2.7334e-05,
         1.5092e-05, 1.8090e-05, 2.4241e-05, 2.0135e-05, 1.0668e-05],
        [1.8918e-05, 1.9017e-05, 1.9067e-05, 1.3058e-05, 1.3942e-05, 3.1456e-05,
         1.7367e-05, 2.0817e-05, 2.7896e-05, 2.3171e-05, 1.2277e-05],
        [1.8642e-05, 1.8740e-05, 1.8789e-05, 1.2868e-05, 1.3739e-05, 3.0997e-05,
         1.7114e-05, 2.0514e-05, 2.7489e-05, 2.2833e-05, 1.2098e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(1235.7661, grad_fn=<NegBackward>) non event loss:  tensor([25.0110], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1846, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.5627e-04, 2.3642e-04, 3.0223e-04, 3.2294e-04, 3.1501e-04, 3.1022e-04],
        [4.6234e-05, 6.9949e-05, 8.9424e-05, 9.5552e-05, 9.3206e-05, 9.1786e-05],
        [3.9210e-05, 5.9321e-05, 7.5838e-05, 8.1035e-05, 7.9045e-05, 7.7841e-05],
        [2.0224e-05, 3.0598e-05, 3.9117e-05, 4.1798e-05, 4.0772e-05, 4.0151e-05],
        [1.1176e-05, 1.6908e-05, 2.1616e-05, 2.3097e-05, 2.2530e-05, 2.2187e-05],
        [9.8458e-06, 1.4896e-05, 1.9044e-05, 2.0349e-05, 1.9849e-05, 1.9547e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(356.6680, grad_fn=<NegBackward>) non event loss:  tensor([18.6704], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0847, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.1000e-04, 8.7332e-05, 8.3760e-05, 9.0009e-05, 7.0985e-05],
        [1.1292e-04, 8.9650e-05, 8.5984e-05, 9.2398e-05, 7.2869e-05],
        [5.2202e-05, 4.1445e-05, 3.9750e-05, 4.2715e-05, 3.3687e-05],
        [3.9648e-05, 3.1477e-05, 3.0190e-05, 3.2442e-05, 2.5585e-05],
        [4.3181e-05, 3.4283e-05, 3.2881e-05, 3.5333e-05, 2.7865e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(246.9585, grad_fn=<NegBackward>) non event loss:  tensor([13.5977], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0785, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.9830e-04, 6.7799e-04, 7.2227e-04, 5.3057e-04, 6.6436e-04, 3.2707e-04,
         5.3718e-04, 5.5769e-04, 4.7965e-04, 5.0120e-04, 4.9722e-04, 4.0633e-04,
         5.4386e-04, 4.6651e-04, 5.8858e-04, 2.8785e-04, 7.1768e-04, 4.5130e-04,
         5.1486e-04, 6.6509e-04, 4.6036e-04, 4.4338e-04, 5.1833e-04, 4.0967e-04],
        [1.4534e-04, 1.9776e-04, 2.1068e-04, 1.5475e-04, 1.9378e-04, 9.5389e-05,
         1.5668e-04, 1.6266e-04, 1.3990e-04, 1.4618e-04, 1.4503e-04, 1.1851e-04,
         1.5863e-04, 1.3606e-04, 1.7168e-04, 8.3951e-05, 2.0934e-04, 1.3163e-04,
         1.5017e-04, 1.9400e-04, 1.3427e-04, 1.2932e-04, 1.5118e-04, 1.1948e-04],
        [1.4215e-04, 1.9343e-04, 2.0606e-04, 1.5136e-04, 1.8954e-04, 9.3300e-05,
         1.5325e-04, 1.5910e-04, 1.3683e-04, 1.4298e-04, 1.4185e-04, 1.1591e-04,
         1.5516e-04, 1.3308e-04, 1.6791e-04, 8.2111e-05, 2.0476e-04, 1.2875e-04,
         1.4688e-04, 1.8975e-04, 1.3133e-04, 1.2648e-04, 1.4787e-04, 1.1687e-04],
        [1.1698e-04, 1.5917e-04, 1.6957e-04, 1.2455e-04, 1.5597e-04, 7.6774e-05,
         1.2610e-04, 1.3092e-04, 1.1260e-04, 1.1766e-04, 1.1672e-04, 9.5382e-05,
         1.2767e-04, 1.0951e-04, 1.3817e-04, 6.7567e-05, 1.6849e-04, 1.0594e-04,
         1.2086e-04, 1.5614e-04, 1.0807e-04, 1.0408e-04, 1.2168e-04, 9.6166e-05],
        [6.2835e-05, 8.5501e-05, 9.1086e-05, 6.6906e-05, 8.3781e-05, 4.1240e-05,
         6.7739e-05, 7.0327e-05, 6.0484e-05, 6.3201e-05, 6.2700e-05, 5.1236e-05,
         6.8582e-05, 5.8826e-05, 7.4223e-05, 3.6294e-05, 9.0508e-05, 5.6908e-05,
         6.4924e-05, 8.3874e-05, 5.8050e-05, 5.5908e-05, 6.5362e-05, 5.1657e-05],
        [5.6103e-05, 7.6341e-05, 8.1328e-05, 5.9738e-05, 7.4805e-05, 3.6821e-05,
         6.0481e-05, 6.2792e-05, 5.4003e-05, 5.6430e-05, 5.5982e-05, 4.5746e-05,
         6.1235e-05, 5.2523e-05, 6.6270e-05, 3.2406e-05, 8.0811e-05, 5.0811e-05,
         5.7968e-05, 7.4888e-05, 5.1831e-05, 4.9918e-05, 5.8359e-05, 4.6122e-05],
        [1.2583e-04, 1.7121e-04, 1.8240e-04, 1.3398e-04, 1.6777e-04, 8.2584e-05,
         1.3565e-04, 1.4083e-04, 1.2112e-04, 1.2656e-04, 1.2556e-04, 1.0260e-04,
         1.3734e-04, 1.1780e-04, 1.4863e-04, 7.2680e-05, 1.8124e-04, 1.1396e-04,
         1.3001e-04, 1.6796e-04, 1.1625e-04, 1.1196e-04, 1.3089e-04, 1.0344e-04],
        [1.9152e-04, 2.6060e-04, 2.7762e-04, 2.0393e-04, 2.5536e-04, 1.2570e-04,
         2.0646e-04, 2.1435e-04, 1.8435e-04, 1.9263e-04, 1.9111e-04, 1.5616e-04,
         2.0903e-04, 1.7930e-04, 2.2622e-04, 1.1063e-04, 2.7585e-04, 1.7345e-04,
         1.9788e-04, 2.5564e-04, 1.7694e-04, 1.7041e-04, 1.9922e-04, 1.5745e-04],
        [6.0860e-05, 8.2814e-05, 8.8224e-05, 6.4803e-05, 8.1148e-05, 3.9944e-05,
         6.5610e-05, 6.8116e-05, 5.8583e-05, 6.1215e-05, 6.0729e-05, 4.9625e-05,
         6.6427e-05, 5.6977e-05, 7.1890e-05, 3.5154e-05, 8.7663e-05, 5.5120e-05,
         6.2884e-05, 8.1238e-05, 5.6226e-05, 5.4151e-05, 6.3308e-05, 5.0033e-05],
        [3.7391e-05, 5.0879e-05, 5.4203e-05, 3.9814e-05, 4.9856e-05, 2.4540e-05,
         4.0309e-05, 4.1849e-05, 3.5992e-05, 3.7609e-05, 3.7311e-05, 3.0489e-05,
         4.0811e-05, 3.5005e-05, 4.4168e-05, 2.1598e-05, 5.3858e-05, 3.3864e-05,
         3.8634e-05, 4.9911e-05, 3.4544e-05, 3.3269e-05, 3.8895e-05, 3.0739e-05],
        [3.8580e-05, 5.2497e-05, 5.5926e-05, 4.1080e-05, 5.1441e-05, 2.5321e-05,
         4.1591e-05, 4.3180e-05, 3.7136e-05, 3.8805e-05, 3.8497e-05, 3.1458e-05,
         4.2109e-05, 3.6118e-05, 4.5572e-05, 2.2284e-05, 5.5571e-05, 3.4941e-05,
         3.9863e-05, 5.1498e-05, 3.5642e-05, 3.4327e-05, 4.0131e-05, 3.1717e-05],
        [4.1910e-04, 5.7024e-04, 6.0749e-04, 4.4625e-04, 5.5878e-04, 2.7508e-04,
         4.5180e-04, 4.6906e-04, 4.0342e-04, 4.2154e-04, 4.1820e-04, 3.4175e-04,
         4.5743e-04, 3.9237e-04, 4.9504e-04, 2.4210e-04, 6.0363e-04, 3.7958e-04,
         4.3303e-04, 5.5939e-04, 3.8719e-04, 3.7291e-04, 4.3595e-04, 3.4456e-04],
        [5.0430e-05, 6.8621e-05, 7.3103e-05, 5.3697e-05, 6.7241e-05, 3.3098e-05,
         5.4365e-05, 5.6442e-05, 4.8542e-05, 5.0723e-05, 5.0321e-05, 4.1120e-05,
         5.5042e-05, 4.7212e-05, 5.9569e-05, 2.9129e-05, 7.2639e-05, 4.5673e-05,
         5.2106e-05, 6.7315e-05, 4.6590e-05, 4.4870e-05, 5.2458e-05, 4.1458e-05],
        [5.2680e-05, 7.1683e-05, 7.6366e-05, 5.6093e-05, 7.0241e-05, 3.4575e-05,
         5.6791e-05, 5.8961e-05, 5.0709e-05, 5.2987e-05, 5.2567e-05, 4.2955e-05,
         5.7499e-05, 4.9319e-05, 6.2227e-05, 3.0429e-05, 7.5881e-05, 4.7711e-05,
         5.4432e-05, 7.0319e-05, 4.8669e-05, 4.6873e-05, 5.4799e-05, 4.3308e-05],
        [4.3488e-05, 5.9175e-05, 6.3041e-05, 4.6306e-05, 5.7985e-05, 2.8542e-05,
         4.6882e-05, 4.8673e-05, 4.1861e-05, 4.3741e-05, 4.3395e-05, 3.5460e-05,
         4.7466e-05, 4.0713e-05, 5.1369e-05, 2.5119e-05, 6.2641e-05, 3.9386e-05,
         4.4934e-05, 5.8049e-05, 4.0177e-05, 3.8694e-05, 4.5237e-05, 3.5752e-05],
        [3.5621e-05, 4.8470e-05, 5.1636e-05, 3.7929e-05, 4.7495e-05, 2.3378e-05,
         3.8401e-05, 3.9868e-05, 3.4288e-05, 3.5828e-05, 3.5544e-05, 2.9045e-05,
         3.8879e-05, 3.3348e-05, 4.2076e-05, 2.0575e-05, 5.1308e-05, 3.2261e-05,
         3.6805e-05, 4.7548e-05, 3.2908e-05, 3.1694e-05, 3.7053e-05, 2.9284e-05],
        [3.5079e-05, 4.7733e-05, 5.0851e-05, 3.7352e-05, 4.6772e-05, 2.3023e-05,
         3.7816e-05, 3.9261e-05, 3.3766e-05, 3.5283e-05, 3.5003e-05, 2.8603e-05,
         3.8287e-05, 3.2840e-05, 4.1436e-05, 2.0262e-05, 5.0528e-05, 3.1770e-05,
         3.6245e-05, 4.6824e-05, 3.2408e-05, 3.1212e-05, 3.6489e-05, 2.8838e-05],
        [2.8920e-05, 3.9352e-05, 4.1922e-05, 3.0793e-05, 3.8560e-05, 1.8980e-05,
         3.1176e-05, 3.2367e-05, 2.7837e-05, 2.9088e-05, 2.8857e-05, 2.3581e-05,
         3.1565e-05, 2.7074e-05, 3.4161e-05, 1.6704e-05, 4.1656e-05, 2.6192e-05,
         2.9881e-05, 3.8603e-05, 2.6717e-05, 2.5731e-05, 3.0083e-05, 2.3775e-05],
        [3.1887e-05, 4.3390e-05, 4.6224e-05, 3.3953e-05, 4.2517e-05, 2.0928e-05,
         3.4376e-05, 3.5689e-05, 3.0694e-05, 3.2073e-05, 3.1818e-05, 2.6001e-05,
         3.4804e-05, 2.9852e-05, 3.7666e-05, 1.8418e-05, 4.5930e-05, 2.8879e-05,
         3.2947e-05, 4.2564e-05, 2.9459e-05, 2.8372e-05, 3.3170e-05, 2.6214e-05],
        [3.1235e-05, 4.2502e-05, 4.5279e-05, 3.3259e-05, 4.1647e-05, 2.0500e-05,
         3.3672e-05, 3.4959e-05, 3.0066e-05, 3.1417e-05, 3.1168e-05, 2.5469e-05,
         3.4092e-05, 2.9242e-05, 3.6896e-05, 1.8042e-05, 4.4991e-05, 2.8289e-05,
         3.2273e-05, 4.1693e-05, 2.8856e-05, 2.7791e-05, 3.2491e-05, 2.5678e-05],
        [2.7676e-04, 3.7658e-04, 4.0118e-04, 2.9469e-04, 3.6901e-04, 1.8165e-04,
         2.9836e-04, 3.0976e-04, 2.6641e-04, 2.7837e-04, 2.7617e-04, 2.2568e-04,
         3.0207e-04, 2.5911e-04, 3.2691e-04, 1.5987e-04, 3.9863e-04, 2.5066e-04,
         2.8596e-04, 3.6942e-04, 2.5569e-04, 2.4626e-04, 2.8789e-04, 2.2753e-04],
        [4.0415e-05, 5.4994e-05, 5.8587e-05, 4.3034e-05, 5.3888e-05, 2.6525e-05,
         4.3569e-05, 4.5234e-05, 3.8903e-05, 4.0651e-05, 4.0328e-05, 3.2954e-05,
         4.4112e-05, 3.7837e-05, 4.7740e-05, 2.3344e-05, 5.8214e-05, 3.6603e-05,
         4.1759e-05, 5.3947e-05, 3.7338e-05, 3.5960e-05, 4.2041e-05, 3.3226e-05],
        [3.8844e-05, 5.2856e-05, 5.6309e-05, 4.1361e-05, 5.1793e-05, 2.5494e-05,
         4.1875e-05, 4.3475e-05, 3.7390e-05, 3.9070e-05, 3.8760e-05, 3.1673e-05,
         4.2397e-05, 3.6365e-05, 4.5883e-05, 2.2437e-05, 5.5951e-05, 3.5180e-05,
         4.0135e-05, 5.1850e-05, 3.5886e-05, 3.4562e-05, 4.0406e-05, 3.1934e-05],
        [4.0243e-05, 5.4760e-05, 5.8337e-05, 4.2851e-05, 5.3658e-05, 2.6412e-05,
         4.3384e-05, 4.5041e-05, 3.8737e-05, 4.0477e-05, 4.0156e-05, 3.2814e-05,
         4.3924e-05, 3.7675e-05, 4.7536e-05, 2.3245e-05, 5.7966e-05, 3.6447e-05,
         4.1581e-05, 5.3718e-05, 3.7179e-05, 3.5807e-05, 4.1862e-05, 3.3084e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(5491.6719, grad_fn=<NegBackward>) non event loss:  tensor([82.6483], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.2432, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0007, 0.0008, 0.0015, 0.0006, 0.0006, 0.0005],
        [0.0005, 0.0006, 0.0010, 0.0004, 0.0004, 0.0004],
        [0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0001],
        [0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0001],
        [0.0002, 0.0002, 0.0004, 0.0001, 0.0001, 0.0001],
        [0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0001]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(294.7774, grad_fn=<NegBackward>) non event loss:  tensor([87.0954], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1026, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.8888e-04, 2.1915e-04, 2.3930e-04, 3.2863e-04, 2.3128e-04],
        [7.6438e-05, 8.8690e-05, 9.6843e-05, 1.3300e-04, 9.3596e-05],
        [6.1486e-05, 7.1341e-05, 7.7900e-05, 1.0698e-04, 7.5288e-05],
        [5.5648e-05, 6.4568e-05, 7.0504e-05, 9.6827e-05, 6.8140e-05],
        [7.4041e-05, 8.5909e-05, 9.3806e-05, 1.2883e-04, 9.0661e-05]],
 23%|████████████████████████████████▎                                                                                                             | 113/496 [00:20<01:10,  5.41it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 107, in train
    loss, pos_timing_loss, neg_timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 195, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 370, in forward
    z_vt_i = z_vt_i.unsqueeze(0).unsqueeze(1) # (1, 1,  N_i_1, embedding_z) # add batch size back
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 197, in format_stack
    return format_list(extract_stack(f, limit=limit))
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 211, in extract_stack
    stack = StackSummary.extract(walk_stack(f), limit=limit)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 366, in extract
    f.line
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 288, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/linecache.py", line 15, in getline
    def getline(filename, lineno, module_globals=None):
KeyboardInterrupt
##### event loss: tensor(229.5791, grad_fn=<NegBackward>) non event loss:  tensor([45.1920], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0978, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[6.8387e-04, 6.3727e-04, 3.2441e-04, 3.4202e-04, 4.4176e-04, 3.3665e-04,
         5.0593e-04, 5.7366e-04, 6.1860e-04, 3.8070e-04],
        [5.5490e-04, 5.1708e-04, 2.6322e-04, 2.7751e-04, 3.5844e-04, 2.7315e-04,
         4.1051e-04, 4.6547e-04, 5.0193e-04, 3.0889e-04],
        [4.3398e-04, 4.0441e-04, 2.0586e-04, 2.1703e-04, 2.8033e-04, 2.1363e-04,
         3.2105e-04, 3.6404e-04, 3.9256e-04, 2.4158e-04],
        [4.5843e-04, 4.2719e-04, 2.1745e-04, 2.2926e-04, 2.9612e-04, 2.2566e-04,
         3.3914e-04, 3.8455e-04, 4.1467e-04, 2.5519e-04],
        [4.6197e-04, 4.3048e-04, 2.1913e-04, 2.3103e-04, 2.9841e-04, 2.2740e-04,
         3.4176e-04, 3.8751e-04, 4.1787e-04, 2.5716e-04],
        [1.4119e-04, 1.3156e-04, 6.6965e-05, 7.0601e-05, 9.1194e-05, 6.9493e-05,
         1.0444e-04, 1.1843e-04, 1.2771e-04, 7.8587e-05],
        [1.0535e-04, 9.8164e-05, 4.9965e-05, 5.2678e-05, 6.8043e-05, 5.1851e-05,
         7.7929e-05, 8.8364e-05, 9.5288e-05, 5.8637e-05],
        [1.3100e-04, 1.2207e-04, 6.2131e-05, 6.5505e-05, 8.4611e-05, 6.4476e-05,
         9.6903e-05, 1.0988e-04, 1.1849e-04, 7.2914e-05],
        [9.0536e-05, 8.4365e-05, 4.2941e-05, 4.5273e-05, 5.8478e-05, 4.4562e-05,
         6.6974e-05, 7.5942e-05, 8.1893e-05, 5.0393e-05],
        [8.5482e-05, 7.9654e-05, 4.0543e-05, 4.2745e-05, 5.5213e-05, 4.2074e-05,
         6.3234e-05, 7.1702e-05, 7.7320e-05, 4.7580e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(873.5438, grad_fn=<NegBackward>) non event loss:  tensor([165.5804], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.2045, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0010, 0.0013, 0.0007],
        [0.0007, 0.0010, 0.0005],
        [0.0008, 0.0011, 0.0006]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(64.0290, grad_fn=<NegBackward>) non event loss:  tensor([151.5708], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0529, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0019, 0.0010, 0.0012, 0.0011, 0.0016, 0.0009, 0.0011],
        [0.0016, 0.0009, 0.0010, 0.0009, 0.0013, 0.0008, 0.0009],
        [0.0006, 0.0003, 0.0003, 0.0003, 0.0005, 0.0003, 0.0003],
        [0.0005, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003],
        [0.0005, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003],
        [0.0004, 0.0002, 0.0003, 0.0003, 0.0004, 0.0002, 0.0002],
        [0.0002, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(381.7670, grad_fn=<NegBackward>) non event loss:  tensor([28.4345], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0411, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.4177e-04, 1.4991e-04, 1.4802e-04, 1.5915e-04, 1.4662e-04, 1.5042e-04,
         1.5195e-04, 1.9839e-04],
        [9.4800e-05, 1.0024e-04, 9.8982e-05, 1.0643e-04, 9.8045e-05, 1.0059e-04,
         1.0161e-04, 1.3267e-04],
        [1.1824e-04, 1.2503e-04, 1.2346e-04, 1.3274e-04, 1.2229e-04, 1.2546e-04,
         1.2673e-04, 1.6547e-04],
        [8.3754e-05, 8.8562e-05, 8.7448e-05, 9.4024e-05, 8.6620e-05, 8.8866e-05,
         8.9769e-05, 1.1721e-04],
        [8.2480e-05, 8.7215e-05, 8.6118e-05, 9.2595e-05, 8.5303e-05, 8.7515e-05,
         8.8404e-05, 1.1542e-04],
        [9.8119e-05, 1.0375e-04, 1.0245e-04, 1.1015e-04, 1.0148e-04, 1.0411e-04,
         1.0517e-04, 1.3731e-04],
        [2.3638e-04, 2.4995e-04, 2.4681e-04, 2.6537e-04, 2.4447e-04, 2.5081e-04,
         2.5336e-04, 3.3079e-04],
        [1.7760e-04, 1.8779e-04, 1.8543e-04, 1.9938e-04, 1.8368e-04, 1.8844e-04,
         1.9035e-04, 2.4853e-04]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(571.6866, grad_fn=<NegBackward>) non event loss:  tensor([121.8421], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1523, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.5560e-04, 2.2707e-04, 1.4857e-04, 1.5731e-04, 1.6051e-04, 1.9771e-04],
        [1.1226e-04, 1.6383e-04, 1.0719e-04, 1.1350e-04, 1.1580e-04, 1.4264e-04],
        [7.0619e-04, 1.0305e-03, 6.7429e-04, 7.1396e-04, 7.2846e-04, 8.9725e-04],
        [8.6373e-05, 1.2605e-04, 8.2471e-05, 8.7324e-05, 8.9098e-05, 1.0975e-04],
        [6.9563e-05, 1.0152e-04, 6.6420e-05, 7.0329e-05, 7.1758e-05, 8.8391e-05],
        [8.4866e-05, 1.2385e-04, 8.1031e-05, 8.5800e-05, 8.7543e-05, 1.0783e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(316.8590, grad_fn=<NegBackward>) non event loss:  tensor([59.2183], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1267, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0016, 0.0013, 0.0009, 0.0010],
        [0.0013, 0.0011, 0.0008, 0.0008],
        [0.0011, 0.0009, 0.0007, 0.0007],
        [0.0009, 0.0007, 0.0005, 0.0005]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(112.5950, grad_fn=<NegBackward>) non event loss:  tensor([7.8155], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0311, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.5927e-04, 4.2743e-04, 4.3757e-04, 4.5787e-04, 4.3649e-04, 7.1151e-04],
        [2.0816e-04, 3.4318e-04, 3.5132e-04, 3.6761e-04, 3.5045e-04, 5.7127e-04],
        [3.2550e-04, 5.3663e-04, 5.4935e-04, 5.7483e-04, 5.4799e-04, 8.9324e-04],
        [6.4363e-05, 1.0612e-04, 1.0863e-04, 1.1367e-04, 1.0837e-04, 1.7666e-04],
        [1.1393e-04, 1.8783e-04, 1.9229e-04, 2.0121e-04, 1.9181e-04, 3.1269e-04],
        [1.3255e-04, 2.1853e-04, 2.2371e-04, 2.3409e-04, 2.2316e-04, 3.6379e-04]],
       grad_fn=<SoftplusBackward>)