
CUDA availability: True
  0%|â–‹                                                                                                                                                                              | 2/496 [00:00<01:29,  5.51it/s]
Traceback (most recent call last):
  File "main.py", line 124, in <module>
    main()
  File "main.py", line 120, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 81, in train
    loss, timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 197, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 363, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
#### mat_b tensor([[[ 0.1745,  0.1597,  0.4391,  0.3151,  0.1645,  0.1894,  0.2744,
           0.3266,  0.5552,  0.0678, -0.1886,  0.5210, -0.0813,  0.1723,
           0.0088,  0.5341, -0.1357, -0.0372, -0.4807,  0.0500, -0.1447,
          -0.3101, -0.1305, -0.1414,  0.1941, -0.0131, -0.3559,  0.0167,
           0.0657, -0.2192,  0.2788, -0.0949],
         [ 0.1730,  0.1546,  0.4389,  0.3187,  0.1632,  0.1902,  0.2652,
           0.3394,  0.5516,  0.0725, -0.1884,  0.5155, -0.0818,  0.1771,
           0.0112,  0.5338, -0.1284, -0.0357, -0.4775,  0.0500, -0.1516,
          -0.3113, -0.1388, -0.1394,  0.2007, -0.0165, -0.3602,  0.0186,
           0.0673, -0.2228,  0.2793, -0.0986],
         [ 0.1705,  0.1462,  0.4383,  0.3238,  0.1606,  0.1925,  0.2513,
           0.3592,  0.5468,  0.0809, -0.1893,  0.5064, -0.0829,  0.1861,
           0.0151,  0.5339, -0.1159, -0.0331, -0.4725,  0.0491, -0.1614,
          -0.3114, -0.1509, -0.1366,  0.2098, -0.0224, -0.3655,  0.0219,
           0.0694, -0.2271,  0.2809, -0.1049],
         [ 0.1682,  0.1386,  0.4376,  0.3287,  0.1582,  0.1947,  0.2382,
           0.3779,  0.5422,  0.0887, -0.1902,  0.4977, -0.0838,  0.1942,
           0.0185,  0.5339, -0.1041, -0.0306, -0.4678,  0.0485, -0.1708,
          -0.3114, -0.1621, -0.1339,  0.2183, -0.0280, -0.3705,  0.0249,
           0.0712, -0.2312,  0.2822, -0.1107],
         [ 0.1701,  0.1310,  0.4350,  0.3381,  0.1575,  0.1980,  0.2246,
           0.3860,  0.5348,  0.1032, -0.1904,  0.4922, -0.0914,  0.1960,
           0.0158,  0.5358, -0.1011, -0.0265, -0.4707,  0.0531, -0.1854,
          -0.3174, -0.1713, -0.1362,  0.2265, -0.0330, -0.3782,  0.0261,
           0.0755, -0.2371,  0.2804, -0.1157],
         [ 0.1676,  0.1218,  0.4332,  0.3417,  0.1537,  0.2032,  0.2128,
           0.4046,  0.5329,  0.1139, -0.1946,  0.4816, -0.0929,  0.2090,
           0.0201,  0.5373, -0.0853, -0.0239, -0.4658,  0.0499, -0.1923,
          -0.3126, -0.1802, -0.1341,  0.2320, -0.0403, -0.3787,  0.0305,
           0.0764, -0.2372,  0.2843, -0.1235],
         [ 0.1639,  0.1104,  0.4312,  0.3477,  0.1492,  0.2089,  0.1955,
           0.4310,  0.5286,  0.1277, -0.1989,  0.4676, -0.0948,  0.2239,
           0.0250,  0.5382, -0.0650, -0.0199, -0.4596,  0.0474, -0.2039,
          -0.3081, -0.1940, -0.1312,  0.2416, -0.0500, -0.3816,  0.0355,
           0.0777, -0.2396,  0.2882, -0.1330],
         [ 0.1600,  0.1012,  0.4296,  0.3521,  0.1452,  0.2137,  0.1814,
           0.4545,  0.5255,  0.1388, -0.2029,  0.4556, -0.0953,  0.2371,
           0.0296,  0.5387, -0.0466, -0.0165, -0.4536,  0.0445, -0.2130,
          -0.3031, -0.2055, -0.1283,  0.2497, -0.0584, -0.3831,  0.0398,
           0.0782, -0.2410,  0.2920, -0.1410],
         [ 0.1641,  0.1239,  0.4352,  0.3360,  0.1526,  0.2018,  0.2169,
           0.4096,  0.5369,  0.1058, -0.1952,  0.4810, -0.0863,  0.2130,
           0.0249,  0.5355, -0.0800, -0.0260, -0.4601,  0.0449, -0.1846,
          -0.3066, -0.1788, -0.1303,  0.2299, -0.0393, -0.3744,  0.0312,
           0.0733, -0.2340,  0.2872, -0.1227],
         [ 0.1641,  0.1239,  0.4352,  0.3360,  0.1526,  0.2018,  0.2169,
           0.4096,  0.5369,  0.1058, -0.1952,  0.4810, -0.0863,  0.2130,
           0.0249,  0.5355, -0.0800, -0.0260, -0.4601,  0.0449, -0.1846,
          -0.3066, -0.1788, -0.1303,  0.2299, -0.0393, -0.3744,  0.0312,
           0.0733, -0.2340,  0.2872, -0.1227],
         [ 0.1641,  0.1239,  0.4352,  0.3360,  0.1526,  0.2018,  0.2169,
           0.4096,  0.5369,  0.1058, -0.1952,  0.4810, -0.0863,  0.2130,
           0.0249,  0.5355, -0.0800, -0.0260, -0.4601,  0.0449, -0.1846,
          -0.3066, -0.1788, -0.1303,  0.2299, -0.0393, -0.3744,  0.0312,
           0.0733, -0.2340,  0.2872, -0.1227],
         [ 0.1523,  0.0877,  0.4280,  0.3636,  0.1399,  0.2188,  0.1498,
           0.5036,  0.5137,  0.1568, -0.2064,  0.4337, -0.0961,  0.2541,
           0.0350,  0.5354, -0.0134, -0.0074, -0.4423,  0.0455, -0.2389,
          -0.3001, -0.2342, -0.1226,  0.2714, -0.0741, -0.3934,  0.0455,
           0.0799, -0.2512,  0.2952, -0.1529],
         [ 0.1641,  0.1239,  0.4352,  0.3360,  0.1526,  0.2018,  0.2169,
           0.4096,  0.5369,  0.1058, -0.1952,  0.4810, -0.0863,  0.2130,
           0.0249,  0.5355, -0.0800, -0.0260, -0.4601,  0.0449, -0.1846,
          -0.3066, -0.1788, -0.1303,  0.2299, -0.0393, -0.3744,  0.0312,
           0.0733, -0.2340,  0.2872, -0.1227],
         [ 0.1466,  0.0934,  0.4323,  0.3527,  0.1394,  0.2141,  0.1593,
           0.5088,  0.5200,  0.1382, -0.2053,  0.4366, -0.0823,  0.2565,
           0.0423,  0.5318, -0.0104, -0.0122, -0.4329,  0.0366, -0.2237,
          -0.2923, -0.2294, -0.1159,  0.2679, -0.0675, -0.3884,  0.0448,
           0.0745, -0.2454,  0.2991, -0.1482],
         [ 0.1472,  0.0971,  0.4335,  0.3491,  0.1405,  0.2121,  0.1664,
           0.5004,  0.5228,  0.1316, -0.2040,  0.4412, -0.0802,  0.2531,
           0.0424,  0.5315, -0.0164, -0.0146, -0.4335,  0.0357, -0.2169,
          -0.2924, -0.2240, -0.1158,  0.2636, -0.0637, -0.3862,  0.0436,
           0.0736, -0.2434,  0.2986, -0.1451],
         [ 0.1641,  0.1239,  0.4352,  0.3360,  0.1526,  0.2018,  0.2169,
           0.4096,  0.5369,  0.1058, -0.1952,  0.4810, -0.0863,  0.2130,
           0.0249,  0.5355, -0.0800, -0.0260, -0.4601,  0.0449, -0.1846,
          -0.3066, -0.1788, -0.1303,  0.2299, -0.0393, -0.3744,  0.0312,
           0.0733, -0.2340,  0.2872, -0.1227],
         [ 0.1641,  0.1239,  0.4352,  0.3360,  0.1526,  0.2018,  0.2169,
           0.4096,  0.5369,  0.1058, -0.1952,  0.4810, -0.0863,  0.2130,
           0.0249,  0.5355, -0.0800, -0.0260, -0.4601,  0.0449, -0.1846,
          -0.3066, -0.1788, -0.1303,  0.2299, -0.0393, -0.3744,  0.0312,
           0.0733, -0.2340,  0.2872, -0.1227],
         [ 0.1641,  0.1239,  0.4352,  0.3360,  0.1526,  0.2018,  0.2169,
           0.4096,  0.5369,  0.1058, -0.1952,  0.4810, -0.0863,  0.2130,
           0.0249,  0.5355, -0.0800, -0.0260, -0.4601,  0.0449, -0.1846,
          -0.3066, -0.1788, -0.1303,  0.2299, -0.0393, -0.3744,  0.0312,
           0.0733, -0.2340,  0.2872, -0.1227],
         [ 0.1503,  0.1114,  0.4379,  0.3400,  0.1459,  0.2028,  0.1872,
           0.4718,  0.5283,  0.1093, -0.1971,  0.4578, -0.0752,  0.2355,
           0.0396,  0.5286, -0.0398, -0.0210, -0.4373,  0.0379, -0.2007,
          -0.2984, -0.2094, -0.1163,  0.2525, -0.0514, -0.3840,  0.0382,
           0.0720, -0.2424,  0.2937, -0.1332],
         [ 0.1646,  0.1220,  0.4335,  0.3339,  0.1512,  0.2056,  0.2208,
           0.4057,  0.5413,  0.1091, -0.1992,  0.4801, -0.0872,  0.2186,
           0.0259,  0.5385, -0.0777, -0.0268, -0.4611,  0.0412, -0.1793,
          -0.3006, -0.1736, -0.1317,  0.2245, -0.0402, -0.3679,  0.0325,
           0.0721, -0.2278,  0.2904, -0.1248],
         [ 0.1644,  0.1228,  0.4342,  0.3347,  0.1518,  0.2040,  0.2192,
           0.4073,  0.5395,  0.1078, -0.1976,  0.4804, -0.0868,  0.2163,
           0.0255,  0.5373, -0.0786, -0.0265, -0.4607,  0.0427, -0.1815,
          -0.3031, -0.1757, -0.1311,  0.2268, -0.0398, -0.3706,  0.0320,
           0.0726, -0.2303,  0.2891, -0.1240],
         [ 0.1645,  0.1224,  0.4338,  0.3343,  0.1515,  0.2048,  0.2199,
           0.4066,  0.5403,  0.1084, -0.1983,  0.4803, -0.0870,  0.2173,
           0.0256,  0.5378, -0.0782, -0.0266, -0.4608,  0.0420, -0.1805,
          -0.3019, -0.1747, -0.1314,  0.2257, -0.0400, -0.3694,  0.0323,
           0.0724, -0.2292,  0.2897, -0.1244],
         [ 0.1645,  0.1222,  0.4337,  0.3341,  0.1513,  0.2052,  0.2204,
           0.4061,  0.5408,  0.1087, -0.1988,  0.4802, -0.0871,  0.2179,
           0.0257,  0.5381, -0.0780, -0.0267, -0.4610,  0.0417, -0.1799,
          -0.3013, -0.1742, -0.1315,  0.2252, -0.0401, -0.3687,  0.0324,
           0.0723, -0.2285,  0.2900, -0.1246],
         [ 0.1741,  0.1623,  0.4406,  0.3165,  0.1662,  0.1855,  0.2729,
           0.3275,  0.5520,  0.0639, -0.1847,  0.5232, -0.0802,  0.1661,
           0.0077,  0.5317, -0.1398, -0.0371, -0.4807,  0.0531, -0.1481,
          -0.3155, -0.1335, -0.1406,  0.1981, -0.0114, -0.3610,  0.0150,
           0.0665, -0.2241,  0.2756, -0.0922]]], grad_fn=<AddBackward0>) torch.Size([1, 24, 32])
#### mat_b tensor([[[ 2.0932e-01,  1.2773e-01,  4.0601e-01,  2.8110e-01,  1.3225e-01,
           1.5509e-01,  2.4234e-01,  3.5623e-01,  5.2177e-01,  3.3512e-02,
          -1.5399e-01,  5.5498e-01, -1.1448e-01,  2.0488e-01, -2.4946e-02,
           5.6989e-01, -1.7227e-01, -3.6852e-03, -4.4854e-01,  1.5986e-02,
          -1.7694e-01, -3.4334e-01, -1.5930e-01, -1.0887e-01,  1.5963e-01,
          -4.4263e-02, -3.8869e-01, -1.7731e-02,  3.2207e-02, -1.8427e-01,
           3.1377e-01, -1.2952e-01],
         [ 2.0785e-01,  1.2275e-01,  4.0583e-01,  2.8456e-01,  1.3097e-01,
           1.5579e-01,  2.3336e-01,  3.6866e-01,  5.1823e-01,  3.8095e-02,
          -1.5375e-01,  5.4966e-01, -1.1496e-01,  2.0964e-01, -2.2550e-02,
           5.6975e-01, -1.6525e-01, -2.1456e-03, -4.4540e-01,  1.5866e-02,
          -1.8360e-01, -3.4446e-01, -1.6730e-01, -1.0691e-01,  1.6604e-01,
          -4.7570e-02, -3.9289e-01, -1.5906e-02,  3.3714e-02, -1.8776e-01,
           3.1434e-01, -1.3324e-01],
         [ 2.0556e-01,  1.1451e-01,  4.0517e-01,  2.8962e-01,  1.2851e-01,
           1.5799e-01,  2.1974e-01,  3.8791e-01,  5.1351e-01,  4.6307e-02,
          -1.5453e-01,  5.4069e-01, -1.1596e-01,  2.1848e-01, -1.8662e-02,
           5.7002e-01, -1.5308e-01,  3.2092e-04, -4.4053e-01,  1.4987e-02,
          -1.9320e-01, -3.4465e-01, -1.7892e-01, -1.0414e-01,  1.7497e-01,
          -5.3252e-02, -3.9806e-01, -1.2645e-02,  3.5781e-02, -1.9191e-01,
           3.1597e-01, -1.3963e-01],
         [ 2.0336e-01,  1.0706e-01,  4.0457e-01,  2.9436e-01,  1.2622e-01,
           1.6007e-01,  2.0698e-01,  4.0603e-01,  5.0901e-01,  5.3946e-02,
          -1.5533e-01,  5.3228e-01, -1.1687e-01,  2.2647e-01, -1.5239e-02,
           5.7010e-01, -1.4161e-01,  2.7653e-03, -4.3599e-01,  1.4318e-02,
          -2.0235e-01, -3.4474e-01, -1.8978e-01, -1.0159e-01,  1.8332e-01,
          -5.8634e-02, -4.0290e-01, -9.6804e-03,  3.7604e-02, -1.9583e-01,
           3.1740e-01, -1.4545e-01],
         [ 2.0526e-01,  9.9522e-02,  4.0200e-01,  3.0352e-01,  1.2552e-01,
           1.6329e-01,  1.9371e-01,  4.1401e-01,  5.0178e-01,  6.8098e-02,
          -1.5554e-01,  5.2687e-01, -1.2425e-01,  2.2838e-01, -1.7901e-02,
           5.7205e-01, -1.3872e-01,  6.7622e-03, -4.3874e-01,  1.8688e-02,
          -2.1665e-01, -3.5063e-01, -1.9871e-01, -1.0379e-01,  1.9138e-01,
          -6.3448e-02, -4.1053e-01, -8.4548e-03,  4.1859e-02, -2.0151e-01,
           3.1565e-01, -1.5032e-01],
         [ 2.0280e-01,  9.0509e-02,  4.0033e-01,  3.0710e-01,  1.2191e-01,
           1.6839e-01,  1.8223e-01,  4.3207e-01,  4.9989e-01,  7.8665e-02,
          -1.5958e-01,  5.1645e-01, -1.2560e-01,  2.4106e-01, -1.3508e-02,
           5.7360e-01, -1.2331e-01,  9.1641e-03, -4.3408e-01,  1.5636e-02,
          -2.2321e-01, -3.4590e-01, -2.0726e-01, -1.0182e-01,  1.9686e-01,
          -7.0499e-02, -4.1097e-01, -4.1259e-03,  4.2776e-02, -2.0163e-01,
           3.1946e-01, -1.5803e-01],
         [ 1.9922e-01,  7.9317e-02,  3.9844e-01,  3.1299e-01,  1.1763e-01,
           1.7398e-01,  1.6532e-01,  4.5774e-01,  4.9564e-01,  9.2240e-02,
          -1.6375e-01,  5.0276e-01, -1.2733e-01,  2.5569e-01, -8.4592e-03,
           5.7464e-01, -1.0349e-01,  1.3002e-02, -4.2795e-01,  1.3088e-02,
          -2.3449e-01, -3.4163e-01, -2.2059e-01, -9.9012e-02,  2.0633e-01,
          -7.9878e-02, -4.1378e-01,  9.7518e-04,  4.4245e-02, -2.0398e-01,
           3.2330e-01, -1.6754e-01],
         [ 1.9545e-01,  7.0319e-02,  3.9693e-01,  3.1735e-01,  1.1384e-01,
           1.7869e-01,  1.5146e-01,  4.8064e-01,  4.9254e-01,  1.0307e-01,
          -1.6765e-01,  4.9089e-01, -1.2781e-01,  2.6861e-01, -3.7791e-03,
           5.7521e-01, -8.5462e-02,  1.6257e-02, -4.2216e-01,  1.0254e-02,
          -2.4338e-01, -3.3675e-01, -2.3172e-01, -9.6283e-02,  2.1432e-01,
          -8.7938e-02, -4.1529e-01,  5.2844e-03,  4.4809e-02, -2.0535e-01,
           3.2703e-01, -1.7543e-01],
         [ 1.9941e-01,  9.2591e-02,  4.0222e-01,  3.0157e-01,  1.2092e-01,
           1.6699e-01,  1.8626e-01,  4.3689e-01,  5.0384e-01,  7.0673e-02,
          -1.6020e-01,  5.1581e-01, -1.1913e-01,  2.4489e-01, -8.7139e-03,
           5.7191e-01, -1.1810e-01,  7.0787e-03, -4.2845e-01,  1.0765e-02,
          -2.1567e-01, -3.4002e-01, -2.0588e-01, -9.8107e-02,  1.9477e-01,
          -6.9494e-02, -4.0675e-01, -3.3375e-03,  3.9840e-02, -1.9851e-01,
           3.2228e-01, -1.5734e-01],
         [ 1.9941e-01,  9.2591e-02,  4.0222e-01,  3.0157e-01,  1.2092e-01,
           1.6699e-01,  1.8626e-01,  4.3689e-01,  5.0384e-01,  7.0673e-02,
          -1.6020e-01,  5.1581e-01, -1.1913e-01,  2.4489e-01, -8.7139e-03,
           5.7191e-01, -1.1810e-01,  7.0787e-03, -4.2845e-01,  1.0765e-02,
          -2.1567e-01, -3.4002e-01, -2.0588e-01, -9.8107e-02,  1.9477e-01,
          -6.9494e-02, -4.0675e-01, -3.3375e-03,  3.9840e-02, -1.9851e-01,
           3.2228e-01, -1.5734e-01],
         [ 1.9941e-01,  9.2591e-02,  4.0222e-01,  3.0157e-01,  1.2092e-01,
           1.6699e-01,  1.8626e-01,  4.3689e-01,  5.0384e-01,  7.0673e-02,
          -1.6020e-01,  5.1581e-01, -1.1913e-01,  2.4489e-01, -8.7139e-03,
           5.7191e-01, -1.1810e-01,  7.0787e-03, -4.2845e-01,  1.0765e-02,
          -2.1567e-01, -3.4002e-01, -2.0588e-01, -9.8107e-02,  1.9477e-01,
          -6.9494e-02, -4.0675e-01, -3.3375e-03,  3.9840e-02, -1.9851e-01,
           3.2228e-01, -1.5734e-01],
         [ 1.8803e-01,  5.6728e-02,  3.9529e-01,  3.2872e-01,  1.0866e-01,
           1.8359e-01,  1.2015e-01,  5.2887e-01,  4.8075e-01,  1.2084e-01,
          -1.7094e-01,  4.6938e-01, -1.2841e-01,  2.8558e-01,  1.9850e-03,
           5.7234e-01, -5.3046e-02,  2.5045e-02, -4.1089e-01,  1.0885e-02,
          -2.6868e-01, -3.3414e-01, -2.5968e-01, -9.0612e-02,  2.3578e-01,
          -1.0326e-01, -4.2565e-01,  1.1096e-02,  4.6692e-02, -2.1535e-01,
           3.3031e-01, -1.8765e-01],
         [ 1.9941e-01,  9.2591e-02,  4.0222e-01,  3.0157e-01,  1.2092e-01,
           1.6699e-01,  1.8626e-01,  4.3689e-01,  5.0384e-01,  7.0673e-02,
          -1.6020e-01,  5.1581e-01, -1.1913e-01,  2.4489e-01, -8.7139e-03,
           5.7191e-01, -1.1810e-01,  7.0787e-03, -4.2845e-01,  1.0765e-02,
          -2.1567e-01, -3.4002e-01, -2.0588e-01, -9.8107e-02,  1.9477e-01,
          -6.9494e-02, -4.0675e-01, -3.3375e-03,  3.9840e-02, -1.9851e-01,
           3.2228e-01, -1.5734e-01],
         [ 1.8251e-01,  6.2402e-02,  3.9955e-01,  3.1802e-01,  1.0816e-01,
           1.7885e-01,  1.2961e-01,  5.3388e-01,  4.8701e-01,  1.0251e-01,
          -1.6976e-01,  4.7219e-01, -1.1500e-01,  2.8793e-01,  9.1761e-03,
           5.6881e-01, -5.0037e-02,  2.0341e-02, -4.0170e-01,  2.2350e-03,
          -2.5370e-01, -3.2647e-01, -2.5498e-01, -8.4010e-02,  2.3228e-01,
          -9.6875e-02, -4.2064e-01,  1.0361e-02,  4.1382e-02, -2.0965e-01,
           3.3417e-01, -1.8307e-01],
         [ 1.8310e-01,  6.6051e-02,  4.0067e-01,  3.1449e-01,  1.0921e-01,
           1.7682e-01,  1.3658e-01,  5.2559e-01,  4.8981e-01,  9.6056e-02,
          -1.6854e-01,  4.7670e-01, -1.1305e-01,  2.8453e-01,  9.1703e-03,
           5.6852e-01, -5.5944e-02,  1.8011e-02, -4.0233e-01,  1.3845e-03,
          -2.4714e-01, -3.2653e-01, -2.4966e-01, -8.3943e-02,  2.2800e-01,
          -9.3175e-02, -4.1847e-01,  9.1265e-03,  4.0451e-02, -2.0775e-01,
           3.3369e-01, -1.8000e-01],
         [ 1.9941e-01,  9.2591e-02,  4.0222e-01,  3.0157e-01,  1.2092e-01,
           1.6699e-01,  1.8626e-01,  4.3689e-01,  5.0384e-01,  7.0673e-02,
          -1.6020e-01,  5.1581e-01, -1.1913e-01,  2.4489e-01, -8.7139e-03,
           5.7191e-01, -1.1810e-01,  7.0787e-03, -4.2845e-01,  1.0765e-02,
          -2.1567e-01, -3.4002e-01, -2.0588e-01, -9.8107e-02,  1.9477e-01,
          -6.9494e-02, -4.0675e-01, -3.3375e-03,  3.9840e-02, -1.9851e-01,
           3.2228e-01, -1.5734e-01]]], grad_fn=<AddBackward0>) torch.Size([1, 16, 32])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<AddBackward0>) torch.Size([1, 17, 32])