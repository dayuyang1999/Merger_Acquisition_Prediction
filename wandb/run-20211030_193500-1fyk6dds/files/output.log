
CUDA availability: True
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 197, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)*1000
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 357, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
#### arr_b tensor([[[ 1.6107,  0.2177, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 1.8378,  0.2896, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 2.2109,  0.3706, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 2.5649,  0.4498, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 2.7934,  0.5735, -0.2877, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2103,  0.5564, -0.2813, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.7867,  0.5924, -0.2826, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 4.3058,  0.6133, -0.2556, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 5.3681,  0.8729, -0.2334, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 5.2710,  0.7727,  0.1726, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 5.0511,  0.7244,  0.2207, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 4.3120,  0.7036,  0.3016, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.3324, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.3956, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.3668, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.3505, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466]]]) torch.Size([1, 23, 14])
#### mat_b tensor([[[-3.7135e-02,  2.1789e-01, -1.3701e-01, -2.0066e-01, -1.3989e-01,
          -1.2243e-01,  1.0096e-01, -1.9061e-01, -2.8012e-01, -3.2914e-01,
           1.4138e-01,  1.5658e-01,  7.5136e-03,  7.3342e-02,  9.0406e-02,
          -1.0154e-01, -2.4685e-02,  3.0905e-01, -1.2241e-01, -1.9450e-01,
           7.9236e-02,  1.2439e-01,  1.4562e-01,  6.9139e-02, -1.5607e-01,
           1.2646e-01,  4.3897e-01,  1.7675e-01, -1.2599e-01, -9.3928e-03,
          -9.7599e-02, -2.9685e-01],
         [-3.4103e-02,  2.1105e-01, -1.4067e-01, -2.3230e-01, -1.4529e-01,
          -1.3004e-01,  1.0429e-01, -2.0015e-01, -2.9020e-01, -3.5309e-01,
           1.7866e-01,  1.6113e-01,  1.0063e-04,  7.9846e-02,  1.0402e-01,
          -1.1161e-01,  5.7153e-05,  3.4229e-01, -1.4291e-01, -2.1545e-01,
           9.6762e-02,  1.2911e-01,  1.5455e-01,  5.5991e-02, -1.4989e-01,
           1.4431e-01,  4.6943e-01,  1.7893e-01, -1.4973e-01, -1.3155e-02,
          -1.0198e-01, -3.1793e-01],
         [-2.8775e-02,  2.0091e-01, -1.4611e-01, -2.8379e-01, -1.5510e-01,
          -1.4712e-01,  1.0568e-01, -2.1340e-01, -3.0495e-01, -3.8946e-01,
           2.3476e-01,  1.6864e-01, -1.3468e-02,  9.0592e-02,  1.2449e-01,
          -1.2788e-01,  3.5040e-02,  3.9368e-01, -1.7401e-01, -2.4911e-01,
           1.2283e-01,  1.3205e-01,  1.7184e-01,  3.1711e-02, -1.4308e-01,
           1.7194e-01,  5.1388e-01,  1.7828e-01, -1.8346e-01, -1.7234e-02,
          -1.0748e-01, -3.5379e-01],
         [-2.8085e-02,  1.9167e-01, -1.5413e-01, -3.2994e-01, -1.6635e-01,
          -1.5925e-01,  1.0373e-01, -2.2259e-01, -3.1478e-01, -4.2251e-01,
           2.8913e-01,  1.7603e-01, -2.8068e-02,  9.6978e-02,  1.4034e-01,
          -1.3886e-01,  7.0943e-02,  4.4552e-01, -1.9893e-01, -2.8609e-01,
           1.4694e-01,  1.3265e-01,  1.8337e-01,  8.0494e-03, -1.4127e-01,
           2.0194e-01,  5.5225e-01,  1.7668e-01, -2.1408e-01, -1.6161e-02,
          -1.1027e-01, -3.8466e-01],
         [-2.6790e-02,  1.9099e-01, -1.7541e-01, -3.5732e-01, -1.6998e-01,
          -1.4551e-01,  1.0192e-01, -2.2157e-01, -3.3641e-01, -4.4441e-01,
           3.5849e-01,  1.6956e-01, -2.1010e-02,  1.0022e-01,  1.6709e-01,
          -1.4399e-01,  1.1251e-01,  4.8530e-01, -2.3333e-01, -3.1115e-01,
           1.6732e-01,  1.5364e-01,  1.6485e-01, -1.7026e-02, -1.3376e-01,
           2.3337e-01,  5.8118e-01,  1.9232e-01, -2.4205e-01, -2.4864e-02,
          -9.9723e-02, -3.8884e-01],
         [-3.1921e-02,  1.8207e-01, -1.8430e-01, -4.1043e-01, -1.8888e-01,
          -1.6619e-01,  8.2869e-02, -2.2350e-01, -3.3632e-01, -4.7321e-01,
           4.0675e-01,  1.8144e-01, -4.8160e-02,  1.0294e-01,  1.7646e-01,
          -1.5203e-01,  1.4377e-01,  5.3926e-01, -2.4682e-01, -3.5601e-01,
           1.8859e-01,  1.3725e-01,  1.7965e-01, -5.2108e-02, -1.4613e-01,
           2.6840e-01,  6.0647e-01,  1.7664e-01, -2.6109e-01, -1.3898e-02,
          -9.6924e-02, -4.2674e-01],
         [-3.4165e-02,  1.7002e-01, -1.9811e-01, -4.8369e-01, -2.1181e-01,
          -1.9008e-01,  6.5810e-02, -2.2928e-01, -3.3894e-01, -5.2173e-01,
           4.8517e-01,  1.9304e-01, -8.2579e-02,  1.0989e-01,  1.9627e-01,
          -1.6058e-01,  1.9587e-01,  6.1624e-01, -2.6901e-01, -4.1900e-01,
           2.2485e-01,  1.2415e-01,  1.9702e-01, -9.4203e-02, -1.5834e-01,
           3.2059e-01,  6.4962e-01,  1.6556e-01, -2.9645e-01, -3.7468e-03,
          -9.4394e-02, -4.7610e-01],
         [-3.3791e-02,  1.6030e-01, -2.0854e-01, -5.4965e-01, -2.3196e-01,
          -2.1433e-01,  5.1434e-02, -2.3505e-01, -3.3691e-01, -5.6878e-01,
           5.5333e-01,  2.0182e-01, -1.1859e-01,  1.1869e-01,  2.1384e-01,
          -1.6489e-01,  2.4269e-01,  6.8173e-01, -2.8244e-01, -4.7585e-01,
           2.5994e-01,  1.1087e-01,  2.1602e-01, -1.2751e-01, -1.7183e-01,
           3.6858e-01,  6.8688e-01,  1.5652e-01, -3.2778e-01,  5.6582e-03,
          -9.2988e-02, -5.2168e-01],
         [-3.0094e-02,  1.7882e-01, -1.7030e-01, -4.1542e-01, -1.9277e-01,
          -1.8726e-01,  8.3209e-02, -2.2905e-01, -3.1685e-01, -4.8002e-01,
           3.8100e-01,  1.8837e-01, -7.0270e-02,  1.0645e-01,  1.6365e-01,
          -1.4759e-01,  1.3260e-01,  5.3366e-01, -2.2282e-01, -3.5983e-01,
           1.9025e-01,  1.1761e-01,  2.0412e-01, -4.0110e-02, -1.5662e-01,
           2.6410e-01,  6.0190e-01,  1.6438e-01, -2.5527e-01, -4.4310e-03,
          -1.0691e-01, -4.4225e-01],
         [-3.0094e-02,  1.7882e-01, -1.7030e-01, -4.1542e-01, -1.9277e-01,
          -1.8726e-01,  8.3209e-02, -2.2905e-01, -3.1685e-01, -4.8002e-01,
           3.8100e-01,  1.8837e-01, -7.0270e-02,  1.0645e-01,  1.6365e-01,
          -1.4759e-01,  1.3260e-01,  5.3366e-01, -2.2282e-01, -3.5983e-01,
           1.9025e-01,  1.1761e-01,  2.0412e-01, -4.0110e-02, -1.5662e-01,
           2.6410e-01,  6.0190e-01,  1.6438e-01, -2.5527e-01, -4.4310e-03,
          -1.0691e-01, -4.4225e-01],
         [-3.0094e-02,  1.7882e-01, -1.7030e-01, -4.1542e-01, -1.9277e-01,
          -1.8726e-01,  8.3209e-02, -2.2905e-01, -3.1685e-01, -4.8002e-01,
           3.8100e-01,  1.8837e-01, -7.0270e-02,  1.0645e-01,  1.6365e-01,
          -1.4759e-01,  1.3260e-01,  5.3366e-01, -2.2282e-01, -3.5983e-01,
           1.9025e-01,  1.1761e-01,  2.0412e-01, -4.0110e-02, -1.5662e-01,
           2.6410e-01,  6.0190e-01,  1.6438e-01, -2.5527e-01, -4.4310e-03,
          -1.0691e-01, -4.4225e-01],
         [-3.9598e-02,  1.3253e-01, -2.3309e-01, -6.8013e-01, -2.6364e-01,
          -2.3634e-01,  3.9048e-02, -2.6314e-01, -3.4817e-01, -6.7807e-01,
           7.2156e-01,  2.2680e-01, -1.6967e-01,  1.4565e-01,  2.5814e-01,
          -1.8038e-01,  3.6082e-01,  8.3843e-01, -3.3158e-01, -6.0068e-01,
           3.4253e-01,  1.1345e-01,  2.5228e-01, -1.9802e-01, -1.6736e-01,
           4.7321e-01,  8.0452e-01,  1.6220e-01, -4.1670e-01,  1.4016e-02,
          -9.0186e-02, -6.1582e-01],
         [-3.0094e-02,  1.7882e-01, -1.7030e-01, -4.1542e-01, -1.9277e-01,
          -1.8726e-01,  8.3209e-02, -2.2905e-01, -3.1685e-01, -4.8002e-01,
           3.8100e-01,  1.8837e-01, -7.0270e-02,  1.0645e-01,  1.6365e-01,
          -1.4759e-01,  1.3260e-01,  5.3366e-01, -2.2282e-01, -3.5983e-01,
           1.9025e-01,  1.1761e-01,  2.0412e-01, -4.0110e-02, -1.5662e-01,
           2.6410e-01,  6.0190e-01,  1.6438e-01, -2.5527e-01, -4.4310e-03,
          -1.0691e-01, -4.4225e-01],
         [-4.5150e-02,  1.3087e-01, -2.0140e-01, -6.7107e-01, -2.6944e-01,
          -2.5753e-01,  5.4125e-02, -2.8311e-01, -3.0352e-01, -6.8314e-01,
           6.6147e-01,  2.3258e-01, -1.9249e-01,  1.4408e-01,  2.2431e-01,
          -1.6362e-01,  3.3969e-01,  8.1065e-01, -2.8083e-01, -5.8406e-01,
           3.5342e-01,  9.8901e-02,  2.9771e-01, -1.4845e-01, -1.8102e-01,
           4.7296e-01,  8.0588e-01,  1.5962e-01, -4.0862e-01,  2.3788e-02,
          -1.0981e-01, -6.3758e-01],
         [-4.3045e-02,  1.3470e-01, -1.9275e-01, -6.4374e-01, -2.6249e-01,
          -2.5669e-01,  6.0426e-02, -2.7890e-01, -2.9892e-01, -6.6315e-01,
           6.2312e-01,  2.2817e-01, -1.8431e-01,  1.4021e-01,  2.1230e-01,
          -1.5829e-01,  3.1539e-01,  7.7828e-01, -2.6674e-01, -5.5937e-01,
           3.3928e-01,  9.7091e-02,  2.9591e-01, -1.2904e-01, -1.8180e-01,
           4.5092e-01,  7.8236e-01,  1.5886e-01, -3.9180e-01,  2.2740e-02,
          -1.1243e-01, -6.2014e-01],
         [-3.0094e-02,  1.7882e-01, -1.7030e-01, -4.1542e-01, -1.9277e-01,
          -1.8726e-01,  8.3209e-02, -2.2905e-01, -3.1685e-01, -4.8002e-01,
           3.8100e-01,  1.8837e-01, -7.0270e-02,  1.0645e-01,  1.6365e-01,
          -1.4759e-01,  1.3260e-01,  5.3366e-01, -2.2282e-01, -3.5983e-01,
           1.9025e-01,  1.1761e-01,  2.0412e-01, -4.0110e-02, -1.5662e-01,
           2.6410e-01,  6.0190e-01,  1.6438e-01, -2.5527e-01, -4.4310e-03,
          -1.0691e-01, -4.4225e-01],
         [-3.0094e-02,  1.7882e-01, -1.7030e-01, -4.1542e-01, -1.9277e-01,
          -1.8726e-01,  8.3209e-02, -2.2905e-01, -3.1685e-01, -4.8002e-01,
           3.8100e-01,  1.8837e-01, -7.0270e-02,  1.0645e-01,  1.6365e-01,
          -1.4759e-01,  1.3260e-01,  5.3366e-01, -2.2282e-01, -3.5983e-01,
           1.9025e-01,  1.1761e-01,  2.0412e-01, -4.0110e-02, -1.5662e-01,
           2.6410e-01,  6.0190e-01,  1.6438e-01, -2.5527e-01, -4.4310e-03,
          -1.0691e-01, -4.4225e-01],
         [-3.0094e-02,  1.7882e-01, -1.7030e-01, -4.1542e-01, -1.9277e-01,
          -1.8726e-01,  8.3209e-02, -2.2905e-01, -3.1685e-01, -4.8002e-01,
           3.8100e-01,  1.8837e-01, -7.0270e-02,  1.0645e-01,  1.6365e-01,
          -1.4759e-01,  1.3260e-01,  5.3366e-01, -2.2282e-01, -3.5983e-01,
           1.9025e-01,  1.1761e-01,  2.0412e-01, -4.0110e-02, -1.5662e-01,
           2.6410e-01,  6.0190e-01,  1.6438e-01, -2.5527e-01, -4.4310e-03,
          -1.0691e-01, -4.4225e-01],
         [-2.8822e-02,  1.4656e-01, -1.7341e-01, -5.5076e-01, -2.2618e-01,
          -2.3821e-01,  8.7929e-02, -2.7299e-01, -3.0231e-01, -6.0336e-01,
           5.1761e-01,  2.1762e-01, -1.5118e-01,  1.3801e-01,  1.8548e-01,
          -1.4940e-01,  2.4804e-01,  6.7967e-01, -2.3945e-01, -4.8321e-01,
           2.8717e-01,  1.0534e-01,  2.7833e-01, -7.2018e-02, -1.6278e-01,
           3.6798e-01,  7.2404e-01,  1.6421e-01, -3.4747e-01,  1.2162e-02,
          -1.2587e-01, -5.5347e-01],
         [-3.1018e-02,  1.8161e-01, -1.6861e-01, -4.1461e-01, -1.9831e-01,
          -2.0230e-01,  6.6377e-02, -2.1811e-01, -3.0333e-01, -4.7043e-01,
           3.6115e-01,  1.8826e-01, -8.0724e-02,  1.0310e-01,  1.5531e-01,
          -1.4127e-01,  1.1426e-01,  5.2196e-01, -2.0373e-01, -3.5913e-01,
           1.8238e-01,  9.5984e-02,  2.0916e-01, -4.7507e-02, -1.7380e-01,
           2.6259e-01,  5.7467e-01,  1.4957e-01, -2.3160e-01,  5.0659e-03,
          -9.7473e-02, -4.4459e-01],
         [-3.0735e-02,  1.8032e-01, -1.6871e-01, -4.1574e-01, -1.9613e-01,
          -1.9537e-01,  7.2666e-02, -2.2303e-01, -3.0850e-01, -4.7456e-01,
           3.6917e-01,  1.8904e-01, -7.7230e-02,  1.0479e-01,  1.5957e-01,
          -1.4396e-01,  1.2251e-01,  5.2598e-01, -2.1106e-01, -3.5858e-01,
           1.8615e-01,  1.0493e-01,  2.0701e-01, -4.4508e-02, -1.6662e-01,
           2.6291e-01,  5.8655e-01,  1.5550e-01, -2.4182e-01,  4.4002e-04,
          -1.0218e-01, -4.4427e-01],
         [-3.0864e-02,  1.8091e-01, -1.6867e-01, -4.1523e-01, -1.9713e-01,
          -1.9853e-01,  6.9795e-02, -2.2079e-01, -3.0614e-01, -4.7268e-01,
           3.6551e-01,  1.8868e-01, -7.8825e-02,  1.0402e-01,  1.5762e-01,
          -1.4273e-01,  1.1874e-01,  5.2414e-01, -2.0771e-01, -3.5883e-01,
           1.8443e-01,  1.0085e-01,  2.0800e-01, -4.5877e-02, -1.6990e-01,
           2.6276e-01,  5.8113e-01,  1.5279e-01, -2.3715e-01,  2.5515e-03,
          -1.0003e-01, -4.4442e-01],
         [-3.0937e-02,  1.8124e-01, -1.6864e-01, -4.1493e-01, -1.9769e-01,
          -2.0032e-01,  6.8175e-02, -2.1952e-01, -3.0481e-01, -4.7161e-01,
           3.6345e-01,  1.8848e-01, -7.9725e-02,  1.0358e-01,  1.5652e-01,
          -1.4204e-01,  1.1662e-01,  5.2311e-01, -2.0583e-01, -3.5897e-01,
           1.8346e-01,  9.8543e-02,  2.0855e-01, -4.6649e-02, -1.7174e-01,
           2.6268e-01,  5.7807e-01,  1.5127e-01, -2.3452e-01,  3.7428e-03,
          -9.8819e-02, -4.4450e-01]]], grad_fn=<AddBackward0>) torch.Size([1, 23, 32])
#### arr_b tensor([[[ 1.6416,  0.2271, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.8722,  0.3001, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.2511,  0.3824, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.6105,  0.4627, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.8426,  0.5884, -0.2861, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2659,  0.5711, -0.2796, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.8512,  0.6076, -0.2809, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3783,  0.6288, -0.2535, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.4571,  0.8924, -0.2310, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.3584,  0.7907,  0.1813, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.1351,  0.7417,  0.2302, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3846,  0.7205,  0.3123, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3435, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4078, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3785, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3619, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.5889,  0.3488, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459]]]) torch.Size([1, 24, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<AddBackward0>) torch.Size([1, 24, 32])