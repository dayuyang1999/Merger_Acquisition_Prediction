
CUDA availability: True
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 198, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)*1000
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 358, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
#### arr_b tensor([[[ 1.6416,  0.2271, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.8722,  0.3001, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.2511,  0.3824, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.6105,  0.4627, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.8426,  0.5884, -0.2861, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2659,  0.5711, -0.2796, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.8512,  0.6076, -0.2809, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3783,  0.6288, -0.2535, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.4571,  0.8924, -0.2310, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.3584,  0.7907,  0.1813, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.1351,  0.7417,  0.2302, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3846,  0.7205,  0.3123, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3435, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4078, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3785, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3619, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.5889,  0.3488, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459]]]) torch.Size([1, 24, 14])
#### mat_b tensor([[[-2.0762e-01, -9.9483e-02, -1.5536e-01,  5.5013e-02,  5.3152e-01,
           8.7681e-02, -1.5848e-01, -1.4760e-02,  2.1392e-01, -9.9728e-02,
          -8.5513e-02,  3.7934e-01,  8.9001e-02, -3.6694e-02, -2.1236e-01,
           2.7815e-02, -4.5183e-01, -3.1763e-01,  5.5144e-01, -1.9802e-01,
          -5.0061e-02,  7.2623e-02, -1.4934e-01, -1.2808e-03, -2.7566e-02,
          -2.7354e-01,  2.7488e-01,  2.7762e-02,  5.7816e-02,  4.6107e-01,
          -3.6167e-01, -5.1261e-02],
         [-1.2845e-01, -7.9780e-02, -2.9232e-01,  1.7245e-02,  4.8073e-01,
          -4.3511e-05, -2.1814e-01,  7.2400e-03,  2.4988e-01, -1.4892e-02,
          -1.7223e-01,  2.1161e-01,  1.2421e-01, -7.8485e-02, -1.2798e-01,
           3.7631e-02, -3.6193e-01, -2.0809e-01,  4.3355e-01, -1.0196e-01,
           2.7314e-01,  2.1214e-01, -6.2068e-02,  1.9564e-01, -8.5518e-02,
          -2.2343e-01,  1.5021e-01, -4.0171e-02,  1.8757e-01,  4.6077e-01,
          -2.9656e-01,  1.9286e-02],
         [-2.3606e-01, -2.0732e-01, -3.5892e-01,  2.8382e-01,  5.8119e-01,
          -4.7111e-02, -3.4946e-02, -1.0030e-01,  2.6971e-01,  5.4023e-03,
          -3.0020e-01,  1.7637e-01, -4.9948e-02, -3.6662e-02, -2.4258e-01,
           2.0716e-02, -1.3287e-01, -2.2657e-01,  6.9301e-01,  2.8660e-02,
          -4.8696e-02,  3.7872e-01, -9.0658e-02,  1.6251e-01, -2.4411e-01,
          -1.2513e-01,  3.8992e-01, -1.1389e-01,  2.2530e-01,  2.9530e-01,
          -1.7196e-01, -1.5947e-01],
         [-2.0852e-01,  1.2016e-01, -1.0872e-01, -2.4214e-01,  3.7269e-01,
           7.9206e-02,  6.9964e-02,  8.4281e-02, -4.6108e-02,  1.8568e-01,
          -3.6719e-01,  2.9792e-01,  2.4718e-01, -2.7273e-01, -3.2466e-01,
           2.5747e-01, -2.7117e-02, -1.8611e-01,  4.5449e-01, -1.4695e-01,
           3.5724e-01,  3.1068e-01,  3.3590e-02,  2.1426e-01, -1.0519e-01,
          -2.0579e-01,  5.3253e-02, -3.2898e-01, -4.4931e-03,  2.1175e-01,
          -3.3524e-01, -2.6145e-02],
         [-3.4029e-01, -4.5530e-02,  2.8583e-03,  1.0625e-01,  6.6960e-01,
           9.0859e-02, -1.1671e-01, -8.2913e-02,  4.0951e-02,  4.6002e-02,
          -6.9314e-02,  4.5088e-01,  1.8013e-01, -2.7690e-01, -1.7021e-01,
           6.9616e-02, -1.2185e-01, -2.1321e-01,  5.1227e-01, -8.9681e-02,
           1.1413e-01,  3.5728e-01, -2.0163e-01, -7.6534e-02, -4.1280e-02,
          -2.6111e-01, -6.1946e-02, -2.1465e-01,  2.8997e-01,  4.8202e-01,
          -3.4528e-01, -1.5939e-01],
         [-1.4671e-01, -2.2665e-03,  1.2425e-01,  3.3154e-01,  2.2625e-01,
          -5.0799e-01, -6.5466e-02,  7.9341e-02,  3.0296e-02, -2.2299e-01,
          -4.5928e-01,  1.7583e-01, -9.5716e-02, -2.9988e-01, -5.2797e-01,
           3.0421e-01, -2.9957e-01, -4.0633e-01,  5.8601e-01, -1.3061e-01,
           9.0660e-02,  1.4465e-01, -2.7640e-01,  2.0571e-01, -3.4055e-02,
          -5.5884e-01,  4.5312e-02, -3.4657e-01, -2.2425e-01,  6.2716e-01,
          -2.9137e-01, -2.1569e-01],
         [-2.8472e-01, -1.5862e-01, -2.2177e-01,  6.1648e-02,  8.8216e-01,
          -1.8883e-01,  1.5811e-01, -2.6169e-01,  5.5568e-01,  1.9230e-01,
          -1.6976e-01,  1.3674e-01,  2.0624e-02, -3.0206e-01, -3.7654e-01,
           2.2413e-01, -5.3322e-01, -1.9018e-01,  3.8880e-01, -2.0260e-01,
           2.5273e-01,  5.0493e-01,  7.8191e-02,  1.1189e-01, -1.5433e-02,
          -8.2603e-01,  1.5698e-01, -3.4493e-01, -1.1644e-01,  5.9781e-01,
          -3.0561e-01, -2.4150e-01],
         [-2.1764e-01,  4.1730e-02,  1.4581e-01,  4.2935e-01,  8.9651e-01,
          -1.5168e-01, -1.3220e-01,  1.1669e-01,  2.8931e-01, -1.9780e-01,
          -3.5432e-01,  4.7169e-01, -1.1980e-01, -9.8650e-02, -1.7234e-01,
           1.2500e-02, -3.6947e-01, -6.4119e-01,  9.1421e-01, -1.1232e-01,
           3.9559e-01,  7.2762e-02, -5.0711e-01,  1.4153e-01, -2.5811e-01,
          -4.4721e-01,  3.6111e-01,  2.4929e-01,  1.0974e-01,  8.2152e-01,
          -3.4664e-01,  1.3649e-01],
         [-4.2963e-01,  1.2615e-02, -1.4087e-02,  2.0296e-01,  6.6846e-01,
          -1.9262e-01, -1.1808e-01,  1.0604e-01,  1.4378e-01, -1.2743e-01,
          -3.5545e-01,  4.4138e-01,  6.3162e-02, -5.0196e-02, -4.4802e-01,
           1.8303e-01, -5.3034e-01, -5.8427e-01,  6.9985e-01, -5.0340e-02,
           1.2265e-01,  3.2072e-01, -1.9559e-01,  1.5226e-01, -2.6810e-01,
          -4.8857e-01,  2.6829e-01, -1.1162e-01,  2.3235e-01,  7.9088e-01,
          -6.0051e-01, -1.6587e-02],
         [-3.1993e-01, -1.3535e-01, -1.7377e-01,  1.3733e-01,  7.5532e-01,
           1.8152e-02,  1.1677e-02, -1.8088e-01,  3.8961e-01,  1.4022e-02,
          -3.3281e-01,  4.7231e-01,  1.8927e-01, -1.3458e-01, -4.2598e-01,
           1.3959e-01, -1.3919e-01, -2.2703e-01,  6.3447e-01, -1.7351e-01,
           2.1558e-01,  3.3882e-01, -8.8468e-02,  9.6228e-02, -9.2657e-02,
          -3.4968e-01,  3.6296e-01, -7.7238e-02, -2.4845e-01,  5.0777e-01,
          -3.4587e-01, -1.8640e-01],
         [-2.1260e-01,  3.8535e-02, -3.9236e-02,  1.8746e-01,  6.9989e-01,
          -1.2227e-01, -1.9993e-01,  2.2139e-01,  3.0088e-01, -5.9523e-02,
          -3.1069e-01,  3.4642e-01, -5.4459e-02, -1.3017e-01, -2.3983e-01,
           1.5271e-01, -4.2141e-01, -6.1842e-01,  6.7952e-01,  8.7732e-02,
           2.5479e-01,  2.1213e-01, -2.9065e-01,  2.3901e-01, -2.7137e-01,
          -5.2526e-01,  3.7852e-01, -7.9387e-02,  1.5869e-01,  6.1752e-01,
          -4.9377e-01,  1.7160e-01],
         [-2.4072e-01, -1.2126e-01, -4.7116e-01,  1.8087e-01,  2.9888e-01,
          -2.9036e-01, -1.2017e-01,  5.0502e-01,  6.9284e-02, -4.5638e-01,
          -7.8135e-01, -2.1722e-01,  4.6682e-02, -1.0063e-01, -6.7602e-01,
           2.5934e-01, -6.3017e-01, -6.2431e-01,  1.0724e+00,  6.6012e-02,
           2.2102e-01,  1.3044e-01, -2.3121e-01,  2.6745e-01, -3.2739e-01,
          -7.9998e-01, -4.4340e-02, -6.6273e-01, -7.0015e-03,  4.9690e-01,
          -8.1705e-01, -6.3215e-03],
         [-3.1465e-01, -1.0660e-01, -9.6787e-02,  2.9892e-01,  4.7447e-01,
          -3.5973e-01, -4.7715e-02,  3.9812e-03,  1.5784e-01, -1.5371e-01,
          -4.9418e-01,  1.7436e-01,  1.2126e-01, -2.9212e-01, -2.3715e-01,
           2.9040e-02,  6.4026e-03, -4.0694e-01,  4.1619e-01, -1.2216e-01,
           1.7065e-01,  6.4337e-01, -8.8584e-02,  3.0769e-01, -1.1692e-01,
          -4.1679e-01,  2.0214e-01, -3.9881e-01,  4.7506e-02,  5.2175e-01,
          -2.5387e-01, -1.8230e-01],
         [-2.0438e-01, -1.4365e-01,  1.2699e-02,  2.9836e-01,  9.7238e-01,
           1.4894e-01,  1.6362e-01, -6.5239e-01,  5.6342e-01,  1.6086e-01,
          -1.7973e-01,  4.3751e-01,  2.7699e-01, -1.3873e-01, -3.8225e-01,
           2.3189e-01,  2.9049e-01, -5.6707e-01,  7.8486e-01, -4.5494e-01,
           7.9679e-02,  7.4697e-01, -6.4657e-01,  8.9758e-02, -5.0721e-01,
           3.3627e-01,  5.3892e-01,  3.1434e-02, -2.3957e-02,  5.1485e-01,
          -2.8984e-01, -6.8813e-01],
         [ 1.7042e-02, -3.4854e-01, -5.4350e-01,  4.3507e-01,  5.1794e-01,
          -1.5869e-01, -2.1446e-02, -1.6917e-01,  4.7238e-01, -4.6891e-01,
          -7.4957e-02,  1.0510e-01,  8.7291e-02, -3.8560e-01,  2.2744e-01,
           4.8832e-01,  4.2638e-02, -3.7974e-01,  1.5685e-02, -8.3772e-02,
          -4.0051e-01,  5.4737e-01, -4.7961e-01,  2.0589e-01, -2.3571e-01,
          -3.4629e-01,  2.1783e-01, -2.3712e-01, -4.2508e-01,  8.0315e-01,
          -4.5335e-01, -6.8098e-01],
         [-2.3330e-01, -7.5917e-02, -2.6319e-01,  1.0043e-01,  5.1327e-01,
          -1.7976e-01, -1.3716e-01, -1.1612e-01,  2.7329e-01, -1.0664e-03,
          -2.9875e-01,  2.1482e-01,  4.6350e-02, -9.2992e-03, -4.6024e-01,
           5.8284e-02, -4.8458e-01, -2.7201e-01,  5.3139e-01, -7.9098e-02,
           2.0210e-01,  3.0602e-01, -3.4963e-01,  5.0649e-01, -6.9811e-02,
          -2.4039e-01,  3.1244e-01, -1.2921e-01, -1.8729e-01,  6.3883e-01,
          -3.3995e-01, -3.7049e-01],
         [-4.0987e-01, -3.7290e-01, -4.5458e-01,  1.8019e-01,  4.5779e-01,
          -1.6746e-01,  7.6703e-02,  8.3923e-02,  3.3247e-01, -2.7206e-01,
          -4.5626e-01,  1.1874e-01,  2.6135e-01, -2.6744e-01, -2.4192e-01,
           1.7392e-01,  2.8999e-02, -3.9536e-01,  5.8934e-01, -1.5400e-01,
          -2.0326e-02,  3.9027e-01, -1.8791e-01,  2.1993e-01, -1.6921e-01,
          -3.7909e-01,  1.9388e-01, -3.5834e-01,  9.0345e-02,  3.9439e-01,
          -2.9899e-01, -1.2826e-01],
         [-3.0574e-01, -2.8667e-01, -3.6408e-01, -7.8660e-02,  4.3049e-01,
          -8.8584e-02,  1.4180e-01, -1.1476e-01,  3.9834e-01,  4.0563e-02,
          -3.0047e-01,  2.4440e-01,  5.1589e-02, -1.5991e-01, -3.3964e-01,
           2.2067e-01, -2.5176e-01, -1.4529e-01,  4.5160e-01, -1.8904e-01,
          -2.2019e-02,  2.9210e-01, -6.8236e-02,  8.0387e-03,  1.2051e-01,
          -3.5786e-01, -6.8636e-02, -2.8984e-01, -2.0643e-01,  2.7656e-01,
          -3.4934e-01, -1.9281e-01],
         [ 3.3636e-02, -3.6800e-01, -3.0023e-01,  4.4773e-01,  8.0279e-01,
          -4.3515e-01,  5.8413e-03, -2.2333e-01,  6.7810e-01, -1.1778e-01,
          -4.9549e-01,  2.7378e-01, -8.6459e-02, -7.5646e-02, -4.1464e-01,
          -1.5281e-01, -2.1300e-01, -7.3969e-01,  5.8366e-01, -3.7816e-01,
          -4.8573e-02,  7.7000e-01,  2.0562e-01, -1.5007e-01, -4.4835e-01,
          -4.4703e-01,  4.9804e-01, -1.3817e-02, -1.1206e-02,  6.9794e-01,
          -4.8119e-01, -2.3594e-02],
         [-1.0066e-01, -1.8975e-01, -3.4472e-01,  2.8476e-01,  5.1094e-01,
          -3.2498e-01, -1.7067e-01,  1.4112e-01,  3.9032e-01, -3.4084e-01,
          -5.5673e-01,  1.0024e-01,  8.5204e-02, -2.7202e-01, -3.6822e-01,
           9.0080e-02, -1.0638e-01, -6.0085e-01,  5.7942e-01, -8.9878e-02,
          -1.7162e-01,  3.9771e-01, -1.7256e-01,  1.3037e-02, -4.2109e-01,
          -2.7500e-01,  4.7514e-01, -2.0526e-01, -4.4911e-02,  4.8574e-01,
          -6.3674e-01, -2.0537e-01],
         [-1.9651e-01, -1.3259e-01, -2.8731e-01,  2.9619e-01,  6.4491e-01,
          -4.3272e-02, -1.7464e-01, -5.9317e-03,  3.5528e-01, -1.9116e-01,
          -1.9906e-01,  1.3504e-01,  2.0266e-01, -1.1837e-01, -3.6208e-01,
           1.7474e-01, -4.6800e-01, -5.1862e-01,  5.7546e-01, -3.7841e-02,
           1.0317e-01,  2.9099e-01, -5.4142e-01,  3.8648e-01, -2.0243e-01,
          -4.4179e-01,  3.8389e-01, -2.9914e-01, -1.3098e-01,  6.9264e-01,
          -5.4450e-01, -4.0550e-01],
         [-2.4881e-01, -2.7957e-01, -1.2757e-01,  1.6570e-01,  7.7554e-01,
          -9.9817e-02,  1.0840e-01, -2.7527e-01,  4.8712e-01,  6.4655e-02,
          -2.6320e-01,  4.2593e-01,  1.8607e-01, -2.0016e-01, -2.9760e-01,
           7.4781e-02, -1.2435e-02, -2.7576e-01,  4.3906e-01, -1.7996e-01,
           2.4652e-01,  5.7190e-01, -2.8579e-02,  2.1491e-01, -1.8783e-01,
          -4.2797e-01,  4.4690e-01, -2.5401e-02, -1.1185e-01,  6.7449e-01,
          -1.2932e-01, -7.3905e-02],
         [-1.1768e-01, -1.2351e-01, -2.0138e-01,  1.5749e-01,  5.0815e-01,
           6.4123e-02, -2.0484e-01,  4.4870e-02,  2.1515e-01, -6.8936e-02,
          -4.7102e-01,  3.6215e-01,  2.6656e-02,  2.2727e-01, -4.4813e-01,
           3.5008e-02, -4.2045e-01, -4.7177e-01,  8.3408e-01, -2.2942e-01,
           1.4924e-01,  3.2980e-01, -3.8239e-02, -5.5098e-03, -4.1471e-01,
          -1.7566e-01,  3.7496e-01, -1.5344e-02,  4.6758e-02,  5.5063e-01,
          -6.3087e-01,  7.2742e-03],
         [-8.3037e-02,  4.5255e-02, -1.2647e-01,  4.5224e-03,  3.6692e-01,
           3.5393e-02, -8.9915e-02, -6.2729e-02,  2.0064e-01,  3.0845e-02,
          -6.4497e-02,  1.6789e-01,  1.2324e-01, -8.1512e-02, -2.1233e-01,
           4.9421e-02, -2.1990e-01, -3.8055e-01,  4.1848e-01, -1.3107e-01,
          -1.3423e-01,  2.6834e-01, -1.2605e-01,  7.5795e-02, -1.6947e-01,
          -7.4402e-02,  2.5839e-01, -1.9671e-01,  1.1839e-01,  3.2958e-01,
          -3.8369e-01, -1.9278e-01]]], grad_fn=<UnsqueezeBackward0>) torch.Size([1, 24, 32])
#### arr_b tensor([[[ 1.9320,  0.3100, -0.0375, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471],
         [ 2.1965,  0.3937, -0.0375, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471],
         [ 2.6309,  0.4881, -0.0375, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471],
         [ 3.0431,  0.5802, -0.0375, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471],
         [ 3.3092,  0.7243, -0.2785, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471],
         [ 3.7946,  0.7044, -0.2711, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471],
         [ 4.4658,  0.7463, -0.2725, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471],
         [ 5.0702,  0.7706, -0.2411, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471],
         [ 3.8289,  0.6222, -0.0375, -0.3471, -0.3471, -0.3471, -0.3471,
          -0.3471, -0.3471, -0.3471, -0.3470, -0.3471, -0.3471, -0.3471]]]) torch.Size([1, 9, 14])
#### mat_b tensor([[[-0.1478,  0.1477, -0.4382, -0.1868,  0.5733,  0.2143, -0.0370,
           0.0154,  0.1081,  0.1101,  0.1150,  0.1185,  0.1027, -0.1780,
          -0.0213, -0.0613, -0.2611, -0.2915,  0.1249,  0.2437,  0.3386,
           0.1868, -0.2218,  0.0390,  0.2779, -0.1745, -0.1025,  0.0100,
           0.0214,  0.3640, -0.0493, -0.1582],
         [ 0.0788,  0.0987, -0.2106, -0.0523,  0.8013,  0.1715,  0.0181,
          -0.2964,  0.1849,  0.1977,  0.0027,  0.3557, -0.1265, -0.1909,
          -0.1672, -0.3062, -0.0679, -0.2661,  0.2778,  0.0406,  0.3089,
           0.4076, -0.1487, -0.1167,  0.2013, -0.1077,  0.1244,  0.3053,
          -0.1483,  0.2728,  0.0753, -0.1039],
         [-0.0701,  0.0640, -0.3166,  0.0733,  0.6144,  0.1457,  0.1313,
          -0.1608,  0.0031,  0.0742, -0.0120,  0.3439, -0.1861, -0.2794,
          -0.0060, -0.0347,  0.1239, -0.2526,  0.3355,  0.0651,  0.1256,
           0.3867, -0.3165, -0.1368,  0.1848, -0.1222, -0.1489,  0.0895,
          -0.0522,  0.2295,  0.1045, -0.2230],
         [-0.2590,  0.2037, -0.1480, -0.1540,  0.4186,  0.1569,  0.1906,
          -0.1564, -0.2042,  0.0530, -0.0720,  0.1777,  0.1611, -0.3692,
          -0.3416,  0.0514,  0.1569, -0.3566,  0.3464, -0.1554,  0.2320,
           0.3622, -0.3291, -0.2475,  0.2554, -0.2266, -0.5210, -0.2711,
           0.0270,  0.1587, -0.0396, -0.2859],
         [ 0.1002,  0.3953, -0.6204, -0.2804,  0.9966,  0.0267,  0.0837,
          -0.1023,  0.0625,  0.2813, -0.1572,  0.2054, -0.1839, -0.3981,
          -0.7223, -0.1902, -0.4540, -0.9069,  0.5013,  0.0742,  0.6092,
           0.4270, -0.2471, -0.1107,  0.1910, -0.2691, -0.0783,  0.2224,
          -0.3813,  0.4940, -0.4070, -0.5459],
         [ 0.0139,  0.3703, -0.0447, -0.0593,  0.6717, -0.1967,  0.2365,
          -0.0200, -0.2515,  0.2164, -0.4402, -0.1385, -0.3798, -0.5624,
          -0.7090,  0.0799,  0.0757, -0.8843,  0.4747,  0.1907,  0.7589,
           0.6627, -0.4535,  0.2315,  0.0126, -0.2442,  0.0638, -0.1652,
          -0.3767,  0.3506, -0.1268, -0.3545],
         [ 0.0929,  0.5155, -0.0973,  0.1296,  0.7222, -0.0548, -0.2142,
           0.4212, -0.4549,  0.0066, -0.2367,  0.3386, -0.4101, -0.4266,
          -0.5132, -0.0517, -0.2848, -0.8887,  0.6093,  0.5354,  0.5330,
           0.2981, -0.6749, -0.1463, -0.0036, -0.4828, -0.0831,  0.0060,
          -0.1965,  0.5588, -0.4815, -0.2503],
         [ 0.0562,  0.0622, -0.3585, -0.2763,  0.5509,  0.1037,  0.4700,
           0.0736,  0.3132,  0.1507, -0.4748,  0.2220, -0.1240, -0.3197,
          -0.7285,  0.0400,  0.0860, -0.8398,  0.8731, -0.1041,  0.5928,
           0.3169, -0.6323,  0.0233,  0.0721, -0.2067, -0.0822,  0.1088,
          -0.5076,  0.1150, -0.0384, -0.1015],
         [ 0.1868,  0.2138, -0.4937,  0.0619,  0.7390, -0.3979,  0.1979,
           0.0897,  0.1145,  0.1687, -0.4282,  0.1350, -0.2146, -0.5973,
          -0.6218, -0.2110, -0.2124, -0.8714,  0.4323,  0.1343,  0.3936,
           0.4927, -0.2331,  0.2296, -0.0329, -0.2939,  0.2800,  0.1486,
          -0.2699,  0.3293, -0.2144, -0.3521]]], grad_fn=<UnsqueezeBackward0>) torch.Size([1, 9, 32])
#### arr_b tensor([[[ 2.7062,  0.5123,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 3.0640,  0.6255,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 3.6516,  0.7531,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 4.2091,  0.8777,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766]]]) torch.Size([1, 4, 14])
#### mat_b tensor([[[-1.5054e-01,  4.4583e-01, -6.3587e-01, -1.3877e-01,  6.3642e-01,
           4.4706e-02,  5.9274e-02,  9.0191e-02, -6.4236e-02,  2.0894e-01,
          -3.6451e-02, -5.7075e-02, -1.2728e-01, -5.1576e-01,  5.5379e-02,
          -8.5067e-02,  7.1232e-02, -5.4226e-01, -7.2963e-02,  5.5908e-01,
           1.6595e-01,  5.1152e-01, -3.3875e-01,  1.3722e-01,  3.3557e-01,
          -7.7801e-02, -1.5042e-01, -1.4424e-01, -1.4043e-01,  2.4195e-03,
          -3.5037e-02, -5.0523e-01],
         [ 3.3250e-02,  4.4793e-01, -2.2736e-01, -8.7882e-02,  9.5811e-01,
          -1.5502e-01,  3.3710e-01, -4.1688e-01,  7.1359e-03,  3.3279e-01,
          -5.8091e-02,  6.9402e-03, -2.5500e-01, -5.8463e-01, -3.6786e-01,
          -2.8419e-01,  2.2032e-01, -7.4148e-01,  1.0187e-01,  3.3753e-01,
           6.4482e-01,  7.6474e-01, -4.1203e-01, -2.0981e-01,  2.9439e-01,
          -3.5546e-02, -3.6085e-01,  1.2973e-01, -1.5864e-01,  3.4820e-01,
           1.5176e-01, -4.8797e-01],
         [ 1.4772e-01,  5.4170e-01, -3.9033e-01, -3.3195e-01,  8.9228e-01,
           1.3114e-01,  2.2024e-01, -4.7274e-01, -1.0011e-01,  6.4052e-01,
          -6.3093e-02,  2.8031e-01, -1.5802e-01, -5.6765e-01, -7.2282e-01,
          -3.3257e-01,  3.2537e-01, -7.3627e-01,  2.3287e-01,  8.3644e-02,
           7.6447e-01,  8.2220e-01, -5.0414e-01, -3.5213e-01,  3.1416e-01,
           3.4638e-02, -5.2585e-01,  1.8204e-01, -3.6173e-01,  8.3376e-02,
           1.7157e-01, -5.8144e-01],
         [ 4.3626e-01,  5.6617e-01, -5.0178e-01, -2.6436e-01,  8.7178e-01,
           3.1971e-01,  1.7879e-02, -1.7139e-01,  1.7524e-01,  4.5333e-01,
          -1.2018e-01,  1.4901e-01, -5.3135e-02, -6.1871e-02, -8.9371e-01,
          -6.4237e-01,  1.0801e-03, -1.1587e+00,  5.9870e-01,  1.8254e-01,
           6.8169e-01,  5.5998e-01, -3.4515e-01, -2.9640e-01, -2.8531e-01,
           2.6163e-01,  3.0936e-01,  5.5547e-01, -2.7738e-01,  2.2277e-01,
          -2.8334e-01, -2.3051e-01]]], grad_fn=<UnsqueezeBackward0>) torch.Size([1, 4, 32])
#### arr_b tensor([[[ 1.6416,  0.2271, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.8722,  0.3001, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.2511,  0.3824, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.6105,  0.4627, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.8426,  0.5884, -0.2861, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2659,  0.5711, -0.2796, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.8512,  0.6076, -0.2809, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3783,  0.6288, -0.2535, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.4571,  0.8924, -0.2310, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.3584,  0.7907,  0.1813, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.1351,  0.7417,  0.2302, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3846,  0.7205,  0.3123, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3435, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4078, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3785, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3619, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.5889,  0.3488, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459]]]) torch.Size([1, 24, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<UnsqueezeBackward0>) torch.Size([1, 24, 32])