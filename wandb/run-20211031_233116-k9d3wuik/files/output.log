
CUDA availability: True
  1%|█▍                                                                                                                                              | 5/496 [00:01<01:44,  4.72it/s]
torch.Size([1, 5, 1])
-1
torch.Size([1, 50, 1])
-1
### event lambdas:  tensor([[4.0369, 3.9563, 4.8534, 4.1707, 3.8517],
        [3.7154, 3.6353, 4.5281, 3.8483, 3.5315],
        [3.6977, 3.6177, 4.5102, 3.8306, 3.5140],
        [5.0126, 4.9310, 5.8354, 5.1478, 4.8251],
        [3.6301, 3.5502, 4.4416, 3.7628, 3.4467]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(-35.3433, grad_fn=<NegBackward>) non event loss:  tensor([532476.7125], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(2.6703, grad_fn=<SumBackward0>)
torch.Size([1, 23, 1])
-1
torch.Size([1, 230, 1])
-1
### event lambdas:  tensor([[0.4156, 0.5022, 0.5368, 0.5271, 0.3995, 0.7824, 0.5460, 0.4711, 0.4359,
         0.5637, 0.6527, 0.6322, 0.6248, 0.7202, 0.4996, 0.5396, 0.6509, 0.4890,
         0.2790, 0.4072, 0.5276, 0.5175, 0.4509],
        [0.4508, 0.5429, 0.5797, 0.5694, 0.4336, 0.8380, 0.5894, 0.5100, 0.4724,
         0.6081, 0.7019, 0.6804, 0.6726, 0.7728, 0.5402, 0.5826, 0.7000, 0.5289,
         0.3043, 0.4418, 0.5699, 0.5591, 0.4885],
        [1.9466, 2.1518, 2.2278, 2.2068, 1.9054, 2.6964, 2.2473, 2.0810, 1.9968,
         2.2847, 2.4619, 2.4224, 2.4081, 2.5870, 2.1460, 2.2337, 2.4584, 2.1221,
         1.5581, 1.9251, 2.2080, 2.1857, 2.0332],
        [1.7268, 1.9245, 1.9980, 1.9776, 1.6874, 2.4548, 2.0169, 1.8561, 1.7751,
         2.0532, 2.2256, 2.1871, 2.1731, 2.3477, 1.9188, 2.0038, 2.2222, 1.8958,
         1.3576, 1.7063, 1.9788, 1.9572, 1.8101],
        [0.6091, 0.7235, 0.7685, 0.7559, 0.5874, 1.0754, 0.7803, 0.6828, 0.6361,
         0.8031, 0.9156, 0.8899, 0.8807, 0.9994, 0.7201, 0.7721, 0.9134, 0.7063,
         0.4212, 0.5977, 0.7566, 0.7434, 0.6561],
        [0.5312, 0.6352, 0.6763, 0.6648, 0.5117, 0.9610, 0.6871, 0.5981, 0.5557,
         0.7081, 0.8120, 0.7882, 0.7797, 0.8899, 0.6321, 0.6796, 0.8099, 0.6195,
         0.3631, 0.5210, 0.6655, 0.6534, 0.5739],
        [1.3568, 1.5370, 1.6047, 1.5859, 1.3213, 2.0334, 1.6222, 1.4742, 1.4005,
         1.6558, 1.8168, 1.7807, 1.7676, 1.9319, 1.5317, 1.6100, 1.8136, 1.5106,
         1.0300, 1.3383, 1.5870, 1.5671, 1.4323],
        [1.9669, 2.1728, 2.2490, 2.2279, 1.9256, 2.7185, 2.2686, 2.1017, 2.0173,
         2.3060, 2.4836, 2.4441, 2.4297, 2.6089, 2.1669, 2.2549, 2.4802, 2.1430,
         1.5769, 1.9454, 2.2291, 2.2068, 2.0539],
        [1.7023, 1.8990, 1.9722, 1.9519, 1.6631, 2.4276, 1.9911, 1.8309, 1.7503,
         2.0272, 2.1989, 2.1606, 2.1467, 2.3207, 1.8934, 1.9780, 2.1956, 1.8704,
         1.3355, 1.6819, 1.9531, 1.9316, 1.7852],
        [1.2671, 1.4418, 1.5078, 1.4895, 1.2327, 1.9276, 1.5249, 1.3809, 1.3093,
         1.5576, 1.7149, 1.6796, 1.6669, 1.8278, 1.4368, 1.5130, 1.7119, 1.4162,
         0.9527, 1.2491, 1.4905, 1.4712, 1.3402],
        [0.8154, 0.9536, 1.0070, 0.9921, 0.7888, 1.3613, 1.0210, 0.9048, 0.8484,
         1.0479, 1.1790, 1.1493, 1.1386, 1.2751, 0.9495, 1.0113, 1.1764, 0.9330,
         0.5805, 0.8015, 0.9930, 0.9773, 0.8726],
        [1.7019, 1.8986, 1.9717, 1.9515, 1.6627, 2.4270, 1.9906, 1.8305, 1.7499,
         2.0267, 2.1985, 2.1601, 2.1462, 2.3202, 1.8929, 1.9775, 2.1951, 1.8699,
         1.3351, 1.6814, 1.9526, 1.9312, 1.7847],
        [0.8687, 1.0122, 1.0675, 1.0521, 0.8410, 1.4319, 1.0819, 0.9617, 0.9030,
         1.1097, 1.2448, 1.2142, 1.2032, 1.3435, 1.0080, 1.0719, 1.2421, 0.9909,
         0.6229, 0.8543, 1.0530, 1.0367, 0.9282],
        [1.0130, 1.1694, 1.2291, 1.2125, 0.9825, 1.6170, 1.2446, 1.1145, 1.0505,
         1.2745, 1.4190, 1.3864, 1.3747, 1.5238, 1.1648, 1.2338, 1.4162, 1.1462,
         0.7397, 0.9971, 1.2134, 1.1959, 1.0781],
        [1.2763, 1.4516, 1.5177, 1.4993, 1.2418, 1.9385, 1.5348, 1.3905, 1.3187,
         1.5677, 1.7254, 1.6900, 1.6772, 1.8385, 1.4465, 1.5229, 1.7223, 1.4258,
         0.9606, 1.2582, 1.5004, 1.4810, 1.3496],
        [2.0021, 2.2090, 2.2855, 2.2643, 1.9605, 2.7566, 2.3052, 2.1376, 2.0527,
         2.3428, 2.5210, 2.4813, 2.4669, 2.6467, 2.2031, 2.2915, 2.5175, 2.1790,
         1.6093, 1.9804, 2.2655, 2.2431, 2.0895],
        [1.8648, 2.0674, 2.1425, 2.1217, 1.8242, 2.6071, 2.1618, 1.9974, 1.9143,
         2.1988, 2.3744, 2.3352, 2.3210, 2.4984, 2.0616, 2.1484, 2.3709, 2.0380,
         1.4830, 1.8436, 2.1229, 2.1009, 1.9502],
        [1.1526, 1.3197, 1.3831, 1.3655, 1.1199, 1.7898, 1.3995, 1.2613, 1.1929,
         1.4311, 1.5832, 1.5490, 1.5366, 1.6927, 1.3149, 1.3881, 1.5802, 1.2951,
         0.8557, 1.1355, 1.3665, 1.3479, 1.2224],
        [2.0137, 2.2209, 2.2975, 2.2763, 1.9720, 2.7692, 2.3172, 2.1494, 2.0644,
         2.3549, 2.5333, 2.4936, 2.4792, 2.6592, 2.2150, 2.3035, 2.5299, 2.1909,
         1.6201, 1.9920, 2.2775, 2.2551, 2.1012],
        [2.0456, 2.2538, 2.3307, 2.3094, 2.0038, 2.8037, 2.3504, 2.1820, 2.0966,
         2.3883, 2.5672, 2.5274, 2.5129, 2.6934, 2.2478, 2.3367, 2.5638, 2.2236,
         1.6497, 2.0238, 2.3106, 2.2881, 2.1336],
        [2.0469, 2.2551, 2.3320, 2.3107, 2.0050, 2.8050, 2.3518, 2.1833, 2.0979,
         2.3896, 2.5686, 2.5287, 2.5143, 2.6948, 2.2491, 2.3380, 2.5651, 2.2249,
         1.6508, 2.0251, 2.3119, 2.2894, 2.1348],
        [0.5892, 0.7011, 0.7451, 0.7328, 0.5680, 1.0466, 0.7566, 0.6613, 0.6156,
         0.7790, 0.8894, 0.8642, 0.8551, 0.9718, 0.6977, 0.7486, 0.8872, 0.6842,
         0.4063, 0.5781, 0.7335, 0.7205, 0.6352],
        [0.9070, 1.0541, 1.1107, 1.0949, 0.8785, 1.4817, 1.1254, 1.0024, 0.9422,
         1.1538, 1.2915, 1.2604, 1.2492, 1.3920, 1.0498, 1.1151, 1.2888, 1.0323,
         0.6535, 0.8921, 1.0958, 1.0792, 0.9681]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(-161.9141, grad_fn=<NegBackward>) non event loss:  tensor([3181402.7269], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(4.5277, grad_fn=<SumBackward0>)
torch.Size([1, 5, 1])
-1
torch.Size([1, 50, 1])
-1
### event lambdas:  tensor([[0.0207, 0.0357, 0.0150, 0.0123, 0.0132],
        [0.0370, 0.0636, 0.0270, 0.0221, 0.0236],
        [0.0598, 0.1019, 0.0437, 0.0359, 0.0384],
        [0.0996, 0.1676, 0.0732, 0.0603, 0.0644],
        [0.0938, 0.1581, 0.0689, 0.0567, 0.0606]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(76.9935, grad_fn=<NegBackward>) non event loss:  tensor([15926.3897], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.2212, grad_fn=<SumBackward0>)
torch.Size([1, 9, 1])
-1
torch.Size([1, 90, 1])
-1
### event lambdas:  tensor([[0.0050, 0.0086, 0.0078, 0.0090, 0.0045, 0.0035, 0.0097, 0.0062, 0.0033],
        [0.0005, 0.0009, 0.0008, 0.0009, 0.0005, 0.0004, 0.0010, 0.0007, 0.0003],
        [0.0038, 0.0064, 0.0058, 0.0067, 0.0034, 0.0026, 0.0073, 0.0047, 0.0025],
        [0.0009, 0.0016, 0.0015, 0.0017, 0.0008, 0.0007, 0.0018, 0.0012, 0.0006],
        [0.0009, 0.0015, 0.0014, 0.0016, 0.0008, 0.0006, 0.0017, 0.0011, 0.0006],
        [0.0043, 0.0074, 0.0067, 0.0077, 0.0038, 0.0030, 0.0083, 0.0054, 0.0028],
        [0.0003, 0.0005, 0.0004, 0.0005, 0.0003, 0.0002, 0.0005, 0.0004, 0.0002],
        [0.0002, 0.0003, 0.0003, 0.0004, 0.0002, 0.0001, 0.0004, 0.0003, 0.0001],
        [0.0003, 0.0005, 0.0004, 0.0005, 0.0003, 0.0002, 0.0006, 0.0004, 0.0002]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(550.5803, grad_fn=<NegBackward>) non event loss:  tensor([1584.8641], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0322, grad_fn=<SumBackward0>)
torch.Size([1, 11, 1])
-1
torch.Size([1, 110, 1])
-1
### event lambdas:  tensor([[4.8323e-04, 2.8746e-04, 1.0303e-03, 5.0810e-04, 3.5900e-04, 3.2118e-04,
         6.2752e-04, 6.1516e-04, 5.3326e-04, 3.2137e-04, 3.9188e-04],
        [1.1922e-03, 7.0931e-04, 2.5408e-03, 1.2535e-03, 8.8577e-04, 7.9248e-04,
         1.5480e-03, 1.5175e-03, 1.3156e-03, 7.9295e-04, 9.6688e-04],
        [1.1604e-04, 6.9022e-05, 2.4745e-04, 1.2201e-04, 8.6201e-05, 7.7118e-05,
         1.5069e-04, 1.4772e-04, 1.2805e-04, 7.7164e-05, 9.4097e-05],
        [3.7603e-04, 2.2369e-04, 8.0176e-04, 3.9538e-04, 2.7935e-04, 2.4992e-04,
         4.8832e-04, 4.7870e-04, 4.1496e-04, 2.5007e-04, 3.0494e-04],
        [1.0225e-04, 6.0823e-05, 2.1805e-04, 1.0752e-04, 7.5961e-05, 6.7957e-05,
         1.3279e-04, 1.3018e-04, 1.1284e-04, 6.7998e-05, 8.2920e-05],
        [1.4783e-03, 8.7959e-04, 3.1500e-03, 1.5543e-03, 1.0984e-03, 9.8271e-04,
         1.9194e-03, 1.8816e-03, 1.6313e-03, 9.8329e-04, 1.1990e-03],
        [2.9966e-04, 1.7825e-04, 6.3895e-04, 3.1508e-04, 2.2261e-04, 1.9916e-04,
         3.8914e-04, 3.8148e-04, 3.3069e-04, 1.9928e-04, 2.4301e-04],
        [2.0714e-03, 1.2326e-03, 4.4124e-03, 2.1779e-03, 1.5392e-03, 1.3771e-03,
         2.6893e-03, 2.6364e-03, 2.2857e-03, 1.3779e-03, 1.6801e-03],
        [1.5847e-03, 9.4292e-04, 3.3766e-03, 1.6662e-03, 1.1775e-03, 1.0535e-03,
         2.0576e-03, 2.0171e-03, 1.7487e-03, 1.0541e-03, 1.2853e-03],
        [2.6378e-03, 1.5699e-03, 5.6171e-03, 2.7734e-03, 1.9602e-03, 1.7538e-03,
         3.4243e-03, 3.3570e-03, 2.9106e-03, 1.7549e-03, 2.1396e-03],
        [3.5039e-04, 2.0844e-04, 7.4711e-04, 3.6842e-04, 2.6031e-04, 2.3288e-04,
         4.5503e-04, 4.4606e-04, 3.8667e-04, 2.3302e-04, 2.8415e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(902.9628, grad_fn=<NegBackward>) non event loss:  tensor([105.0803], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0084, grad_fn=<SumBackward0>)
torch.Size([1, 26, 1])
-1
torch.Size([1, 260, 1])
-1
### event lambdas:  tensor([[4.0188e-04, 1.9011e-04, 3.0496e-04, 2.2252e-04, 2.4801e-04, 2.7399e-04,
         2.8456e-04, 7.5134e-04, 2.5239e-04, 5.0789e-04, 4.8116e-04, 2.6857e-04,
         2.4558e-04, 2.9818e-04, 3.7965e-04, 5.4540e-04, 4.9824e-04, 2.9736e-04,
         1.7500e-04, 2.6889e-04, 2.3672e-04, 3.5588e-04, 3.3468e-04, 2.8200e-04,
         1.1779e-04, 2.5407e-04],
        [4.2762e-05, 2.0227e-05, 3.2447e-05, 2.3675e-05, 2.6388e-05, 2.9152e-05,
         3.0276e-05, 7.9958e-05, 2.6853e-05, 5.4044e-05, 5.1199e-05, 2.8575e-05,
         2.6129e-05, 3.1726e-05, 4.0395e-05, 5.8036e-05, 5.3017e-05, 3.1639e-05,
         1.8618e-05, 2.8609e-05, 2.5186e-05, 3.7866e-05, 3.5610e-05, 3.0004e-05,
         1.2532e-05, 2.7032e-05],
        [2.9689e-05, 1.4043e-05, 2.2528e-05, 1.6437e-05, 1.8321e-05, 2.0240e-05,
         2.1020e-05, 5.5514e-05, 1.8644e-05, 3.7522e-05, 3.5547e-05, 1.9839e-05,
         1.8141e-05, 2.2027e-05, 2.8046e-05, 4.0294e-05, 3.6809e-05, 2.1966e-05,
         1.2927e-05, 1.9863e-05, 1.7487e-05, 2.6290e-05, 2.4723e-05, 2.0831e-05,
         8.7007e-06, 1.8768e-05],
        [4.1732e-05, 1.9740e-05, 3.1666e-05, 2.3105e-05, 2.5752e-05, 2.8450e-05,
         2.9547e-05, 7.8033e-05, 2.6206e-05, 5.2743e-05, 4.9967e-05, 2.7887e-05,
         2.5500e-05, 3.0962e-05, 3.9423e-05, 5.6639e-05, 5.1741e-05, 3.0877e-05,
         1.8170e-05, 2.7921e-05, 2.4580e-05, 3.6955e-05, 3.4752e-05, 2.9281e-05,
         1.2230e-05, 2.6381e-05],
        [1.3732e-04, 6.4958e-05, 1.0420e-04, 7.6031e-05, 8.4743e-05, 9.3621e-05,
         9.7231e-05, 2.5677e-04, 8.6237e-05, 1.7355e-04, 1.6442e-04, 9.1767e-05,
         8.3911e-05, 1.0189e-04, 1.2973e-04, 1.8637e-04, 1.7026e-04, 1.0161e-04,
         5.9793e-05, 9.1878e-05, 8.0885e-05, 1.2160e-04, 1.1436e-04, 9.6355e-05,
         4.0246e-05, 8.6812e-05],
        [2.0854e-04, 9.8648e-05, 1.5825e-04, 1.1546e-04, 1.2869e-04, 1.4218e-04,
         1.4766e-04, 3.8992e-04, 1.3096e-04, 2.6356e-04, 2.4969e-04, 1.3936e-04,
         1.2743e-04, 1.5473e-04, 1.9700e-04, 2.8303e-04, 2.5855e-04, 1.5430e-04,
         9.0805e-05, 1.3953e-04, 1.2284e-04, 1.8467e-04, 1.7367e-04, 1.4633e-04,
         6.1120e-05, 1.3184e-04],
        [1.3667e-04, 6.4649e-05, 1.0371e-04, 7.5669e-05, 8.4339e-05, 9.3175e-05,
         9.6768e-05, 2.5554e-04, 8.5826e-05, 1.7273e-04, 1.6364e-04, 9.1330e-05,
         8.3511e-05, 1.0140e-04, 1.2911e-04, 1.8549e-04, 1.6945e-04, 1.0112e-04,
         5.9508e-05, 9.1440e-05, 8.0500e-05, 1.2102e-04, 1.1381e-04, 9.5896e-05,
         4.0055e-05, 8.6399e-05],
        [2.5741e-05, 1.2176e-05, 1.9532e-05, 1.4251e-05, 1.5884e-05, 1.7548e-05,
         1.8225e-05, 4.8132e-05, 1.6164e-05, 3.2532e-05, 3.0820e-05, 1.7201e-05,
         1.5728e-05, 1.9098e-05, 2.4316e-05, 3.4936e-05, 3.1914e-05, 1.9045e-05,
         1.1208e-05, 1.7222e-05, 1.5161e-05, 2.2794e-05, 2.1436e-05, 1.8061e-05,
         7.5437e-06, 1.6272e-05],
        [2.6387e-05, 1.2481e-05, 2.0023e-05, 1.4609e-05, 1.6283e-05, 1.7989e-05,
         1.8683e-05, 4.9341e-05, 1.6570e-05, 3.3349e-05, 3.1594e-05, 1.7633e-05,
         1.6123e-05, 1.9577e-05, 2.4927e-05, 3.5813e-05, 3.2716e-05, 1.9524e-05,
         1.1489e-05, 1.7654e-05, 1.5542e-05, 2.3366e-05, 2.1974e-05, 1.8515e-05,
         7.7331e-06, 1.6681e-05],
        [3.0268e-04, 1.4318e-04, 2.2968e-04, 1.6759e-04, 1.8679e-04, 2.0636e-04,
         2.1431e-04, 5.6590e-04, 1.9008e-04, 3.8252e-04, 3.6239e-04, 2.0227e-04,
         1.8496e-04, 2.2457e-04, 2.8593e-04, 4.1078e-04, 3.7526e-04, 2.2396e-04,
         1.3180e-04, 2.0251e-04, 1.7829e-04, 2.6803e-04, 2.5206e-04, 2.1238e-04,
         8.8712e-05, 1.9135e-04],
        [5.0351e-04, 2.3819e-04, 3.8208e-04, 2.7879e-04, 3.1074e-04, 3.4328e-04,
         3.5652e-04, 9.4130e-04, 3.1621e-04, 6.3631e-04, 6.0283e-04, 3.3649e-04,
         3.0769e-04, 3.7359e-04, 4.7565e-04, 6.8330e-04, 6.2423e-04, 3.7256e-04,
         2.1926e-04, 3.3689e-04, 2.9659e-04, 4.4588e-04, 4.1931e-04, 3.5331e-04,
         1.4758e-04, 3.1832e-04],
        [2.3116e-04, 1.0935e-04, 1.7541e-04, 1.2799e-04, 1.4265e-04, 1.5760e-04,
         1.6367e-04, 4.3220e-04, 1.4517e-04, 2.9214e-04, 2.7677e-04, 1.5447e-04,
         1.4125e-04, 1.7151e-04, 2.1837e-04, 3.1372e-04, 2.8659e-04, 1.7104e-04,
         1.0065e-04, 1.5466e-04, 1.3616e-04, 2.0470e-04, 1.9250e-04, 1.6220e-04,
         6.7749e-05, 1.4613e-04],
        [1.0706e-05, 5.0639e-06, 8.1234e-06, 5.9272e-06, 6.6063e-06, 7.2985e-06,
         7.5799e-06, 2.0018e-05, 6.7228e-06, 1.3530e-05, 1.2818e-05, 7.1539e-06,
         6.5415e-06, 7.9428e-06, 1.0113e-05, 1.4530e-05, 1.3273e-05, 7.9210e-06,
         4.6612e-06, 7.1626e-06, 6.3056e-06, 9.4801e-06, 8.9151e-06, 7.5116e-06,
         3.1374e-06, 6.7677e-06],
        [2.9247e-05, 1.3834e-05, 2.2193e-05, 1.6193e-05, 1.8048e-05, 1.9939e-05,
         2.0708e-05, 5.4689e-05, 1.8366e-05, 3.6964e-05, 3.5018e-05, 1.9544e-05,
         1.7871e-05, 2.1699e-05, 2.7629e-05, 3.9695e-05, 3.6262e-05, 2.1640e-05,
         1.2734e-05, 1.9568e-05, 1.7226e-05, 2.5899e-05, 2.4356e-05, 2.0521e-05,
         8.5713e-06, 1.8489e-05],
        [2.3847e-05, 1.1280e-05, 1.8095e-05, 1.3203e-05, 1.4716e-05, 1.6257e-05,
         1.6884e-05, 4.4591e-05, 1.4975e-05, 3.0139e-05, 2.8552e-05, 1.5935e-05,
         1.4571e-05, 1.7693e-05, 2.2527e-05, 3.2365e-05, 2.9566e-05, 1.7644e-05,
         1.0383e-05, 1.5955e-05, 1.4046e-05, 2.1117e-05, 1.9858e-05, 1.6732e-05,
         6.9886e-06, 1.5075e-05],
        [6.2490e-05, 2.9559e-05, 4.7417e-05, 3.4598e-05, 3.8562e-05, 4.2602e-05,
         4.4245e-05, 1.1685e-04, 3.9242e-05, 7.8977e-05, 7.4820e-05, 4.1758e-05,
         3.8183e-05, 4.6363e-05, 5.9032e-05, 8.4811e-05, 7.7477e-05, 4.6236e-05,
         2.7208e-05, 4.1809e-05, 3.6806e-05, 5.5336e-05, 5.2038e-05, 4.3846e-05,
         1.8314e-05, 3.9504e-05],
        [2.6291e-05, 1.2436e-05, 1.9949e-05, 1.4556e-05, 1.6224e-05, 1.7923e-05,
         1.8614e-05, 4.9160e-05, 1.6510e-05, 3.3227e-05, 3.1478e-05, 1.7568e-05,
         1.6064e-05, 1.9506e-05, 2.4836e-05, 3.5682e-05, 3.2596e-05, 1.9452e-05,
         1.1447e-05, 1.7590e-05, 1.5485e-05, 2.3281e-05, 2.1893e-05, 1.8447e-05,
         7.7048e-06, 1.6620e-05],
        [1.0297e-04, 4.8705e-05, 7.8131e-05, 5.7008e-05, 6.3540e-05, 7.0197e-05,
         7.2904e-05, 1.9253e-04, 6.4660e-05, 1.3013e-04, 1.2328e-04, 6.8807e-05,
         6.2916e-05, 7.6394e-05, 9.7269e-05, 1.3974e-04, 1.2766e-04, 7.6185e-05,
         4.4833e-05, 6.8890e-05, 6.0648e-05, 9.1179e-05, 8.5746e-05, 7.2247e-05,
         3.0176e-05, 6.5092e-05],
        [8.7871e-05, 4.1565e-05, 6.6677e-05, 4.8650e-05, 5.4225e-05, 5.9906e-05,
         6.2215e-05, 1.6430e-04, 5.5181e-05, 1.1105e-04, 1.0521e-04, 5.8719e-05,
         5.3693e-05, 6.5194e-05, 8.3009e-05, 1.1926e-04, 1.0894e-04, 6.5016e-05,
         3.8260e-05, 5.8790e-05, 5.1756e-05, 7.7812e-05, 7.3175e-05, 6.1655e-05,
         2.5752e-05, 5.5549e-05],
        [5.5189e-05, 2.6105e-05, 4.1877e-05, 3.0555e-05, 3.4057e-05, 3.7625e-05,
         3.9075e-05, 1.0319e-04, 3.4657e-05, 6.9750e-05, 6.6078e-05, 3.6879e-05,
         3.3722e-05, 4.0946e-05, 5.2135e-05, 7.4902e-05, 6.8425e-05, 4.0834e-05,
         2.4030e-05, 3.6924e-05, 3.2506e-05, 4.8871e-05, 4.5959e-05, 3.8723e-05,
         1.6174e-05, 3.4888e-05],
        [1.3553e-05, 6.4108e-06, 1.0284e-05, 7.5037e-06, 8.3636e-06, 9.2398e-06,
         9.5961e-06, 2.5343e-05, 8.5110e-06, 1.7129e-05, 1.6228e-05, 9.0568e-06,
         8.2815e-06, 1.0056e-05, 1.2803e-05, 1.8395e-05, 1.6804e-05, 1.0028e-05,
         5.9011e-06, 9.0677e-06, 7.9828e-06, 1.2002e-05, 1.1286e-05, 9.5097e-06,
         3.9719e-06, 8.5678e-06],
        [1.1980e-04, 5.6666e-05, 9.0901e-05, 6.6326e-05, 7.3925e-05, 8.1670e-05,
         8.4819e-05, 2.2399e-04, 7.5229e-05, 1.5140e-04, 1.4343e-04, 8.0053e-05,
         7.3200e-05, 8.8880e-05, 1.1317e-04, 1.6258e-04, 1.4852e-04, 8.8636e-05,
         5.2160e-05, 8.0149e-05, 7.0560e-05, 1.0608e-04, 9.9760e-05, 8.4055e-05,
         3.5109e-05, 7.5730e-05],
        [3.4199e-05, 1.6176e-05, 2.5950e-05, 1.8934e-05, 2.1104e-05, 2.3315e-05,
         2.4214e-05, 6.3947e-05, 2.1476e-05, 4.3222e-05, 4.0947e-05, 2.2853e-05,
         2.0896e-05, 2.5373e-05, 3.2306e-05, 4.6415e-05, 4.2401e-05, 2.5303e-05,
         1.4890e-05, 2.2880e-05, 2.0143e-05, 3.0284e-05, 2.8479e-05, 2.3996e-05,
         1.0022e-05, 2.1619e-05],
        [3.4519e-05, 1.6328e-05, 2.6193e-05, 1.9111e-05, 2.1301e-05, 2.3533e-05,
         2.4440e-05, 6.4546e-05, 2.1677e-05, 4.3626e-05, 4.1330e-05, 2.3067e-05,
         2.1092e-05, 2.5610e-05, 3.2609e-05, 4.6849e-05, 4.2798e-05, 2.5540e-05,
         1.5030e-05, 2.3095e-05, 2.0331e-05, 3.0567e-05, 2.8746e-05, 2.4220e-05,
         1.0116e-05, 2.1821e-05],
        [1.1118e-04, 5.2588e-05, 8.4360e-05, 6.1553e-05, 6.8606e-05, 7.5793e-05,
         7.8716e-05, 2.0788e-04, 6.9815e-05, 1.4051e-04, 1.3311e-04, 7.4292e-05,
         6.7932e-05, 8.2484e-05, 1.0502e-04, 1.5088e-04, 1.3784e-04, 8.2258e-05,
         4.8407e-05, 7.4382e-05, 6.5483e-05, 9.8448e-05, 9.2581e-05, 7.8007e-05,
         3.2582e-05, 7.0281e-05],
        [7.2682e-05, 3.4380e-05, 5.5151e-05, 4.0241e-05, 4.4852e-05, 4.9551e-05,
         5.1461e-05, 1.3590e-04, 4.5642e-05, 9.1858e-05, 8.7023e-05, 4.8569e-05,
         4.4411e-05, 5.3925e-05, 6.8660e-05, 9.8644e-05, 9.0114e-05, 5.3777e-05,
         3.1646e-05, 4.8628e-05, 4.2810e-05, 6.4362e-05, 6.0526e-05, 5.0998e-05,

  2%|███▍                                                                                                                                           | 12/496 [00:02<01:47,  4.50it/s]
##### event loss: tensor(6701.1328, grad_fn=<NegBackward>) non event loss:  tensor([96.2346], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0606, grad_fn=<SumBackward0>)
torch.Size([1, 6, 1])
-1
torch.Size([1, 60, 1])
-1
### event lambdas:  tensor([[1.0325e-05, 2.3269e-05, 8.0127e-06, 8.7776e-06, 1.7607e-05, 2.5420e-05],
        [8.9600e-06, 2.0193e-05, 6.9534e-06, 7.6172e-06, 1.5279e-05, 2.2059e-05],
        [1.8868e-06, 4.2524e-06, 1.4643e-06, 1.6041e-06, 3.2176e-06, 4.6454e-06],
        [3.6870e-06, 8.3096e-06, 2.8613e-06, 3.1345e-06, 6.2875e-06, 9.0776e-06],
        [4.2054e-06, 9.4779e-06, 3.2636e-06, 3.5752e-06, 7.1715e-06, 1.0354e-05],
        [2.8613e-06, 6.4485e-06, 2.2205e-06, 2.4325e-06, 4.8793e-06, 7.0445e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(432.6418, grad_fn=<NegBackward>) non event loss:  tensor([4.1245], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0304, grad_fn=<SumBackward0>)
torch.Size([1, 4, 1])
-1
torch.Size([1, 40, 1])
-1
### event lambdas:  tensor([[2.5720e-06, 2.0279e-06, 3.6695e-06, 1.5276e-06],
        [2.1241e-06, 1.6748e-06, 3.0305e-06, 1.2616e-06],
        [2.2779e-06, 1.7960e-06, 3.2499e-06, 1.3529e-06],
        [1.6946e-06, 1.3361e-06, 2.4177e-06, 1.0065e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(210.4664, grad_fn=<NegBackward>) non event loss:  tensor([0.2201], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0182, grad_fn=<SumBackward0>)
torch.Size([1, 4, 1])
-1
torch.Size([1, 40, 1])
-1
### event lambdas:  tensor([[1.6210e-05, 7.0775e-06, 1.3713e-05, 7.9776e-06],
        [2.7591e-05, 1.2046e-05, 2.3340e-05, 1.3578e-05],
        [2.0577e-06, 8.9839e-07, 1.7407e-06, 1.0126e-06],
        [9.0890e-06, 3.9683e-06, 7.6888e-06, 4.4730e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(191.7413, grad_fn=<NegBackward>) non event loss:  tensor([0.4423], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0127, grad_fn=<SumBackward0>)
torch.Size([1, 7, 1])
-1
torch.Size([1, 70, 1])
-1
### event lambdas:  tensor([[2.5907e-06, 2.5907e-06, 3.1942e-06, 2.4974e-06, 3.4440e-06, 1.6405e-06,
         3.8357e-06],
        [1.1604e-05, 1.1604e-05, 1.4307e-05, 1.1186e-05, 1.5426e-05, 7.3479e-06,
         1.7180e-05],
        [1.1010e-05, 1.1010e-05, 1.3575e-05, 1.0614e-05, 1.4637e-05, 6.9719e-06,
         1.6301e-05],
        [1.9141e-07, 1.9141e-07, 2.3599e-07, 1.8452e-07, 2.5445e-07, 1.2120e-07,
         2.8339e-07],
        [3.2204e-06, 3.2204e-06, 3.9705e-06, 3.1045e-06, 4.2811e-06, 2.0392e-06,
         4.7680e-06],
        [5.3122e-07, 5.3122e-07, 6.5496e-07, 5.1210e-07, 7.0619e-07, 3.3638e-07,
         7.8650e-07],
        [5.4935e-07, 5.4935e-07, 6.7731e-07, 5.2957e-07, 7.3029e-07, 3.4786e-07,
         8.1334e-07]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(645.6028, grad_fn=<NegBackward>) non event loss:  tensor([0.4519], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0456, grad_fn=<SumBackward0>)
torch.Size([1, 4, 1])
-1
torch.Size([1, 40, 1])
-1
### event lambdas:  tensor([[2.8374e-08, 2.4109e-08, 3.5724e-08, 5.3266e-08],
        [7.9139e-08, 6.7242e-08, 9.9639e-08, 1.4857e-07],
        [2.2501e-08, 1.9118e-08, 2.8329e-08, 4.2240e-08],
        [4.5614e-07, 3.8756e-07, 5.7429e-07, 8.5629e-07]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(260.9711, grad_fn=<NegBackward>) non event loss:  tensor([0.1240], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0376, grad_fn=<SumBackward0>)
torch.Size([1, 8, 1])
-1
torch.Size([1, 80, 1])
-1
### event lambdas:  tensor([[2.6933e-07, 3.6054e-07, 7.3308e-07, 7.8600e-07, 6.7130e-07, 3.6808e-07,
         7.8355e-07, 1.1074e-06],
        [1.1911e-07, 1.5945e-07, 3.2420e-07, 3.4760e-07, 2.9688e-07, 1.6278e-07,
         3.4652e-07, 4.8973e-07],
        [5.0346e-08, 6.7395e-08, 1.3703e-07, 1.4693e-07, 1.2548e-07, 6.8805e-08,
         1.4647e-07, 2.0700e-07],
        [6.9084e-08, 9.2479e-08, 1.8804e-07, 2.0161e-07, 1.7219e-07, 9.4413e-08,
         2.0098e-07, 2.8404e-07],
        [5.8375e-09, 7.8143e-09, 1.5889e-08, 1.7036e-08, 1.4550e-08, 7.9777e-09,
         1.6983e-08, 2.4001e-08],
        [5.8139e-08, 7.7828e-08, 1.5825e-07, 1.6967e-07, 1.4491e-07, 7.9456e-08,
         1.6914e-07, 2.3904e-07],
        [1.0837e-07, 1.4507e-07, 2.9498e-07, 3.1627e-07, 2.7012e-07, 1.4811e-07,
         3.1528e-07, 4.4558e-07],
        [5.6708e-08, 7.5911e-08, 1.5435e-07, 1.6549e-07, 1.4134e-07, 7.7499e-08,
         1.6498e-07, 2.3316e-07]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1013.0717, grad_fn=<NegBackward>) non event loss:  tensor([0.1275], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1015, grad_fn=<SumBackward0>)
torch.Size([1, 99, 1])
-1
torch.Size([1, 990, 1])
-1
### event lambdas:  tensor([[1.5297e-06, 2.5118e-06, 3.7755e-06,  ..., 5.3601e-06, 9.8136e-07,
         1.1022e-06],
        [1.8897e-06, 3.1027e-06, 4.6638e-06,  ..., 6.6212e-06, 1.2123e-06,
         1.3615e-06],
        [5.2012e-07, 8.5400e-07, 1.2837e-06,  ..., 1.8225e-06, 3.3367e-07,
         3.7475e-07],
        ...,
        [4.8028e-08, 7.8860e-08, 1.1854e-07,  ..., 1.6829e-07, 3.0811e-08,
         3.4605e-08],
        [4.9072e-08, 8.0574e-08, 1.2111e-07,  ..., 1.7195e-07, 3.1481e-08,
         3.5357e-08],
        [5.4119e-08, 8.8860e-08, 1.3357e-07,  ..., 1.8963e-07, 3.4718e-08,
         3.8993e-08]], grad_fn=<SoftplusBackward>)
  4%|█████▊                                                                                                                                         | 20/496 [00:05<01:59,  3.97it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 107, in train
    loss, pos_timing_loss, neg_timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 195, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 364, in forward
    z_vt_i = self.gnn_choice(features_i.squeeze(), edges_i.squeeze()) # (N_i_1, embedding_z)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 467, in forward
    x = self.convs[i](x, edge_index)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 507, in forward
    out = F.normalize(out, p=2)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 4270, in normalize
    denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 401, in norm
    return torch.norm(self, p, dim, keepdim, dtype=dtype)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/functional.py", line 1420, in norm
    return _VF.norm(input, p, _dim, keepdim=keepdim)  # type: ignore
KeyboardInterrupt
torch.Size([1, 6, 1])
-1
torch.Size([1, 60, 1])
-1
### event lambdas:  tensor([[8.3015e-07, 2.4699e-07, 2.6766e-07, 5.6539e-07, 3.7719e-07, 4.6279e-07],
        [6.9909e-07, 2.0800e-07, 2.2540e-07, 4.7613e-07, 3.1764e-07, 3.8973e-07],
        [2.9271e-06, 8.7091e-07, 9.4378e-07, 1.9936e-06, 1.3300e-06, 1.6318e-06],
        [3.5119e-07, 1.0449e-07, 1.1323e-07, 2.3919e-07, 1.5957e-07, 1.9578e-07],
        [5.2897e-07, 1.5738e-07, 1.7055e-07, 3.6026e-07, 2.4034e-07, 2.9489e-07],
        [1.0225e-08, 3.0423e-09, 3.2968e-09, 6.9640e-09, 4.6459e-09, 5.7003e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(556.3845, grad_fn=<NegBackward>) non event loss:  tensor([0.0439], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0468, grad_fn=<SumBackward0>)
torch.Size([1, 6, 1])
-1
torch.Size([1, 60, 1])
-1
### event lambdas:  tensor([[3.4195e-08, 8.4755e-08, 1.0700e-07, 1.5625e-07, 1.0560e-07, 3.4508e-08],
        [1.2951e-08, 3.2099e-08, 4.0524e-08, 5.9178e-08, 3.9993e-08, 1.3069e-08],
        [1.2683e-08, 3.1435e-08, 3.9685e-08, 5.7953e-08, 3.9165e-08, 1.2799e-08],
        [2.3958e-08, 5.9382e-08, 7.4967e-08, 1.0948e-07, 7.3985e-08, 2.4177e-08],
        [1.1215e-08, 2.7798e-08, 3.5093e-08, 5.1248e-08, 3.4633e-08, 1.1318e-08],
        [1.0479e-09, 2.5974e-09, 3.2790e-09, 4.7885e-09, 3.2361e-09, 1.0575e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(632.1676, grad_fn=<NegBackward>) non event loss:  tensor([0.0096], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0680, grad_fn=<SumBackward0>)
torch.Size([1, 6, 1])
-1
torch.Size([1, 60, 1])
-1
### event lambdas:  tensor([[2.4213e-07, 1.9141e-07, 1.0576e-07, 1.1773e-07, 3.2795e-07, 2.0657e-07],
        [1.5241e-07, 1.2049e-07, 6.6576e-08, 7.4106e-08, 2.0644e-07, 1.3003e-07],
        [1.9769e-08, 1.5628e-08, 8.6351e-09, 9.6118e-09, 2.6775e-08, 1.6865e-08],
        [3.5007e-08, 2.7674e-08, 1.5291e-08, 1.7021e-08, 4.7414e-08, 2.9865e-08],
        [2.3198e-09, 1.8338e-09, 1.0133e-09, 1.1279e-09, 3.1420e-09, 1.9790e-09],
        [1.8924e-09, 1.4960e-09, 8.2663e-10, 9.2013e-10, 2.5632e-09, 1.6145e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(644.6667, grad_fn=<NegBackward>) non event loss:  tensor([0.0328], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0675, grad_fn=<SumBackward0>)
torch.Size([1, 5, 1])
-1
torch.Size([1, 50, 1])
-1
### event lambdas:  tensor([[6.3251e-08, 2.0607e-07, 7.8299e-08, 8.5041e-08, 1.4377e-07],
        [2.2338e-08, 7.2774e-08, 2.7652e-08, 3.0033e-08, 5.0775e-08],
        [2.8607e-08, 9.3200e-08, 3.5413e-08, 3.8463e-08, 6.5026e-08],
        [2.3407e-08, 7.6259e-08, 2.8976e-08, 3.1472e-08, 5.3207e-08],
        [2.1337e-08, 6.9515e-08, 2.6414e-08, 2.8688e-08, 4.8501e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(421.4205, grad_fn=<NegBackward>) non event loss:  tensor([0.0059], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0682, grad_fn=<SumBackward0>)
torch.Size([1, 5, 1])
-1
torch.Size([1, 50, 1])
-1
### event lambdas:  tensor([[2.4541e-08, 3.5352e-08, 3.4925e-08, 3.1197e-08, 2.0031e-08],
        [5.8339e-08, 8.4039e-08, 8.3024e-08, 7.4161e-08, 4.7619e-08],
        [7.1221e-09, 1.0260e-08, 1.0136e-08, 9.0538e-09, 5.8134e-09],
        [3.3165e-09, 4.7775e-09, 4.7198e-09, 4.2160e-09, 2.7071e-09],
        [1.4878e-08, 2.1432e-08, 2.1173e-08, 1.8913e-08, 1.2144e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(448.6646, grad_fn=<NegBackward>) non event loss:  tensor([0.0014], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0597, grad_fn=<SumBackward0>)
torch.Size([1, 7, 1])
-1
torch.Size([1, 70, 1])
-1
### event lambdas:  tensor([[6.0835e-09, 4.3500e-09, 2.7669e-09, 3.8390e-09, 8.2376e-09, 5.0428e-09,
         2.4151e-09],
        [1.8813e-07, 1.3452e-07, 8.5565e-08, 1.1872e-07, 2.5474e-07, 1.5595e-07,
         7.4685e-08],
        [2.7487e-08, 1.9655e-08, 1.2502e-08, 1.7346e-08, 3.7220e-08, 2.2785e-08,
         1.0912e-08],
        [2.5396e-08, 1.8160e-08, 1.1551e-08, 1.6026e-08, 3.4389e-08, 2.1052e-08,
         1.0082e-08],
        [7.1924e-08, 5.1429e-08, 3.2713e-08, 4.5387e-08, 9.7392e-08, 5.9621e-08,
         2.8553e-08],
        [2.3437e-09, 1.6758e-09, 1.0660e-09, 1.4790e-09, 3.1735e-09, 1.9428e-09,
         9.3042e-10],
        [2.3786e-09, 1.7008e-09, 1.0819e-09, 1.5010e-09, 3.2209e-09, 1.9717e-09,
         9.4430e-10]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(895.0894, grad_fn=<NegBackward>) non event loss:  tensor([0.0090], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0906, grad_fn=<SumBackward0>)
torch.Size([1, 10, 1])
-1
torch.Size([1, 100, 1])
-1
### event lambdas:  tensor([[1.8184e-08, 8.3310e-09, 1.1554e-08, 1.2539e-08, 1.8668e-08, 1.0640e-08,
         9.4195e-09, 1.3830e-08, 1.7997e-08, 1.4574e-08],
        [1.7957e-08, 8.2266e-09, 1.1409e-08, 1.2382e-08, 1.8434e-08, 1.0507e-08,
         9.3015e-09, 1.3657e-08, 1.7771e-08, 1.4392e-08],
        [1.5164e-08, 6.9474e-09, 9.6352e-09, 1.0456e-08, 1.5568e-08, 8.8728e-09,
         7.8551e-09, 1.1533e-08, 1.5008e-08, 1.2154e-08],
        [1.4168e-09, 6.4908e-10, 9.0020e-10, 9.7691e-10, 1.4545e-09, 8.2897e-10,
         7.3389e-10, 1.0775e-09, 1.4022e-09, 1.1355e-09],
        [1.1023e-09, 5.0502e-10, 7.0041e-10, 7.6009e-10, 1.1317e-09, 6.4499e-10,
         5.7101e-10, 8.3837e-10, 1.0910e-09, 8.8349e-10],
        [1.7400e-08, 7.9717e-09, 1.1056e-08, 1.1998e-08, 1.7863e-08, 1.0181e-08,
         9.0133e-09, 1.3234e-08, 1.7221e-08, 1.3946e-08],
        [2.0267e-08, 9.2849e-09, 1.2877e-08, 1.3974e-08, 2.0806e-08, 1.1858e-08,
         1.0498e-08, 1.5414e-08, 2.0057e-08, 1.6243e-08],
        [4.3535e-09, 1.9945e-09, 2.7662e-09, 3.0019e-09, 4.4693e-09, 2.5473e-09,
         2.2551e-09, 3.3110e-09, 4.3086e-09, 3.4892e-09],
        [3.4553e-09, 1.5830e-09, 2.1955e-09, 2.3825e-09, 3.5472e-09, 2.0217e-09,
         1.7898e-09, 2.6279e-09, 3.4197e-09, 2.7693e-09],
        [1.9042e-08, 8.7240e-09, 1.2099e-08, 1.3130e-08, 1.9549e-08, 1.1142e-08,
         9.8638e-09, 1.4482e-08, 1.8846e-08, 1.5262e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(1900.3510, grad_fn=<NegBackward>) non event loss:  tensor([0.0151], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1556, grad_fn=<SumBackward0>)
torch.Size([1, 13, 1])
-1
torch.Size([1, 130, 1])
-1
### event lambdas:  tensor([[5.3479e-08, 4.8242e-08, 5.0009e-08, 3.2236e-08, 4.4027e-08, 7.6258e-08,
         1.5164e-08, 1.1843e-07, 1.1663e-07, 7.8662e-08, 4.8762e-08, 4.4580e-08,
         4.4938e-08],
        [3.3215e-08, 2.9962e-08, 3.1059e-08, 2.0021e-08, 2.7344e-08, 4.7362e-08,
         9.4180e-09, 7.3556e-08, 7.2434e-08, 4.8855e-08, 3.0285e-08, 2.7688e-08,
         2.7910e-08],
        [2.2546e-08, 2.0338e-08, 2.1083e-08, 1.3590e-08, 1.8561e-08, 3.2149e-08,
         6.3929e-09, 4.9930e-08, 4.9168e-08, 3.3163e-08, 2.0557e-08, 1.8794e-08,
         1.8945e-08],
        [1.5014e-08, 1.3544e-08, 1.4040e-08, 9.0503e-09, 1.2361e-08, 2.1409e-08,
         4.2572e-09, 3.3250e-08, 3.2742e-08, 2.2084e-08, 1.3690e-08, 1.2516e-08,
         1.2616e-08],
        [7.0801e-09, 6.3868e-09, 6.6207e-09, 4.2678e-09, 5.8288e-09, 1.0096e-08,
         2.0076e-09, 1.5679e-08, 1.5440e-08, 1.0414e-08, 6.4556e-09, 5.9019e-09,
         5.9493e-09],
        [1.0828e-08, 9.7677e-09, 1.0125e-08, 6.5269e-09, 8.9142e-09, 1.5440e-08,
         3.0703e-09, 2.3979e-08, 2.3613e-08, 1.5927e-08, 9.8728e-09, 9.0261e-09,
         9.0986e-09],
        [9.8516e-09, 8.8870e-09, 9.2124e-09, 5.9385e-09, 8.1105e-09, 1.4048e-08,
         2.7934e-09, 2.1817e-08, 2.1484e-08, 1.4491e-08, 8.9827e-09, 8.2123e-09,
         8.2782e-09],
        [8.9721e-09, 8.0936e-09, 8.3899e-09, 5.4083e-09, 7.3864e-09, 1.2794e-08,
         2.5440e-09, 1.9869e-08, 1.9566e-08, 1.3197e-08, 8.1808e-09, 7.4791e-09,
         7.5392e-09],
        [9.4564e-09, 8.5305e-09, 8.8428e-09, 5.7002e-09, 7.7851e-09, 1.3484e-08,
         2.6814e-09, 2.0942e-08, 2.0622e-08, 1.3909e-08, 8.6223e-09, 7.8828e-09,
         7.9461e-09],
        [5.8545e-10, 5.2812e-10, 5.4746e-10, 3.5290e-10, 4.8198e-10, 8.3481e-10,
         1.6600e-10, 1.2965e-09, 1.2767e-09, 8.6113e-10, 5.3381e-10, 4.8803e-10,
         4.9195e-10],
        [9.5606e-09, 8.6245e-09, 8.9402e-09, 5.7630e-09, 7.8709e-09, 1.3633e-08,
         2.7109e-09, 2.1173e-08, 2.0850e-08, 1.4063e-08, 8.7173e-09, 7.9697e-09,
         8.0337e-09],
        [1.8626e-09, 1.6802e-09, 1.7417e-09, 1.1228e-09, 1.5334e-09, 2.6560e-09,
         5.2814e-10, 4.1249e-09, 4.0619e-09, 2.7397e-09, 1.6983e-09, 1.5527e-09,
         1.5651e-09],
        [1.2317e-08, 1.1111e-08, 1.1517e-08, 7.4243e-09, 1.0140e-08, 1.7563e-08,
         3.4924e-09, 2.7276e-08, 2.6860e-08, 1.8116e-08, 1.1230e-08, 1.0267e-08,
         1.0349e-08]], grad_fn=<SoftplusBackward>)