
CUDA availability: True
-1
-1
### event lambdas:  tensor([[0.1885, 0.2458, 0.3468, 0.1060, 0.1388],
        [0.2065, 0.2685, 0.3773, 0.1165, 0.1524],
        [0.2864, 0.3683, 0.5084, 0.1644, 0.2135],
        [0.7339, 0.8980, 1.1520, 0.4597, 0.5752],
        [0.7436, 0.9090, 1.1647, 0.4666, 0.5834]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(25.6698, grad_fn=<NegBackward>) non event loss:  tensor([207745.5107], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(2.7876, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0181, 0.0239, 0.0165, 0.0600, 0.0272],
        [0.0336, 0.0443, 0.0306, 0.1095, 0.0503],
        [0.0328, 0.0431, 0.0298, 0.1068, 0.0490],
        [0.0504, 0.0661, 0.0459, 0.1611, 0.0750],
        [0.0480, 0.0630, 0.0437, 0.1539, 0.0715]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(75.4365, grad_fn=<NegBackward>) non event loss:  tensor([35539.5460], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.8225, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[0.0040, 0.0057, 0.0038, 0.0045, 0.0099, 0.0063, 0.0047, 0.0076, 0.0050,
         0.0041],
        [0.0131, 0.0188, 0.0124, 0.0147, 0.0322, 0.0208, 0.0154, 0.0248, 0.0164,
         0.0136],
        [0.0035, 0.0051, 0.0033, 0.0040, 0.0087, 0.0056, 0.0042, 0.0067, 0.0044,
         0.0037],
        [0.0060, 0.0087, 0.0057, 0.0068, 0.0149, 0.0096, 0.0071, 0.0114, 0.0075,
         0.0063],
        [0.0039, 0.0055, 0.0036, 0.0043, 0.0096, 0.0061, 0.0045, 0.0073, 0.0048,
         0.0040],
        [0.0035, 0.0050, 0.0033, 0.0039, 0.0085, 0.0055, 0.0041, 0.0066, 0.0043,
         0.0036],
        [0.0039, 0.0055, 0.0036, 0.0043, 0.0095, 0.0061, 0.0045, 0.0073, 0.0048,
         0.0040],
        [0.0148, 0.0212, 0.0139, 0.0165, 0.0363, 0.0234, 0.0173, 0.0279, 0.0184,
         0.0153],
        [0.0042, 0.0061, 0.0040, 0.0047, 0.0105, 0.0067, 0.0050, 0.0080, 0.0053,
         0.0044],
        [0.0166, 0.0237, 0.0156, 0.0186, 0.0407, 0.0263, 0.0195, 0.0313, 0.0207,
         0.0172]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(483.3409, grad_fn=<NegBackward>) non event loss:  tensor([11543.3873], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.3062, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[4.2089e-04, 3.8838e-04, 5.2435e-04, 4.1102e-04, 3.3743e-04, 3.2377e-04,
         2.5652e-04, 3.3790e-04, 3.2416e-04, 7.3497e-04, 2.3611e-04, 3.9439e-04,
         6.0676e-04, 6.7978e-04, 4.1574e-04, 2.0141e-04, 3.4749e-04, 3.1751e-04,
         4.4093e-04, 5.7370e-04, 4.3124e-04, 1.6714e-04, 4.3098e-04],
        [2.5814e-04, 2.3820e-04, 3.2160e-04, 2.5209e-04, 2.0695e-04, 1.9857e-04,
         1.5733e-04, 2.0724e-04, 1.9881e-04, 4.5080e-04, 1.4481e-04, 2.4189e-04,
         3.7215e-04, 4.1695e-04, 2.5498e-04, 1.2352e-04, 2.1312e-04, 1.9473e-04,
         2.7044e-04, 3.5188e-04, 2.6449e-04, 1.0251e-04, 2.6433e-04],
        [3.9628e-04, 3.6568e-04, 4.9369e-04, 3.8699e-04, 3.1770e-04, 3.0484e-04,
         2.4152e-04, 3.1814e-04, 3.0521e-04, 6.9200e-04, 2.2231e-04, 3.7133e-04,
         5.7129e-04, 6.4004e-04, 3.9143e-04, 1.8963e-04, 3.2717e-04, 2.9894e-04,
         4.1515e-04, 5.4016e-04, 4.0602e-04, 1.5737e-04, 4.0578e-04],
        [5.3180e-04, 4.9073e-04, 6.6251e-04, 5.1932e-04, 4.2635e-04, 4.0909e-04,
         3.2413e-04, 4.2695e-04, 4.0959e-04, 9.2860e-04, 2.9834e-04, 4.9832e-04,
         7.6663e-04, 8.5888e-04, 5.2529e-04, 2.5449e-04, 4.3906e-04, 4.0118e-04,
         5.5712e-04, 7.2487e-04, 5.4487e-04, 2.1119e-04, 5.4455e-04],
        [5.9373e-04, 5.4788e-04, 7.3965e-04, 5.7980e-04, 4.7600e-04, 4.5674e-04,
         3.6188e-04, 4.7667e-04, 4.5729e-04, 1.0367e-03, 3.3308e-04, 5.5635e-04,
         8.5589e-04, 9.5888e-04, 5.8646e-04, 2.8413e-04, 4.9019e-04, 4.4790e-04,
         6.2200e-04, 8.0927e-04, 6.0833e-04, 2.3579e-04, 6.0797e-04],
        [6.8338e-04, 6.3060e-04, 8.5133e-04, 6.6735e-04, 5.4788e-04, 5.2570e-04,
         4.1653e-04, 5.4865e-04, 5.2634e-04, 1.1932e-03, 3.8338e-04, 6.4035e-04,
         9.8511e-04, 1.1036e-03, 6.7501e-04, 3.2704e-04, 5.6421e-04, 5.1554e-04,
         7.1592e-04, 9.3145e-04, 7.0018e-04, 2.7140e-04, 6.9976e-04],
        [2.5068e-04, 2.3131e-04, 3.1230e-04, 2.4479e-04, 2.0096e-04, 1.9283e-04,
         1.5278e-04, 2.0125e-04, 1.9306e-04, 4.3776e-04, 1.4062e-04, 2.3489e-04,
         3.6139e-04, 4.0489e-04, 2.4761e-04, 1.1995e-04, 2.0696e-04, 1.8910e-04,
         2.6261e-04, 3.4170e-04, 2.5684e-04, 9.9542e-05, 2.5669e-04],
        [5.2292e-04, 4.8253e-04, 6.5144e-04, 5.1065e-04, 4.1922e-04, 4.0226e-04,
         3.1871e-04, 4.1981e-04, 4.0274e-04, 9.1309e-04, 2.9335e-04, 4.8999e-04,
         7.5382e-04, 8.4454e-04, 5.1651e-04, 2.5024e-04, 4.3172e-04, 3.9448e-04,
         5.4782e-04, 7.1276e-04, 5.3577e-04, 2.0767e-04, 5.3545e-04],
        [3.5576e-04, 3.2829e-04, 4.4322e-04, 3.4742e-04, 2.8521e-04, 2.7367e-04,
         2.1683e-04, 2.8562e-04, 2.7400e-04, 6.2126e-04, 1.9957e-04, 3.3336e-04,
         5.1288e-04, 5.7461e-04, 3.5141e-04, 1.7024e-04, 2.9372e-04, 2.6838e-04,
         3.7271e-04, 4.8494e-04, 3.6451e-04, 1.4128e-04, 3.6430e-04],
        [2.7012e-04, 2.4925e-04, 3.3652e-04, 2.6378e-04, 2.1655e-04, 2.0778e-04,
         1.6462e-04, 2.1685e-04, 2.0803e-04, 4.7171e-04, 1.5152e-04, 2.5311e-04,
         3.8941e-04, 4.3628e-04, 2.6681e-04, 1.2925e-04, 2.2300e-04, 2.0376e-04,
         2.8298e-04, 3.6820e-04, 2.7676e-04, 1.0726e-04, 2.7659e-04],
        [2.9434e-04, 2.7160e-04, 3.6669e-04, 2.8743e-04, 2.3597e-04, 2.2641e-04,
         1.7939e-04, 2.3630e-04, 2.2669e-04, 5.1400e-04, 1.6511e-04, 2.7580e-04,
         4.2433e-04, 4.7540e-04, 2.9073e-04, 1.4084e-04, 2.4300e-04, 2.2204e-04,
         3.0835e-04, 4.0121e-04, 3.0157e-04, 1.1688e-04, 3.0139e-04],
        [5.6711e-04, 5.2331e-04, 7.0649e-04, 5.5381e-04, 4.5466e-04, 4.3626e-04,
         3.4565e-04, 4.5530e-04, 4.3678e-04, 9.9024e-04, 3.1815e-04, 5.3140e-04,
         8.1752e-04, 9.1590e-04, 5.6017e-04, 2.7139e-04, 4.6821e-04, 4.2782e-04,
         5.9411e-04, 7.7299e-04, 5.8105e-04, 2.2522e-04, 5.8071e-04],
        [3.9439e-04, 3.6393e-04, 4.9134e-04, 3.8514e-04, 3.1618e-04, 3.0338e-04,
         2.4037e-04, 3.1663e-04, 3.0375e-04, 6.8870e-04, 2.2125e-04, 3.6956e-04,
         5.6856e-04, 6.3699e-04, 3.8956e-04, 1.8873e-04, 3.2561e-04, 2.9752e-04,
         4.1317e-04, 5.3759e-04, 4.0409e-04, 1.5662e-04, 4.0385e-04],
        [4.8019e-04, 4.4311e-04, 5.9822e-04, 4.6893e-04, 3.8497e-04, 3.6939e-04,
         2.9267e-04, 3.8551e-04, 3.6984e-04, 8.3850e-04, 2.6938e-04, 4.4996e-04,
         6.9224e-04, 7.7555e-04, 4.7431e-04, 2.2979e-04, 3.9645e-04, 3.6225e-04,
         5.0306e-04, 6.5453e-04, 4.9200e-04, 1.9070e-04, 4.9171e-04],
        [5.7501e-04, 5.3060e-04, 7.1633e-04, 5.6152e-04, 4.6099e-04, 4.4233e-04,
         3.5047e-04, 4.6164e-04, 4.4287e-04, 1.0040e-03, 3.2258e-04, 5.3880e-04,
         8.2891e-04, 9.2865e-04, 5.6797e-04, 2.7517e-04, 4.7474e-04, 4.3378e-04,
         6.0239e-04, 7.8376e-04, 5.8914e-04, 2.2836e-04, 5.8880e-04],
        [7.1366e-04, 6.5855e-04, 8.8905e-04, 6.9692e-04, 5.7216e-04, 5.4900e-04,
         4.3499e-04, 5.7296e-04, 5.4966e-04, 1.2461e-03, 4.0038e-04, 6.6873e-04,
         1.0288e-03, 1.1525e-03, 7.0492e-04, 3.4153e-04, 5.8922e-04, 5.3838e-04,
         7.4764e-04, 9.7272e-04, 7.3120e-04, 2.8343e-04, 7.3077e-04],
        [2.3454e-04, 2.1642e-04, 2.9220e-04, 2.2904e-04, 1.8803e-04, 1.8042e-04,
         1.4294e-04, 1.8829e-04, 1.8063e-04, 4.0959e-04, 1.3157e-04, 2.1977e-04,
         3.3813e-04, 3.7883e-04, 2.3167e-04, 1.1223e-04, 1.9363e-04, 1.7693e-04,
         2.4571e-04, 3.1970e-04, 2.4031e-04, 9.3134e-05, 2.4016e-04],
        [1.8313e-04, 1.6899e-04, 2.2816e-04, 1.7884e-04, 1.4681e-04, 1.4087e-04,
         1.1161e-04, 1.4702e-04, 1.4104e-04, 3.1982e-04, 1.0273e-04, 1.7160e-04,
         2.6402e-04, 2.9580e-04, 1.8089e-04, 8.7629e-05, 1.5119e-04, 1.3815e-04,
         1.9186e-04, 2.4963e-04, 1.8764e-04, 7.2720e-05, 1.8753e-04],
        [1.3193e-04, 1.2174e-04, 1.6436e-04, 1.2883e-04, 1.0576e-04, 1.0148e-04,
         8.0402e-05, 1.0591e-04, 1.0160e-04, 2.3040e-04, 7.4004e-05, 1.2362e-04,
         1.9020e-04, 2.1309e-04, 1.3031e-04, 6.3126e-05, 1.0892e-04, 9.9518e-05,
         1.3821e-04, 1.7983e-04, 1.3517e-04, 5.2386e-05, 1.3509e-04],
        [3.2802e-04, 3.0269e-04, 4.0866e-04, 3.2033e-04, 2.6297e-04, 2.5233e-04,
         1.9992e-04, 2.6334e-04, 2.5263e-04, 5.7282e-04, 1.8401e-04, 3.0737e-04,
         4.7289e-04, 5.2980e-04, 3.2401e-04, 1.5696e-04, 2.7081e-04, 2.4745e-04,
         3.4364e-04, 4.4712e-04, 3.3609e-04, 1.3026e-04, 3.3589e-04],
        [5.3807e-04, 4.9651e-04, 6.7032e-04, 5.2545e-04, 4.3137e-04, 4.1391e-04,
         3.2795e-04, 4.3198e-04, 4.1441e-04, 9.3955e-04, 3.0185e-04, 5.0419e-04,
         7.7566e-04, 8.6900e-04, 5.3148e-04, 2.5749e-04, 4.4424e-04, 4.0591e-04,
         5.6369e-04, 7.3341e-04, 5.5130e-04, 2.1368e-04, 5.5097e-04],
        [1.2305e-04, 1.1355e-04, 1.5331e-04, 1.2017e-04, 9.8649e-05, 9.4656e-05,
         7.4994e-05, 9.8788e-05, 9.4770e-05, 2.1490e-04, 6.9026e-05, 1.1530e-04,
         1.7741e-04, 1.9876e-04, 1.2155e-04, 5.8880e-05, 1.0159e-04, 9.2825e-05,
         1.2891e-04, 1.6774e-04, 1.2608e-04, 4.8862e-05, 1.2600e-04],
        [4.7353e-04, 4.3696e-04, 5.8992e-04, 4.6242e-04, 3.7963e-04, 3.6426e-04,
         2.8861e-04, 3.8016e-04, 3.6470e-04, 8.2687e-04, 2.6564e-04, 4.4371e-04,
         6.8263e-04, 7.6478e-04, 4.6773e-04, 2.2660e-04, 3.9095e-04, 3.5722e-04,
         4.9608e-04, 6.4545e-04, 4.8517e-04, 1.8805e-04, 4.8488e-04]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(4240.1182, grad_fn=<NegBackward>) non event loss:  tensor([756.5902], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1469, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.4152e-05, 3.6404e-05, 4.3273e-05, 4.0737e-05, 4.3199e-05, 6.4080e-05,
         6.9966e-05, 3.7364e-05, 2.5286e-05, 7.0479e-05, 5.3778e-05],
        [4.4879e-05, 4.7839e-05, 5.6865e-05, 5.3533e-05, 5.6768e-05, 8.4207e-05,
         9.1942e-05, 4.9101e-05, 3.3228e-05, 9.2617e-05, 7.0669e-05],
        [4.3880e-05, 4.6774e-05, 5.5599e-05, 5.2341e-05, 5.5505e-05, 8.2333e-05,
         8.9896e-05, 4.8008e-05, 3.2488e-05, 9.0555e-05, 6.9096e-05],
        [9.3914e-05, 1.0011e-04, 1.1900e-04, 1.1202e-04, 1.1879e-04, 1.7621e-04,
         1.9239e-04, 1.0275e-04, 6.9534e-05, 1.9381e-04, 1.4788e-04],
        [2.7678e-05, 2.9504e-05, 3.5071e-05, 3.3015e-05, 3.5011e-05, 5.1934e-05,
         5.6704e-05, 3.0282e-05, 2.0493e-05, 5.7120e-05, 4.3584e-05],
        [2.1379e-05, 2.2789e-05, 2.7088e-05, 2.5501e-05, 2.7042e-05, 4.0113e-05,
         4.3798e-05, 2.3390e-05, 1.5828e-05, 4.4120e-05, 3.3664e-05],
        [7.9605e-05, 8.4854e-05, 1.0086e-04, 9.4953e-05, 1.0069e-04, 1.4936e-04,
         1.6308e-04, 8.7092e-05, 5.8938e-05, 1.6428e-04, 1.2535e-04],
        [9.7386e-06, 1.0381e-05, 1.2340e-05, 1.1616e-05, 1.2319e-05, 1.8273e-05,
         1.9951e-05, 1.0655e-05, 7.2103e-06, 2.0098e-05, 1.5335e-05],
        [6.7588e-06, 7.2046e-06, 8.5640e-06, 8.0621e-06, 8.5494e-06, 1.2682e-05,
         1.3847e-05, 7.3946e-06, 5.0041e-06, 1.3948e-05, 1.0643e-05],
        [1.3474e-05, 1.4363e-05, 1.7073e-05, 1.6072e-05, 1.7044e-05, 2.5282e-05,
         2.7604e-05, 1.4741e-05, 9.9760e-06, 2.7807e-05, 2.1217e-05],
        [2.2067e-05, 2.3522e-05, 2.7961e-05, 2.6322e-05, 2.7913e-05, 4.1405e-05,
         4.5209e-05, 2.4143e-05, 1.6338e-05, 4.5540e-05, 3.4748e-05]],
       grad_fn=<SoftplusBackward>)

  3%|███▋                                                                                                                                           | 13/496 [00:03<01:35,  5.05it/s]
##### event loss: tensor(1239.0819, grad_fn=<NegBackward>) non event loss:  tensor([11.4493], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0130, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.3340e-05, 1.7685e-05, 1.0547e-05, 1.1152e-05, 1.0549e-05, 9.9959e-06,
         1.0201e-05, 1.4496e-05, 1.0443e-05, 8.2894e-06, 1.6897e-05, 1.5271e-05,
         1.8376e-05, 7.9401e-06, 1.6728e-05, 1.1304e-05, 1.1833e-05, 8.2591e-06,
         8.1016e-06, 1.6852e-05, 5.4671e-06, 1.0193e-05, 1.0921e-05, 1.1399e-05,
         1.6515e-05, 1.5169e-05, 7.1038e-06],
        [1.5134e-05, 2.0064e-05, 1.1966e-05, 1.2651e-05, 1.1968e-05, 1.1340e-05,
         1.1573e-05, 1.6445e-05, 1.1847e-05, 9.4042e-06, 1.9169e-05, 1.7325e-05,
         2.0847e-05, 9.0080e-06, 1.8977e-05, 1.2824e-05, 1.3424e-05, 9.3698e-06,
         9.1911e-06, 1.9118e-05, 6.2024e-06, 1.1563e-05, 1.2390e-05, 1.2932e-05,
         1.8736e-05, 1.7209e-05, 8.0592e-06],
        [2.2432e-05, 2.9739e-05, 1.7735e-05, 1.8752e-05, 1.7739e-05, 1.6809e-05,
         1.7153e-05, 2.4375e-05, 1.7560e-05, 1.3939e-05, 2.8413e-05, 2.5680e-05,
         3.0899e-05, 1.3352e-05, 2.8128e-05, 1.9008e-05, 1.9898e-05, 1.3888e-05,
         1.3623e-05, 2.8337e-05, 9.1932e-06, 1.7139e-05, 1.8365e-05, 1.9168e-05,
         2.7771e-05, 2.5508e-05, 1.1945e-05],
        [7.5680e-06, 1.0033e-05, 5.9835e-06, 6.3264e-06, 5.9848e-06, 5.6708e-06,
         5.7870e-06, 8.2237e-06, 5.9244e-06, 4.7027e-06, 9.5857e-06, 8.6637e-06,
         1.0425e-05, 4.5045e-06, 9.4898e-06, 6.4128e-06, 6.7130e-06, 4.6855e-06,
         4.5961e-06, 9.5604e-06, 3.1016e-06, 5.7824e-06, 6.1959e-06, 6.4669e-06,
         9.3693e-06, 8.6058e-06, 4.0301e-06],
        [9.5136e-06, 1.2613e-05, 7.5218e-06, 7.9528e-06, 7.5234e-06, 7.1287e-06,
         7.2748e-06, 1.0338e-05, 7.4475e-06, 5.9117e-06, 1.2050e-05, 1.0891e-05,
         1.3105e-05, 5.6626e-06, 1.1929e-05, 8.0614e-06, 8.4388e-06, 5.8901e-06,
         5.7777e-06, 1.2018e-05, 3.8989e-06, 7.2690e-06, 7.7888e-06, 8.1295e-06,
         1.1778e-05, 1.0818e-05, 5.0662e-06],
        [1.9450e-05, 2.5786e-05, 1.5378e-05, 1.6259e-05, 1.5381e-05, 1.4574e-05,
         1.4873e-05, 2.1135e-05, 1.5226e-05, 1.2086e-05, 2.4636e-05, 2.2266e-05,
         2.6792e-05, 1.1577e-05, 2.4389e-05, 1.6481e-05, 1.7253e-05, 1.2042e-05,
         1.1812e-05, 2.4571e-05, 7.9712e-06, 1.4861e-05, 1.5924e-05, 1.6620e-05,
         2.4079e-05, 2.2117e-05, 1.0358e-05],
        [1.2439e-05, 1.6491e-05, 9.8345e-06, 1.0398e-05, 9.8367e-06, 9.3206e-06,
         9.5115e-06, 1.3516e-05, 9.7374e-06, 7.7294e-06, 1.5755e-05, 1.4240e-05,
         1.7134e-05, 7.4037e-06, 1.5597e-05, 1.0540e-05, 1.1033e-05, 7.7011e-06,
         7.5542e-06, 1.5713e-05, 5.0977e-06, 9.5040e-06, 1.0184e-05, 1.0629e-05,
         1.5399e-05, 1.4144e-05, 6.6239e-06],
        [1.1673e-05, 1.5476e-05, 9.2293e-06, 9.7582e-06, 9.2313e-06, 8.7470e-06,
         8.9262e-06, 1.2685e-05, 9.1382e-06, 7.2537e-06, 1.4786e-05, 1.3363e-05,
         1.6080e-05, 6.9481e-06, 1.4637e-05, 9.8914e-06, 1.0354e-05, 7.2272e-06,
         7.0893e-06, 1.4746e-05, 4.7840e-06, 8.9191e-06, 9.5569e-06, 9.9749e-06,
         1.4452e-05, 1.3274e-05, 6.2162e-06],
        [4.3889e-06, 5.8185e-06, 3.4700e-06, 3.6688e-06, 3.4707e-06, 3.2886e-06,
         3.3560e-06, 4.7691e-06, 3.4357e-06, 2.7272e-06, 5.5590e-06, 5.0243e-06,
         6.0456e-06, 2.6123e-06, 5.5033e-06, 3.7189e-06, 3.8930e-06, 2.7172e-06,
         2.6654e-06, 5.5443e-06, 1.7987e-06, 3.3534e-06, 3.5931e-06, 3.7503e-06,
         5.4335e-06, 4.9907e-06, 2.3371e-06],
        [6.4396e-06, 8.5373e-06, 5.0914e-06, 5.3832e-06, 5.0925e-06, 4.8253e-06,
         4.9242e-06, 6.9975e-06, 5.0411e-06, 4.0015e-06, 8.1565e-06, 7.3720e-06,
         8.8705e-06, 3.8329e-06, 8.0749e-06, 5.4567e-06, 5.7121e-06, 3.9869e-06,
         3.9109e-06, 8.1350e-06, 2.6391e-06, 4.9203e-06, 5.2721e-06, 5.5027e-06,
         7.9723e-06, 7.3227e-06, 3.4292e-06],
        [5.2615e-06, 6.9754e-06, 4.1599e-06, 4.3983e-06, 4.1608e-06, 3.9425e-06,
         4.0233e-06, 5.7173e-06, 4.1188e-06, 3.2694e-06, 6.6643e-06, 6.0232e-06,
         7.2476e-06, 3.1317e-06, 6.5975e-06, 4.4583e-06, 4.6670e-06, 3.2575e-06,
         3.1953e-06, 6.6466e-06, 2.1563e-06, 4.0201e-06, 4.3075e-06, 4.4960e-06,
         6.5138e-06, 5.9830e-06, 2.8018e-06],
        [3.7916e-06, 5.0267e-06, 2.9978e-06, 3.1695e-06, 2.9984e-06, 2.8411e-06,
         2.8993e-06, 4.1201e-06, 2.9682e-06, 2.3561e-06, 4.8025e-06, 4.3405e-06,
         5.2229e-06, 2.2568e-06, 4.7544e-06, 3.2128e-06, 3.3632e-06, 2.3474e-06,
         2.3027e-06, 4.7898e-06, 1.5539e-06, 2.8970e-06, 3.1042e-06, 3.2399e-06,
         4.6940e-06, 4.3115e-06, 2.0191e-06],
        [3.2019e-06, 4.2449e-06, 2.5315e-06, 2.6766e-06, 2.5321e-06, 2.3992e-06,
         2.4484e-06, 3.4793e-06, 2.5065e-06, 1.9896e-06, 4.0555e-06, 3.6654e-06,
         4.4105e-06, 1.9058e-06, 4.0149e-06, 2.7131e-06, 2.8401e-06, 1.9823e-06,
         1.9445e-06, 4.0448e-06, 1.3122e-06, 2.4464e-06, 2.6214e-06, 2.7360e-06,
         3.9639e-06, 3.6409e-06, 1.7050e-06],
        [4.7885e-06, 6.3482e-06, 3.7859e-06, 4.0029e-06, 3.7867e-06, 3.5881e-06,
         3.6616e-06, 5.2033e-06, 3.7485e-06, 2.9755e-06, 6.0651e-06, 5.4817e-06,
         6.5960e-06, 2.8501e-06, 6.0044e-06, 4.0575e-06, 4.2475e-06, 2.9646e-06,
         2.9081e-06, 6.0491e-06, 1.9624e-06, 3.6587e-06, 3.9203e-06, 4.0918e-06,
         5.9282e-06, 5.4451e-06, 2.5499e-06],
        [3.9270e-06, 5.2062e-06, 3.1048e-06, 3.2827e-06, 3.1055e-06, 2.9425e-06,
         3.0028e-06, 4.2672e-06, 3.0741e-06, 2.4402e-06, 4.9740e-06, 4.4955e-06,
         5.4093e-06, 2.3374e-06, 4.9242e-06, 3.3275e-06, 3.4833e-06, 2.4313e-06,
         2.3849e-06, 4.9608e-06, 1.6094e-06, 3.0005e-06, 3.2150e-06, 3.3556e-06,
         4.8616e-06, 4.4655e-06, 2.0912e-06],
        [5.9777e-06, 7.9249e-06, 4.7262e-06, 4.9970e-06, 4.7272e-06, 4.4792e-06,
         4.5710e-06, 6.4956e-06, 4.6795e-06, 3.7145e-06, 7.5715e-06, 6.8432e-06,
         8.2342e-06, 3.5580e-06, 7.4956e-06, 5.0653e-06, 5.3024e-06, 3.7009e-06,
         3.6303e-06, 7.5514e-06, 2.4498e-06, 4.5674e-06, 4.8939e-06, 5.1080e-06,
         7.4005e-06, 6.7974e-06, 3.1832e-06],
        [7.6938e-06, 1.0200e-05, 6.0830e-06, 6.4316e-06, 6.0843e-06, 5.7651e-06,
         5.8832e-06, 8.3604e-06, 6.0230e-06, 4.7809e-06, 9.7451e-06, 8.8078e-06,
         1.0598e-05, 4.5795e-06, 9.6476e-06, 6.5194e-06, 6.8246e-06, 4.7634e-06,
         4.6726e-06, 9.7193e-06, 3.1531e-06, 5.8786e-06, 6.2989e-06, 6.5745e-06,
         9.5251e-06, 8.7489e-06, 4.0971e-06],
        [9.3524e-06, 1.2399e-05, 7.3944e-06, 7.8181e-06, 7.3960e-06, 7.0079e-06,
         7.1515e-06, 1.0163e-05, 7.3213e-06, 5.8115e-06, 1.1846e-05, 1.0706e-05,
         1.2883e-05, 5.5667e-06, 1.1727e-05, 7.9248e-06, 8.2958e-06, 5.7903e-06,
         5.6798e-06, 1.1815e-05, 3.8329e-06, 7.1459e-06, 7.6568e-06, 7.9917e-06,
         1.1578e-05, 1.0635e-05, 4.9803e-06],
        [8.9018e-06, 1.1801e-05, 7.0381e-06, 7.4414e-06, 7.0396e-06, 6.6703e-06,
         6.8069e-06, 9.6730e-06, 6.9686e-06, 5.5315e-06, 1.1275e-05, 1.0191e-05,
         1.2262e-05, 5.2984e-06, 1.1162e-05, 7.5430e-06, 7.8961e-06, 5.5113e-06,
         5.4062e-06, 1.1245e-05, 3.6482e-06, 6.8015e-06, 7.2879e-06, 7.6067e-06,
         1.1021e-05, 1.0122e-05, 4.7404e-06],
        [1.4390e-05, 1.9078e-05, 1.1378e-05, 1.2030e-05, 1.1380e-05, 1.0783e-05,
         1.1004e-05, 1.5637e-05, 1.1265e-05, 8.9421e-06, 1.8227e-05, 1.6474e-05,
         1.9822e-05, 8.5653e-06, 1.8045e-05, 1.2194e-05, 1.2765e-05, 8.9094e-06,
         8.7394e-06, 1.8179e-05, 5.8976e-06, 1.0995e-05, 1.1781e-05, 1.2297e-05,
         1.7815e-05, 1.6364e-05, 7.6631e-06],
        [1.5443e-05, 2.0474e-05, 1.2210e-05, 1.2910e-05, 1.2213e-05, 1.1572e-05,
         1.1809e-05, 1.6781e-05, 1.2089e-05, 9.5963e-06, 1.9561e-05, 1.7679e-05,
         2.1273e-05, 9.1919e-06, 1.9365e-05, 1.3086e-05, 1.3698e-05, 9.5612e-06,
         9.3788e-06, 1.9509e-05, 6.3290e-06, 1.1800e-05, 1.2643e-05, 1.3196e-05,
         1.9119e-05, 1.7561e-05, 8.2238e-06],
        [9.6483e-06, 1.2791e-05, 7.6283e-06, 8.0654e-06, 7.6299e-06, 7.2296e-06,
         7.3777e-06, 1.0484e-05, 7.5529e-06, 5.9954e-06, 1.2221e-05, 1.1045e-05,
         1.3290e-05, 5.7427e-06, 1.2098e-05, 8.1755e-06, 8.5582e-06, 5.9734e-06,
         5.8595e-06, 1.2188e-05, 3.9541e-06, 7.3719e-06, 7.8990e-06, 8.2445e-06,
         1.1945e-05, 1.0971e-05, 5.1379e-06],
        [2.6792e-06, 3.5520e-06, 2.1183e-06, 2.2397e-06, 2.1188e-06, 2.0076e-06,
         2.0487e-06, 2.9114e-06, 2.0974e-06, 1.6649e-06, 3.3936e-06, 3.0671e-06,
         3.6906e-06, 1.5947e-06, 3.3596e-06, 2.2703e-06, 2.3765e-06, 1.6588e-06,
         1.6271e-06, 3.3846e-06, 1.0980e-06, 2.0471e-06, 2.1935e-06, 2.2894e-06,
         3.3169e-06, 3.0466e-06, 1.4267e-06],
        [1.2792e-06, 1.6959e-06, 1.0114e-06, 1.0694e-06, 1.0116e-06, 9.5856e-07,
         9.7819e-07, 1.3901e-06, 1.0014e-06, 7.9491e-07, 1.6203e-06, 1.4645e-06,
         1.7621e-06, 7.6142e-07, 1.6041e-06, 1.0840e-06, 1.1347e-06, 7.9200e-07,
         7.7690e-07, 1.6160e-06, 5.2427e-07, 9.7742e-07, 1.0473e-06, 1.0931e-06,
         1.5837e-06, 1.4547e-06, 6.8122e-07],
        [1.3053e-05, 1.7305e-05, 1.0320e-05, 1.0912e-05, 1.0323e-05, 9.7811e-06,
         9.9815e-06, 1.4184e-05, 1.0219e-05, 8.1113e-06, 1.6534e-05, 1.4943e-05,
         1.7981e-05, 7.7695e-06, 1.6368e-05, 1.1061e-05, 1.1579e-05, 8.0816e-06,
         7.9274e-06, 1.6490e-05, 5.3496e-06, 9.9736e-06, 1.0687e-05, 1.1154e-05,
         1.6160e-05, 1.4843e-05, 6.9511e-06],
        [1.1134e-05, 1.4760e-05, 8.8028e-06, 9.3072e-06, 8.8047e-06, 8.3427e-06,
         8.5136e-06, 1.2098e-05, 8.7158e-06, 6.9185e-06, 1.4102e-05, 1.2746e-05,
         1.5337e-05, 6.6269e-06, 1.3961e-05, 9.4343e-06, 9.8759e-06, 6.8932e-06,
         6.7617e-06, 1.4065e-05, 4.5629e-06, 8.5069e-06, 9.1152e-06, 9.5139e-06,
         1.3784e-05, 1.2661e-05, 5.9289e-06],
        [7.0606e-06, 9.3605e-06, 5.5823e-06, 5.9022e-06, 5.5836e-06, 5.2906e-06,
         5.3990e-06, 7.6723e-06, 5.5272e-06, 4.3874e-06, 8.9430e-06, 8.0828e-06,
         9.7258e-06, 4.2025e-06, 8.8535e-06, 5.9828e-06, 6.2629e-06, 4.3713e-06,
         4.2880e-06, 8.9194e-06, 2.8936e-06, 5.3947e-06, 5.7805e-06, 6.0333e-06,
         8.7411e-06, 8.0288e-06, 3.7599e-06]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(8690.1592, grad_fn=<NegBackward>) non event loss:  tensor([14.8953], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0512, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.1652e-06, 1.2894e-06, 2.1089e-06, 2.4187e-06, 2.0915e-06],
        [7.0186e-07, 7.7667e-07, 1.2703e-06, 1.4569e-06, 1.2599e-06],
        [7.0186e-07, 7.7667e-07, 1.2703e-06, 1.4569e-06, 1.2599e-06],
        [8.5393e-07, 9.4494e-07, 1.5455e-06, 1.7726e-06, 1.5328e-06],
        [7.9356e-07, 8.7814e-07, 1.4363e-06, 1.6473e-06, 1.4245e-06]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(340.0596, grad_fn=<NegBackward>) non event loss:  tensor([0.5780], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0170, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[6.4529e-07, 2.9374e-07, 2.4501e-07, 7.2446e-07, 3.1288e-07],
        [3.0010e-07, 1.3661e-07, 1.1395e-07, 3.3692e-07, 1.4551e-07],
        [1.6000e-07, 7.2832e-08, 6.0749e-08, 1.7963e-07, 7.7578e-08],
        [3.6031e-08, 1.6402e-08, 1.3680e-08, 4.0451e-08, 1.7470e-08],
        [4.6382e-08, 2.1113e-08, 1.7610e-08, 5.2072e-08, 2.2489e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(406.5481, grad_fn=<NegBackward>) non event loss:  tensor([0.1264], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0425, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[2.2041e-07, 2.2578e-07, 2.3422e-07, 2.0756e-07, 1.6486e-07],
        [1.4378e-07, 1.4728e-07, 1.5278e-07, 1.3539e-07, 1.0754e-07],
        [6.5135e-09, 6.6721e-09, 6.9215e-09, 6.1335e-09, 4.8718e-09],
        [1.0500e-07, 1.0756e-07, 1.1158e-07, 9.8879e-08, 7.8538e-08],
        [5.2646e-07, 5.3928e-07, 5.5944e-07, 4.9575e-07, 3.9377e-07]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(403.6211, grad_fn=<NegBackward>) non event loss:  tensor([0.0555], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0504, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.9860e-07, 3.1146e-07, 2.5283e-07, 3.3809e-07, 4.0310e-07],
        [8.2128e-08, 6.4174e-08, 5.2093e-08, 6.9660e-08, 8.3056e-08],
        [6.6423e-09, 5.1902e-09, 4.2131e-09, 5.6339e-09, 6.7173e-09],
        [6.7931e-09, 5.3081e-09, 4.3088e-09, 5.7618e-09, 6.8698e-09],
        [4.9007e-08, 3.8293e-08, 3.1084e-08, 4.1567e-08, 4.9560e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(431.8707, grad_fn=<NegBackward>) non event loss:  tensor([0.0056], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0410, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[8.9367e-08, 1.1649e-07, 1.3487e-07, 1.0934e-07, 8.3673e-08, 8.8690e-08,
         1.5064e-07, 1.2016e-07, 1.7120e-07, 9.1272e-08, 1.8496e-07, 1.3200e-07,
         1.5367e-07, 1.4484e-07, 1.3323e-07, 1.6072e-07, 1.9417e-07],
        [9.3311e-08, 1.2163e-07, 1.4082e-07, 1.1417e-07, 8.7366e-08, 9.2605e-08,
         1.5729e-07, 1.2546e-07, 1.7875e-07, 9.5300e-08, 1.9313e-07, 1.3782e-07,
         1.6045e-07, 1.5123e-07, 1.3912e-07, 1.6782e-07, 2.0274e-07],
        [5.9572e-08, 7.7651e-08, 8.9905e-08, 7.2887e-08, 5.5777e-08, 5.9121e-08,
         1.0042e-07, 8.0098e-08, 1.1412e-07, 6.0842e-08, 1.2330e-07, 8.7991e-08,
         1.0244e-07, 9.6551e-08, 8.8815e-08, 1.0714e-07, 1.2943e-07],
        [5.8460e-08, 7.6201e-08, 8.8226e-08, 7.1526e-08, 5.4735e-08, 5.8017e-08,
         9.8544e-08, 7.8603e-08, 1.1199e-07, 5.9706e-08, 1.2099e-07, 8.6348e-08,
         1.0052e-07, 9.4748e-08, 8.7156e-08, 1.0514e-07, 1.2701e-07],
        [3.5430e-08, 4.6182e-08, 5.3470e-08, 4.3348e-08, 3.3172e-08, 3.5161e-08,
         5.9723e-08, 4.7637e-08, 6.7871e-08, 3.6185e-08, 7.3329e-08, 5.2331e-08,
         6.0923e-08, 5.7422e-08, 5.2821e-08, 6.3719e-08, 7.6977e-08],
        [2.5693e-08, 3.3491e-08, 3.8776e-08, 3.1436e-08, 2.4057e-08, 2.5499e-08,
         4.3311e-08, 3.4546e-08, 4.9220e-08, 2.6241e-08, 5.3178e-08, 3.7950e-08,
         4.4181e-08, 4.1642e-08, 3.8306e-08, 4.6209e-08, 5.5824e-08],
        [3.0141e-08, 3.9289e-08, 4.5489e-08, 3.6878e-08, 2.8221e-08, 2.9913e-08,
         5.0808e-08, 4.0527e-08, 5.7740e-08, 3.0784e-08, 6.2383e-08, 4.4520e-08,
         5.1829e-08, 4.8851e-08, 4.4937e-08, 5.4208e-08, 6.5487e-08],
        [2.0875e-08, 2.7210e-08, 3.1504e-08, 2.5540e-08, 1.9545e-08, 2.0717e-08,
         3.5188e-08, 2.8067e-08, 3.9989e-08, 2.1320e-08, 4.3204e-08, 3.0833e-08,
         3.5895e-08, 3.3832e-08, 3.1122e-08, 3.7542e-08, 4.5354e-08],
        [4.0287e-08, 5.2514e-08, 6.0801e-08, 4.9292e-08, 3.7721e-08, 3.9982e-08,
         6.7911e-08, 5.4169e-08, 7.7177e-08, 4.1146e-08, 8.3383e-08, 5.9506e-08,
         6.9276e-08, 6.5295e-08, 6.0064e-08, 7.2456e-08, 8.7532e-08],
        [9.8683e-09, 1.2863e-08, 1.4893e-08, 1.2074e-08, 9.2396e-09, 9.7936e-09,
         1.6635e-08, 1.3269e-08, 1.8904e-08, 1.0079e-08, 2.0425e-08, 1.4576e-08,
         1.6969e-08, 1.5994e-08, 1.4712e-08, 1.7748e-08, 2.1441e-08],
        [4.5435e-09, 5.9224e-09, 6.8570e-09, 5.5590e-09, 4.2540e-09, 4.5091e-09,
         7.6589e-09, 6.1090e-09, 8.7038e-09, 4.6404e-09, 9.4037e-09, 6.7110e-09,
         7.8128e-09, 7.3638e-09, 6.7738e-09, 8.1714e-09, 9.8716e-09],
        [3.0387e-09, 3.9609e-09, 4.5859e-09, 3.7179e-09, 2.8451e-09, 3.0157e-09,
         5.1222e-09, 4.0857e-09, 5.8211e-09, 3.1035e-09, 6.2892e-09, 4.4883e-09,
         5.2252e-09, 4.9249e-09, 4.5303e-09, 5.4650e-09, 6.6021e-09],
        [3.0732e-09, 4.0059e-09, 4.6380e-09, 3.7601e-09, 2.8774e-09, 3.0499e-09,
         5.1804e-09, 4.1321e-09, 5.8872e-09, 3.1387e-09, 6.3606e-09, 4.5392e-09,
         5.2845e-09, 4.9808e-09, 4.5818e-09, 5.5271e-09, 6.6771e-09],
        [3.7943e-09, 4.9458e-09, 5.7263e-09, 4.6424e-09, 3.5526e-09, 3.7656e-09,
         6.3960e-09, 5.1017e-09, 7.2686e-09, 3.8752e-09, 7.8531e-09, 5.6044e-09,
         6.5245e-09, 6.1496e-09, 5.6569e-09, 6.8239e-09, 8.2439e-09],
        [5.6133e-10, 7.3168e-10, 8.4715e-10, 6.8679e-10, 5.2557e-10, 5.5708e-10,
         9.4622e-10, 7.5474e-10, 1.0753e-09, 5.7329e-10, 1.1618e-09, 8.2911e-10,
         9.6524e-10, 9.0977e-10, 8.3687e-10, 1.0095e-09, 1.2196e-09],
        [8.0954e-10, 1.0552e-09, 1.2218e-09, 9.9049e-10, 7.5797e-10, 8.0342e-10,
         1.3646e-09, 1.0885e-09, 1.5508e-09, 8.2680e-10, 1.6755e-09, 1.1957e-09,
         1.3921e-09, 1.3121e-09, 1.2069e-09, 1.4559e-09, 1.7589e-09],
        [1.2467e-08, 1.6251e-08, 1.8815e-08, 1.5254e-08, 1.1673e-08, 1.2373e-08,
         2.1016e-08, 1.6763e-08, 2.3883e-08, 1.2733e-08, 2.5804e-08, 1.8415e-08,
         2.1438e-08, 2.0206e-08, 1.8587e-08, 2.2422e-08, 2.7087e-08]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(5148.5020, grad_fn=<NegBackward>) non event loss:  tensor([0.0268], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1334, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.9640e-09, 5.1276e-09, 2.7335e-09],
        [1.3943e-09, 1.8036e-09, 9.6146e-10],
        [1.3943e-09, 1.8036e-09, 9.6146e-10]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(180.7263, grad_fn=<NegBackward>) non event loss:  tensor([0.0004], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0281, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[3.3827e-08, 4.3254e-08, 8.0427e-08, 6.0498e-08, 6.5471e-08, 5.4650e-08],
        [3.8843e-09, 4.9669e-09, 9.2354e-09, 6.9469e-09, 7.5179e-09, 6.2754e-09],
        [1.2687e-09, 1.6222e-09, 3.0164e-09, 2.2690e-09, 2.4554e-09, 2.0496e-09],
        [5.3580e-10, 6.8513e-10, 1.2739e-09, 9.5827e-10, 1.0370e-09, 8.6563e-10],
        [2.6862e-09, 3.4348e-09, 6.3867e-09, 4.8041e-09, 5.1990e-09, 4.3397e-09],
        [2.5616e-09, 3.2755e-09, 6.0905e-09, 4.5814e-09, 4.9579e-09, 4.1385e-09]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(690.5120, grad_fn=<NegBackward>) non event loss:  tensor([0.0036], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1066, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[1.5027e-09, 1.3616e-09, 1.2019e-09, 8.5564e-10, 1.0133e-09, 6.9526e-10,
         1.1300e-09, 7.2875e-10, 1.3023e-09, 1.1612e-09, 1.1943e-09, 1.1612e-09,
         7.4207e-10],
        [9.5638e-10, 8.6657e-10, 7.6492e-10, 5.4456e-10, 6.4491e-10, 4.4249e-10,
         7.1916e-10, 4.6380e-10, 8.2885e-10, 7.3906e-10, 7.6008e-10, 7.3906e-10,
         4.7228e-10],
        [7.5014e-10, 6.7969e-10, 5.9997e-10, 4.2713e-10, 5.0584e-10, 3.4707e-10,
         5.6407e-10, 3.6378e-10, 6.5011e-10, 5.7968e-10, 5.9617e-10, 5.7968e-10,
         3.7043e-10],
        [2.8860e-09, 2.6149e-09, 2.3082e-09, 1.6433e-09, 1.9461e-09, 1.3353e-09,
         2.1701e-09, 1.3996e-09, 2.5011e-09, 2.2302e-09, 2.2936e-09, 2.2302e-09,
         1.4251e-09],
        [2.9342e-09, 2.6587e-09, 2.3468e-09, 1.6707e-09, 1.9786e-09, 1.3576e-09,
         2.2064e-09, 1.4230e-09, 2.5429e-09, 2.2675e-09, 2.3320e-09, 2.2675e-09,
         1.4490e-09],
        [3.1711e-09, 2.8733e-09, 2.5363e-09, 1.8056e-09, 2.1384e-09, 1.4672e-09,
         2.3846e-09, 1.5379e-09, 2.7483e-09, 2.4505e-09, 2.5203e-09, 2.4505e-09,
         1.5660e-09],
        [7.8203e-11, 7.0859e-11, 6.2548e-11, 4.4529e-11, 5.2734e-11, 3.6183e-11,
         5.8806e-11, 3.7925e-11, 6.7775e-11, 6.0433e-11, 6.2152e-11, 6.0433e-11,
         3.8618e-11],
        [7.4599e-11, 6.7594e-11, 5.9665e-11, 4.2477e-11, 5.0304e-11, 3.4515e-11,
         5.6096e-11, 3.6177e-11, 6.4651e-11, 5.7648e-11, 5.9288e-11, 5.7648e-11,
         3.6838e-11],
        [7.8817e-11, 7.1415e-11, 6.3038e-11, 4.4878e-11, 5.3148e-11, 3.6466e-11,
         5.9267e-11, 3.8223e-11, 6.8307e-11, 6.0907e-11, 6.2640e-11, 6.0907e-11,
         3.8921e-11],
        [2.2006e-10, 1.9939e-10, 1.7600e-10, 1.2530e-10, 1.4839e-10, 1.0181e-10,
         1.6547e-10, 1.0672e-10, 1.9071e-10, 1.7005e-10, 1.7489e-10, 1.7005e-10,
         1.0867e-10],
        [1.5017e-10, 1.3607e-10, 1.2011e-10, 8.5508e-11, 1.0126e-10, 6.9481e-11,
         1.1292e-10, 7.2827e-11, 1.3015e-10, 1.1605e-10, 1.1935e-10, 1.1605e-10,
         7.4158e-11],
        [1.0867e-10, 9.8467e-11, 8.6917e-11, 6.1878e-11, 7.3281e-11, 5.0280e-11,
         8.1718e-11, 5.2702e-11, 9.4181e-11, 8.3978e-11, 8.6367e-11, 8.3978e-11,
         5.3664e-11],
        [3.7746e-09, 3.4201e-09, 3.0190e-09, 2.1492e-09, 2.5453e-09, 1.7464e-09,
         2.8384e-09, 1.8305e-09, 3.2713e-09, 2.9169e-09, 2.9999e-09, 2.9169e-09,
  3%|███▋                                                                                                                                           | 13/496 [00:03<02:04,  3.89it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 107, in train
    loss, pos_timing_loss, neg_timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 195, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 369, in forward
    z_dt_i = self.transform(torch.cat((torch.index_select(mat_b, 1, arr_b_idx_i), torch.index_select(mat_c, 1, arr_c_idx_i)), dim=2))  # z_dt : (N_i_2, embedding_b + embedding_c)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 197, in format_stack
    return format_list(extract_stack(f, limit=limit))
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 211, in extract_stack
    stack = StackSummary.extract(walk_stack(f), limit=limit)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 362, in extract
    linecache.checkcache(filename)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/linecache.py", line 74, in checkcache
    stat = os.stat(fullname)
KeyboardInterrupt