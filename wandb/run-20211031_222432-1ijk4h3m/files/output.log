
CUDA availability: True
  0%|                                                                                                                                                                                                              | 0/496 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 111, in train
    torch.nn.utils.clip_grad_norm_(model.parameters())
TypeError: clip_grad_norm_() missing 1 required positional argument: 'max_norm'
-1
-1
### event lambdas:  tensor([[ 3.8056],
        [ 2.2899],
        [ 9.4064],
        [ 7.0768],
        [20.4803],
        [21.9977],
        [ 5.7972],
        [31.4252]], grad_fn=<AddBackward0>)
##### event loss: tensor(-17678.5957, grad_fn=<NegBackward>) non event loss:  tensor([77237.1838], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(5.0315, grad_fn=<SumBackward0>)