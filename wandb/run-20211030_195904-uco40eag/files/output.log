
CUDA availability: True
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 199, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)*1000
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 359, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
batch number:  0
#### arr_b tensor([[[ 1.6461,  0.2316, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 1.8767,  0.3045, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.2556,  0.3868, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.6150,  0.4672, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.8471,  0.5928, -0.2817, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.2704,  0.5755, -0.2752, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.8558,  0.6120, -0.2765, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 4.3829,  0.6332, -0.2491, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 5.4617,  0.8968, -0.2266, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 5.3630,  0.7951,  0.1857, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415]]]) torch.Size([1, 14, 14])
#### mat_b tensor([[[-0.0085, -0.1482, -0.0721, -0.2664,  0.1386, -0.0467,  0.4100,
           0.1704,  0.1103, -0.0256, -0.0617, -0.2371, -0.0882, -0.1610,
          -0.3381,  0.0235,  0.4750, -0.3541,  0.0856,  0.0690, -0.0495,
          -0.0450, -0.0952, -0.1072,  0.0831, -0.0901, -0.1496,  0.0899,
           0.0970,  0.0224, -0.0401, -0.1776],
         [ 0.1105, -0.1275, -0.0589, -0.3038,  0.0311,  0.0259,  0.3089,
           0.2010,  0.0956, -0.0968,  0.0307, -0.2719, -0.0239, -0.2364,
          -0.3717, -0.0457,  0.4779, -0.3653,  0.1353,  0.0013, -0.0318,
           0.1110, -0.0434,  0.0492,  0.0438, -0.0984, -0.1000, -0.0497,
           0.1629, -0.0437,  0.0653, -0.1475],
         [-0.0850, -0.1280, -0.0926, -0.1604,  0.1078,  0.0324,  0.3309,
          -0.0678, -0.0866, -0.0090, -0.1945, -0.2981, -0.1341, -0.3445,
          -0.3669, -0.0043,  0.4413, -0.3968,  0.2499, -0.0905,  0.0829,
           0.1516,  0.0648, -0.1561, -0.0186, -0.0434, -0.2322, -0.0440,
           0.1430,  0.0596, -0.0556, -0.2134],
         [-0.0341, -0.3138, -0.0559, -0.3871,  0.3327, -0.0957,  0.3292,
           0.1445,  0.0072,  0.0923, -0.0740, -0.0462, -0.0838, -0.1282,
          -0.4524,  0.0674,  0.5979, -0.3853,  0.0788, -0.0509, -0.1051,
           0.3293,  0.0354, -0.1327, -0.1228, -0.3694, -0.2910, -0.0162,
          -0.0838, -0.1598,  0.0459, -0.1585],
         [ 0.0726, -0.0595, -0.2598, -0.2193,  0.1113,  0.0332,  0.6353,
           0.3083,  0.2126, -0.1541, -0.0530, -0.2342, -0.0487, -0.2057,
          -0.5813,  0.0601,  0.7603, -0.3715,  0.0753,  0.0894, -0.1402,
           0.0795, -0.1606, -0.1791,  0.2685, -0.2631, -0.1613,  0.0083,
           0.1896,  0.0481, -0.0681, -0.0874],
         [ 0.0105, -0.5447,  0.0670, -0.5436,  0.2166, -0.0408,  0.3725,
           0.1862, -0.1193,  0.0084, -0.0934,  0.0048, -0.1344, -0.0497,
          -0.2828, -0.1006,  0.6131, -0.4182, -0.1217,  0.0490, -0.2352,
           0.1284, -0.2721, -0.1732,  0.0332, -0.1646, -0.1910, -0.1369,
          -0.0825, -0.1313,  0.0709,  0.0303],
         [ 0.2392, -0.2978, -0.2326, -0.5379,  0.1302,  0.2783,  0.5336,
           0.2752,  0.0364, -0.1910,  0.0335, -0.2808, -0.1938, -0.3005,
          -0.3348,  0.0651,  0.9111, -0.4560,  0.1150,  0.0218, -0.0488,
          -0.0492, -0.1831, -0.2701,  0.2795, -0.1016, -0.2211,  0.2531,
           0.1193,  0.1264,  0.1261,  0.1026],
         [ 0.2958, -0.2713, -0.1390, -0.5207,  0.2311,  0.0607,  0.6070,
           0.1496,  0.2908, -0.1381, -0.2039, -0.2047, -0.0702, -0.4917,
          -0.3596,  0.1303,  1.0882, -0.5891,  0.0649, -0.1386, -0.0374,
          -0.0093, -0.0957,  0.1636,  0.1773, -0.2637, -0.2998,  0.1495,
           0.0121, -0.0780,  0.1368, -0.1166],
         [ 0.1717, -0.3315, -0.1033, -0.3955,  0.1616,  0.0435,  0.5254,
           0.1297, -0.0677, -0.1271, -0.1831, -0.3063, -0.0930, -0.3683,
          -0.5745, -0.1298,  0.7273, -0.4834,  0.1301, -0.0052, -0.1624,
           0.2347, -0.1882, -0.0523, -0.0282, -0.0215, -0.1275, -0.0978,
           0.1555, -0.0271, -0.0434, -0.2305],
         [ 0.1199, -0.2007, -0.2682, -0.3225,  0.4245,  0.0289,  0.5018,
           0.1330,  0.0237,  0.0463, -0.2454, -0.1570, -0.0435, -0.2718,
          -0.6199,  0.0975,  0.6766, -0.5305,  0.1788, -0.0106, -0.0018,
           0.2999, -0.1221, -0.2819,  0.0317, -0.2521, -0.3467,  0.1369,
          -0.0274, -0.1194, -0.0802, -0.2696],
         [ 0.0876, -0.2001, -0.3448, -0.4560,  0.2821,  0.0722,  0.5988,
           0.1981,  0.1011, -0.0993, -0.1346, -0.2345, -0.1426, -0.3314,
          -0.4827,  0.1693,  0.8043, -0.6045,  0.0354, -0.0303, -0.0503,
           0.1392, -0.1684, -0.2511,  0.2347, -0.1455, -0.1667,  0.1403,
           0.0045, -0.0294,  0.0612, -0.1413],
         [ 0.3130, -0.3793, -0.4007, -0.8663,  0.2582,  0.1845,  0.7019,
           0.2623,  0.2507, -0.3293, -0.1725, -0.1319, -0.4417, -0.5135,
          -0.3840,  0.2843,  1.2027, -0.8713, -0.0963, -0.2072,  0.0341,
           0.1806, -0.1933, -0.2424,  0.4332, -0.3558, -0.1645, -0.0505,
          -0.0410, -0.0936,  0.4697,  0.1654],
         [-0.0893, -0.1504,  0.0821, -0.4402,  0.2411, -0.0506,  0.3369,
           0.0544, -0.0550,  0.0468, -0.0173, -0.1591, -0.0257, -0.2313,
          -0.4094, -0.0383,  0.7269, -0.4493,  0.0345, -0.1331, -0.0719,
           0.4617,  0.2471,  0.1155, -0.1955, -0.3872, -0.2394, -0.1112,
           0.0369, -0.1582,  0.0923, -0.2438],
         [ 0.2064, -0.2710, -0.2656, -0.5998,  0.2711,  0.2784,  0.9289,
           0.3138,  0.0700, -0.1904, -0.1659, -0.1089, -0.4808, -0.3844,
          -0.5655, -0.0853,  1.0740, -0.5167, -0.0791,  0.1165, -0.2563,
           0.0175, -0.4186, -0.7287,  0.4710, -0.1280, -0.2794, -0.0452,
           0.0844,  0.1845, -0.1809,  0.1344]]], grad_fn=<AddBackward0>) torch.Size([1, 14, 32])
batch number:  1
#### arr_b tensor([[[ 1.6044,  0.2160, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 1.8308,  0.2877, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 2.2026,  0.3684, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 2.5554,  0.4473, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 2.7832,  0.5706, -0.2877, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.1987,  0.5536, -0.2814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.7732,  0.5895, -0.2826, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 4.2905,  0.6102, -0.2557, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 5.3494,  0.8690, -0.2337, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 5.2525,  0.7692,  0.1711, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 5.0334,  0.7211,  0.2190, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.4833, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 4.2967,  0.7003,  0.2996, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.3303, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.3933, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464],
         [ 3.2280,  0.3646, -0.0814, -0.3464, -0.3464, -0.3464, -0.3464,
          -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464, -0.3464]]]) torch.Size([1, 22, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<AddBackward0>) torch.Size([1, 22, 32])