
CUDA availability: True
  0%|                                                                                                                                                       | 0/496 [00:00<?, ?it/s]/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in ExpBackward. Traceback of forward call that caused the error:
  File "main.py", line 126, in <module>
    main()
  File "main.py", line 122, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 81, in train
    loss, timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 185, in forward
    non_event_lambdas = self.timing_net(mat_b, mat_c, non_event_data) # (L_Neg, )
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 268, in forward
    lambda_dt = self.f_lambda(rate)
  File "/home/dalab5/Projects/MA_packed/model.py", line 228, in f_lambda
    lambda_dt = self.phi*torch.log(1+torch.exp(x/self.phi))
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378062065/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
  0%|                                                                                                                                                       | 0/496 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 126, in <module>
    main()
  File "main.py", line 122, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 84, in train
    loss.backward() # required_graph = True
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Function 'ExpBackward' returned nan values in its 0th output.
#### arr_b tensor([[[ 1.7840,  0.2733, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 2.1480,  0.3524, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 2.4934,  0.4296, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 2.7164,  0.5503, -0.2900, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1231,  0.5337, -0.2838, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.6856,  0.5688, -0.2850, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 4.1920,  0.5891, -0.2586, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.4648, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.4648, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.4648, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 5.2286,  0.8425, -0.2370, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.4648, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 5.1338,  0.7447,  0.1592, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 4.9193,  0.6976,  0.2061, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.4648, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.4648, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.4648, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 4.1981,  0.6773,  0.2850, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.3151, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474],
         [ 3.1519,  0.3768, -0.0880, -0.3474, -0.3474, -0.3474, -0.3474,
          -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474, -0.3474]]]) torch.Size([1, 20, 14])
#### mat_b tensor([[[ 2.0076e-01,  1.0317e-01,  1.0450e-01, -1.1307e-01,  1.3590e-01,
          -8.4690e-04,  8.6102e-02,  9.4376e-02, -2.3698e-01,  2.2412e-01,
          -5.2910e-02, -1.6352e-03, -3.9850e-02,  2.7033e-01, -1.9533e-01,
           4.8118e-01, -1.8976e-01,  4.1456e-01,  1.5281e-02,  8.8588e-02,
          -1.0365e-01, -2.5344e-01, -1.8519e-01,  1.1664e-02,  4.3705e-01,
           3.3612e-01,  2.1248e-01,  6.7423e-02,  3.0016e-01, -3.8470e-02,
          -5.4748e-03, -2.4045e-01],
         [ 2.1114e-01,  8.4159e-02,  1.4468e-01, -1.0642e-01,  1.5532e-01,
           4.7821e-02,  6.9392e-02,  9.4018e-02, -2.6440e-01,  2.4462e-01,
          -4.9519e-02, -5.5765e-03, -4.0014e-02,  2.8679e-01, -2.2886e-01,
           5.1441e-01, -2.0811e-01,  4.5606e-01,  7.5763e-03,  1.0253e-01,
          -1.3772e-01, -2.6269e-01, -1.9655e-01,  2.2394e-02,  5.0829e-01,
           3.4781e-01,  2.1655e-01,  7.4431e-02,  3.3278e-01, -4.4619e-02,
          -1.7974e-02, -2.8868e-01],
         [ 2.2368e-01,  6.9023e-02,  1.7842e-01, -9.6489e-02,  1.6434e-01,
           9.4199e-02,  5.7371e-02,  1.0109e-01, -2.8330e-01,  2.6677e-01,
          -3.8047e-02, -5.9747e-04, -4.2355e-02,  3.1184e-01, -2.6452e-01,
           5.4611e-01, -2.3020e-01,  5.0370e-01, -2.3955e-04,  1.1780e-01,
          -1.6869e-01, -2.6799e-01, -2.0243e-01,  3.5436e-02,  5.7201e-01,
           3.6279e-01,  2.1899e-01,  9.0123e-02,  3.5342e-01, -5.2340e-02,
          -3.3739e-02, -3.2912e-01],
         [ 2.4363e-01,  6.0887e-02,  2.3038e-01, -9.1432e-02,  1.6558e-01,
           1.2923e-01,  2.4648e-02,  7.5500e-02, -3.1281e-01,  2.8747e-01,
          -3.6098e-02, -6.7495e-03, -1.7792e-02,  3.4473e-01, -2.9533e-01,
           5.6431e-01, -2.3064e-01,  5.5225e-01, -4.5229e-03,  1.2209e-01,
          -1.7811e-01, -2.3922e-01, -1.9125e-01,  3.3347e-02,  6.0841e-01,
           3.7205e-01,  2.2176e-01,  1.1774e-01,  3.5627e-01, -4.7164e-02,
          -3.6288e-02, -3.5161e-01],
         [ 2.5843e-01,  4.3062e-02,  2.6681e-01, -8.1513e-02,  1.8580e-01,
           1.7871e-01,  2.7432e-02,  9.3552e-02, -3.3421e-01,  3.2190e-01,
          -2.2367e-02,  7.3628e-03, -2.5613e-02,  3.6963e-01, -3.2793e-01,
           6.1058e-01, -2.5967e-01,  6.1437e-01, -1.4890e-02,  1.3776e-01,
          -2.1541e-01, -2.5775e-01, -2.0862e-01,  5.1087e-02,  6.8415e-01,
           3.8801e-01,  2.2680e-01,  1.3656e-01,  3.7772e-01, -6.4294e-02,
          -5.2963e-02, -4.0749e-01],
         [ 2.7855e-01,  1.6389e-02,  3.1947e-01, -6.9209e-02,  2.1025e-01,
           2.5202e-01,  2.2612e-02,  1.1134e-01, -3.6303e-01,  3.6680e-01,
          -2.5177e-03,  2.1603e-02, -3.5476e-02,  4.0660e-01, -3.7812e-01,
           6.6805e-01, -2.9969e-01,  6.9596e-01, -2.9054e-02,  1.5879e-01,
          -2.6420e-01, -2.7856e-01, -2.2864e-01,  7.3558e-02,  7.8976e-01,
           4.1154e-01,  2.3413e-01,  1.6148e-01,  4.0971e-01, -8.4007e-02,
          -7.4490e-02, -4.8186e-01],
         [ 2.9033e-01, -2.3310e-03,  3.5317e-01, -7.4006e-02,  2.3604e-01,
           3.1422e-01,  1.6930e-02,  1.3639e-01, -3.8464e-01,  3.9951e-01,
           8.8575e-03,  3.3112e-02, -3.8595e-02,  4.3362e-01, -4.2350e-01,
           7.2049e-01, -3.3491e-01,  7.7094e-01, -4.1510e-02,  1.7214e-01,
          -3.0511e-01, -3.0064e-01, -2.5346e-01,  9.0238e-02,  8.9569e-01,
           4.3810e-01,  2.3373e-01,  1.7006e-01,  4.4759e-01, -1.0591e-01,
          -9.1710e-02, -5.4029e-01],
         [ 2.4511e-01,  3.9792e-02,  2.4031e-01, -8.0656e-02,  1.9197e-01,
           1.7774e-01,  5.1193e-02,  1.2561e-01, -3.1874e-01,  3.1903e-01,
          -1.6227e-02,  1.8896e-02, -5.1351e-02,  3.5541e-01, -3.2260e-01,
           6.1745e-01, -2.7437e-01,  6.0248e-01, -1.6020e-02,  1.4313e-01,
          -2.2822e-01, -2.8948e-01, -2.2590e-01,  6.3994e-02,  6.9501e-01,
           3.8928e-01,  2.2481e-01,  1.2195e-01,  3.9015e-01, -7.4982e-02,
          -6.2159e-02, -4.1292e-01],
         [ 2.4511e-01,  3.9792e-02,  2.4031e-01, -8.0656e-02,  1.9197e-01,
           1.7774e-01,  5.1193e-02,  1.2561e-01, -3.1874e-01,  3.1903e-01,
          -1.6227e-02,  1.8896e-02, -5.1351e-02,  3.5541e-01, -3.2260e-01,
           6.1745e-01, -2.7437e-01,  6.0248e-01, -1.6020e-02,  1.4313e-01,
          -2.2822e-01, -2.8948e-01, -2.2590e-01,  6.3994e-02,  6.9501e-01,
           3.8928e-01,  2.2481e-01,  1.2195e-01,  3.9015e-01, -7.4982e-02,
          -6.2159e-02, -4.1292e-01],
         [ 2.4511e-01,  3.9792e-02,  2.4031e-01, -8.0656e-02,  1.9197e-01,
           1.7774e-01,  5.1193e-02,  1.2561e-01, -3.1874e-01,  3.1903e-01,
          -1.6227e-02,  1.8896e-02, -5.1351e-02,  3.5541e-01, -3.2260e-01,
           6.1745e-01, -2.7437e-01,  6.0248e-01, -1.6020e-02,  1.4313e-01,
          -2.2822e-01, -2.8948e-01, -2.2590e-01,  6.3994e-02,  6.9501e-01,
           3.8928e-01,  2.2481e-01,  1.2195e-01,  3.9015e-01, -7.4982e-02,
          -6.2159e-02, -4.1292e-01],
         [ 3.1918e-01, -4.2944e-02,  4.2854e-01, -8.3429e-02,  2.7305e-01,
           4.5204e-01, -2.6507e-02,  1.5605e-01, -4.2493e-01,  4.5525e-01,
           3.5301e-02,  3.3921e-02, -4.2394e-02,  5.0318e-01, -5.3481e-01,
           8.0702e-01, -4.0144e-01,  9.0543e-01, -6.3621e-02,  1.9583e-01,
          -3.7866e-01, -3.2476e-01, -2.8637e-01,  1.1324e-01,  1.1121e+00,
           5.0024e-01,  2.3782e-01,  1.8731e-01,  5.2965e-01, -1.2990e-01,
          -1.2539e-01, -6.5059e-01],
         [ 2.4511e-01,  3.9792e-02,  2.4031e-01, -8.0656e-02,  1.9197e-01,
           1.7774e-01,  5.1193e-02,  1.2561e-01, -3.1874e-01,  3.1903e-01,
          -1.6227e-02,  1.8896e-02, -5.1351e-02,  3.5541e-01, -3.2260e-01,
           6.1745e-01, -2.7437e-01,  6.0248e-01, -1.6020e-02,  1.4313e-01,
          -2.2822e-01, -2.8948e-01, -2.2590e-01,  6.3994e-02,  6.9501e-01,
           3.8928e-01,  2.2481e-01,  1.2195e-01,  3.9015e-01, -7.4982e-02,
          -6.2159e-02, -4.1292e-01],
         [ 2.8037e-01, -3.2720e-02,  3.5258e-01, -9.5070e-02,  2.6380e-01,
           4.2549e-01,  4.8313e-03,  2.3269e-01, -3.9259e-01,  4.2162e-01,
           3.1532e-02,  5.6828e-02, -6.9391e-02,  4.6093e-01, -5.1483e-01,
           8.0474e-01, -4.0860e-01,  8.7358e-01, -5.9467e-02,  2.0646e-01,
          -4.0271e-01, -3.6788e-01, -3.0663e-01,  1.3089e-01,  1.1065e+00,
           4.9982e-01,  2.1593e-01,  1.4713e-01,  5.4731e-01, -1.5023e-01,
          -1.4053e-01, -6.1742e-01],
         [ 2.7130e-01, -2.5107e-02,  3.2746e-01, -9.3880e-02,  2.5274e-01,
           3.9664e-01,  1.4649e-02,  2.3805e-01, -3.8010e-01,  4.0579e-01,
           2.8163e-02,  5.6748e-02, -7.3833e-02,  4.4209e-01, -4.9217e-01,
           7.8412e-01, -3.9714e-01,  8.3989e-01, -5.2268e-02,  2.0330e-01,
          -3.9141e-01, -3.7028e-01, -3.0192e-01,  1.2743e-01,  1.0616e+00,
           4.8856e-01,  2.1396e-01,  1.3816e-01,  5.3420e-01, -1.4862e-01,
          -1.3359e-01, -5.9248e-01],
         [ 2.4511e-01,  3.9792e-02,  2.4031e-01, -8.0656e-02,  1.9197e-01,
           1.7774e-01,  5.1193e-02,  1.2561e-01, -3.1874e-01,  3.1903e-01,
          -1.6227e-02,  1.8896e-02, -5.1351e-02,  3.5541e-01, -3.2260e-01,
           6.1745e-01, -2.7437e-01,  6.0248e-01, -1.6020e-02,  1.4313e-01,
          -2.2822e-01, -2.8948e-01, -2.2590e-01,  6.3994e-02,  6.9501e-01,
           3.8928e-01,  2.2481e-01,  1.2195e-01,  3.9015e-01, -7.4982e-02,
          -6.2159e-02, -4.1292e-01],
         [ 2.4511e-01,  3.9792e-02,  2.4031e-01, -8.0656e-02,  1.9197e-01,
           1.7774e-01,  5.1193e-02,  1.2561e-01, -3.1874e-01,  3.1903e-01,
          -1.6227e-02,  1.8896e-02, -5.1351e-02,  3.5541e-01, -3.2260e-01,
           6.1745e-01, -2.7437e-01,  6.0248e-01, -1.6020e-02,  1.4313e-01,
          -2.2822e-01, -2.8948e-01, -2.2590e-01,  6.3994e-02,  6.9501e-01,
           3.8928e-01,  2.2481e-01,  1.2195e-01,  3.9015e-01, -7.4982e-02,
          -6.2159e-02, -4.1292e-01],
         [ 2.4511e-01,  3.9792e-02,  2.4031e-01, -8.0656e-02,  1.9197e-01,
           1.7774e-01,  5.1193e-02,  1.2561e-01, -3.1874e-01,  3.1903e-01,
          -1.6227e-02,  1.8896e-02, -5.1351e-02,  3.5541e-01, -3.2260e-01,
           6.1745e-01, -2.7437e-01,  6.0248e-01, -1.6020e-02,  1.4313e-01,
          -2.2822e-01, -2.8948e-01, -2.2590e-01,  6.3994e-02,  6.9501e-01,
           3.8928e-01,  2.2481e-01,  1.2195e-01,  3.9015e-01, -7.4982e-02,
          -6.2159e-02, -4.1292e-01],
         [ 2.4459e-01, -4.2119e-03,  2.6214e-01, -8.9831e-02,  2.1535e-01,
           3.1297e-01,  2.9839e-02,  2.1907e-01, -3.3815e-01,  3.5402e-01,
           1.8405e-02,  4.2304e-02, -8.5534e-02,  3.9280e-01, -4.2785e-01,
           7.0294e-01, -3.5767e-01,  7.1839e-01, -3.3638e-02,  1.8775e-01,
          -3.3773e-01, -3.5816e-01, -2.7383e-01,  1.0950e-01,  9.1399e-01,
           4.5157e-01,  2.1350e-01,  1.1269e-01,  4.9024e-01, -1.2417e-01,
          -1.0925e-01, -5.0860e-01],
         [ 2.4369e-01,  3.8617e-02,  2.3680e-01, -8.2248e-02,  2.0584e-01,
           1.7147e-01,  7.2187e-02,  1.4279e-01, -3.2186e-01,  3.2794e-01,
          -1.7305e-02,  2.8613e-02, -5.4874e-02,  3.4756e-01, -3.1045e-01,
           6.2954e-01, -2.8017e-01,  6.1277e-01, -1.5813e-02,  1.3941e-01,
          -2.3018e-01, -3.0569e-01, -2.4310e-01,  6.9268e-02,  6.9719e-01,
           3.8581e-01,  2.2406e-01,  1.2104e-01,  3.9135e-01, -9.0061e-02,
          -5.7413e-02, -4.2301e-01],
         [ 2.4403e-01,  3.9634e-02,  2.3830e-01, -8.2069e-02,  2.0080e-01,
           1.7407e-01,  6.3947e-02,  1.3504e-01, -3.2038e-01,  3.2438e-01,
          -1.7105e-02,  2.4847e-02, -5.3020e-02,  3.5087e-01, -3.1547e-01,
           6.2508e-01, -2.7768e-01,  6.0883e-01, -1.6588e-02,  1.4093e-01,
          -2.2886e-01, -2.9843e-01, -2.3570e-01,  6.6996e-02,  6.9679e-01,
           3.8709e-01,  2.2423e-01,  1.2121e-01,  3.9056e-01, -8.3218e-02,
          -6.0024e-02, -4.1862e-01]]], grad_fn=<AddBackward0>) torch.Size([1, 20, 32])