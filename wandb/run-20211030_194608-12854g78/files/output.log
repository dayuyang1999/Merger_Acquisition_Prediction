
CUDA availability: True
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 199, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)*1000
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 359, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
#### arr_b tensor([[[ 1.8953,  0.2978, -0.0445, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 2.1558,  0.3802, -0.0445, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 2.5837,  0.4731, -0.0445, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 2.9897,  0.5639, -0.0445, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 3.2518,  0.7058, -0.2819, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 3.7299,  0.6862, -0.2746, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 4.3910,  0.7275, -0.2761, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 4.9863,  0.7514, -0.2451, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 3.7637,  0.6052, -0.0445, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 3.7637,  0.6052, -0.0445, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495],
         [ 3.7637,  0.6052, -0.0445, -0.3495, -0.3495, -0.3495, -0.3495,
          -0.3495, -0.3495, -0.3495, -0.3494, -0.3495, -0.3495, -0.3495]]]) torch.Size([1, 11, 14])
#### mat_b tensor([[[-1.0709e-01, -5.8866e-01, -2.5154e-02, -6.5019e-02,  4.6985e-03,
          -8.7544e-02,  1.1153e-01,  8.6081e-02, -1.2784e-01,  1.8819e-01,
           3.7037e-02, -6.9132e-02,  1.4595e-01, -2.8013e-01, -1.4944e-01,
           3.9388e-01,  3.9120e-02,  4.5058e-02, -5.2435e-02, -2.5603e-01,
           2.0499e-01, -2.7180e-01, -3.7827e-02, -1.6952e-01, -3.3372e-02,
           3.0087e-01, -2.2302e-01, -2.0453e-01, -1.8494e-01, -2.5842e-01,
          -3.8182e-01, -1.7201e-01],
         [-4.4048e-02, -4.8315e-01,  2.3538e-01, -2.0524e-01,  2.0222e-02,
           1.5564e-02,  5.1210e-02, -5.0715e-04, -1.7651e-01,  1.8733e-01,
           1.7550e-01, -7.9632e-02,  1.0166e-01, -7.3095e-02, -2.2396e-01,
           6.3367e-02, -1.2290e-02,  4.7825e-02, -1.0834e-01, -3.0284e-01,
           1.4989e-01, -1.8448e-01, -1.4345e-01, -3.3287e-03, -2.8770e-01,
           2.2064e-01,  7.3664e-02, -1.8046e-01, -5.0538e-02, -1.3559e-01,
          -1.8727e-01,  5.9240e-02],
         [-1.3852e-01, -7.4899e-01,  2.1602e-02, -7.9327e-02,  2.9486e-02,
          -1.3237e-01,  1.4981e-01,  1.9564e-01, -2.5781e-01,  1.9356e-01,
           3.8547e-04, -9.4310e-03,  2.3460e-01, -3.2317e-01, -1.1499e-01,
           3.7484e-01,  4.5807e-02,  4.3902e-02, -1.7373e-01, -3.9198e-01,
           1.9377e-01, -3.6247e-01, -3.6921e-02, -2.0070e-01, -3.0810e-02,
           2.9799e-01, -2.5431e-01, -2.6631e-01, -2.4252e-01, -2.8890e-01,
          -3.9037e-01, -1.9001e-01],
         [ 2.7791e-01, -6.9737e-01,  1.6120e-02,  3.7296e-02,  2.8171e-01,
           2.4569e-01,  1.5610e-01,  2.5586e-02, -9.1477e-02,  4.1806e-01,
           8.7907e-02, -1.4714e-02,  9.2210e-02, -3.3599e-02, -7.1801e-02,
           3.2448e-01,  3.5932e-02, -1.3317e-01, -2.8184e-01, -3.7704e-01,
           1.5699e-01, -2.5755e-01,  5.4218e-02,  6.9192e-02, -2.8292e-01,
           2.8704e-01, -6.7897e-02, -8.7148e-02, -7.9888e-03, -4.6533e-01,
          -2.8540e-01, -2.7861e-01],
         [ 3.5384e-01, -5.1012e-01, -7.8990e-04, -1.0340e-01,  9.5478e-02,
           3.5930e-01,  1.8336e-01,  7.5280e-02, -1.1134e-01,  3.3040e-01,
          -6.6154e-02, -9.2784e-02, -1.8994e-02,  8.4483e-02, -1.8400e-02,
           1.8544e-01, -1.4619e-01, -1.7711e-01, -1.8099e-02, -2.6815e-01,
           4.0873e-01, -1.2746e-01,  6.5752e-02,  2.0519e-01, -2.3150e-01,
           1.5193e-01,  1.1063e-01, -1.2333e-01,  8.3232e-02, -5.4498e-01,
          -1.7166e-01, -4.4527e-01],
         [ 1.1371e-01, -6.2010e-01,  2.8014e-01, -1.8426e-01,  6.4601e-02,
           2.2957e-01,  7.4561e-02,  7.0857e-02, -3.3141e-01,  3.4868e-01,
           2.7615e-02,  3.1508e-02,  7.4238e-02, -2.9773e-02,  1.0090e-01,
           3.0590e-01, -7.7276e-02, -1.4900e-01,  1.3759e-01, -4.2936e-01,
           4.2112e-01,  1.1576e-01, -5.5904e-02,  1.0265e-01, -2.3085e-01,
           9.7741e-02,  1.4651e-01,  6.7765e-03, -7.5828e-03, -3.0102e-01,
          -2.0798e-01, -4.9848e-01],
         [-1.0667e-01, -9.1542e-01, -1.6713e-01, -1.7917e-01,  1.1454e-02,
          -1.8443e-01,  1.7664e-01,  4.6865e-01, -1.4019e-01,  1.1542e-01,
          -6.2902e-02, -3.6468e-01,  1.5359e-01, -4.2773e-01,  7.6665e-04,
           4.6718e-01, -2.6089e-01,  2.2679e-01,  2.9596e-01, -4.1999e-01,
           1.6212e-01, -1.6574e-01,  3.2015e-01,  4.4927e-02, -1.8298e-01,
           3.3176e-01,  6.8392e-02, -6.4223e-02, -3.0586e-01, -5.7384e-01,
          -4.5825e-01, -5.6810e-01],
         [ 3.1689e-02, -1.0656e+00, -1.4895e-01, -9.7131e-02,  1.4478e-01,
           5.7578e-02,  3.2034e-01,  5.7839e-01,  3.0046e-01,  5.0875e-02,
           2.7135e-02, -3.5883e-01,  1.1091e-01, -1.9840e-01,  2.6867e-01,
           5.4186e-01, -3.3522e-01,  3.2136e-01,  4.3103e-01, -3.1593e-01,
           7.6467e-02,  7.9656e-02,  4.7304e-01,  3.5670e-01, -4.2388e-01,
           3.9993e-01,  4.1705e-01,  2.0574e-01, -2.4230e-01, -5.0404e-01,
          -2.1898e-01, -5.9759e-01],
         [-3.8803e-02, -9.7735e-01,  3.6450e-01, -2.3469e-01,  2.9245e-01,
          -1.5629e-01, -3.1952e-02,  1.7236e-01, -1.9492e-01,  2.0447e-01,
          -7.1688e-02, -3.2394e-01,  1.0188e-01, -3.7270e-01,  1.2609e-01,
           4.2628e-01, -1.1067e-01,  1.2965e-01,  5.0543e-03, -5.6831e-01,
           1.5257e-01, -2.5358e-01,  3.6924e-02,  1.1541e-02, -3.9953e-01,
           4.1082e-01,  3.5208e-02, -7.9126e-02, -1.2746e-02, -4.5962e-01,
          -5.8710e-01, -3.9934e-01],
         [ 3.6120e-01, -6.3894e-01,  2.8969e-02, -4.0192e-02,  4.6633e-02,
           1.9381e-01,  1.7866e-01, -8.8994e-02, -2.5425e-01,  5.0529e-01,
           2.9831e-01, -2.1229e-01,  1.5766e-01, -1.5199e-01, -1.9722e-02,
           4.0398e-01,  2.2507e-01, -1.0863e-01, -2.0907e-01, -3.9437e-01,
          -2.2078e-02, -4.0233e-01, -2.1127e-02,  1.2689e-01, -2.5348e-01,
           2.4464e-01, -1.2929e-01, -7.0834e-02, -4.4722e-02, -4.8688e-01,
          -2.9282e-01, -5.2429e-01],
         [-2.8535e-02, -6.8902e-01,  1.5958e-01, -4.5787e-01,  2.3716e-01,
           5.9714e-02,  1.8012e-01,  3.6787e-02, -3.5617e-01,  2.0085e-01,
           1.0549e-01, -2.3514e-01,  1.2928e-01, -1.1995e-01, -1.9534e-01,
           4.2242e-01, -1.1961e-01, -1.0902e-01, -1.7255e-02, -3.9202e-01,
           2.2965e-01, -4.9218e-01, -7.3735e-02,  2.1421e-01, -4.9054e-01,
           2.6834e-01, -1.2683e-01, -1.0089e-01, -7.6581e-02, -2.5993e-01,
          -3.9325e-01, -2.7927e-01]]], grad_fn=<AddBackward0>) torch.Size([1, 11, 32])
#### arr_b tensor([[[ 2.7062,  0.5123,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 3.0640,  0.6255,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 3.6516,  0.7531,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 4.2091,  0.8777,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766]]]) torch.Size([1, 4, 14])
#### mat_b tensor([[[ 0.1512, -0.7528, -0.2188, -0.0309,  0.4328, -0.2218,  0.0563,
          -0.0682,  0.1827,  0.0576, -0.1031, -0.1502,  0.3065, -0.3459,
          -0.3670,  0.1925, -0.2588,  0.2888,  0.1951, -0.1598, -0.0391,
          -0.5216,  0.2501, -0.3094, -0.4469,  0.6076,  0.1213, -0.2575,
          -0.0680, -0.1250, -0.5365,  0.0678],
         [ 0.0115, -0.7949, -0.2362,  0.1310,  0.3856, -0.1873,  0.1132,
           0.0266,  0.0666,  0.0099, -0.1377, -0.1650,  0.3073, -0.4009,
          -0.1824,  0.0262, -0.2585,  0.3435,  0.0635, -0.2119,  0.0629,
          -0.4612,  0.2634, -0.4076, -0.4425,  0.3891,  0.0769, -0.1947,
          -0.1454, -0.1999, -0.6656, -0.0409],
         [ 0.1565, -0.8400, -0.3758,  0.1290,  0.2945, -0.2640, -0.0892,
          -0.3286,  0.1241,  0.1425, -0.1791, -0.4984,  0.2904, -0.4145,
          -0.4471,  0.1053, -0.2790,  0.0708,  0.1735, -0.2350,  0.2991,
          -0.4066,  0.0635, -0.2120, -0.4362,  0.6832,  0.1167, -0.2093,
           0.2253, -0.1949, -0.8048, -0.0597],
         [ 0.2554, -0.7901, -0.2693,  0.0231,  0.1727, -0.3069,  0.0464,
          -0.2752,  0.1074,  0.1793,  0.1819, -0.5569,  0.2711, -0.5201,
          -0.1840,  0.4108, -0.1219,  0.2245,  0.1884,  0.1857, -0.2154,
          -0.3775,  0.1639, -0.0816, -0.5609,  0.3012, -0.0774, -0.1729,
           0.0748, -0.1915, -0.5517, -0.2713]]], grad_fn=<AddBackward0>) torch.Size([1, 4, 32])
#### arr_b tensor([[[ 2.7062,  0.5123,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 3.0640,  0.6255,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 3.6516,  0.7531,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766],
         [ 4.2091,  0.8777,  0.0422, -0.3765, -0.3766, -0.3766, -0.3766,
          -0.3766, -0.3766, -0.3766, -0.3765, -0.3766, -0.3766, -0.3766]]]) torch.Size([1, 4, 14])
#### mat_b tensor([[[ 0.3027, -0.5166, -0.1005,  0.0264,  0.4106, -0.1231, -0.1510,
          -0.2046,  0.3016, -0.1364, -0.1703, -0.3969,  0.4196, -0.3825,
          -0.1539,  0.0937, -0.0960,  0.3222,  0.2427,  0.1493, -0.4874,
          -0.3134,  0.2761, -0.2344, -0.4847,  0.5380,  0.3403, -0.1429,
           0.1038,  0.0702, -0.4162,  0.0537],
         [ 0.2476, -0.8781, -0.7219,  0.3979,  0.4177, -0.2887, -0.2551,
          -0.2703,  0.3778,  0.0141, -0.2933, -0.4874,  0.3446, -0.5068,
          -0.3876, -0.1274, -0.2705,  0.4977,  0.3279, -0.0581, -0.0436,
          -0.5966,  0.4048, -0.5137, -0.4785,  0.6420,  0.2951, -0.3027,
           0.0801, -0.1329, -0.9647,  0.0956],
         [ 0.2787, -0.8759, -0.3345,  0.1480,  0.6350, -0.4018, -0.0634,
          -0.3835,  0.2916, -0.0330, -0.4647, -0.5697,  0.3586, -0.4139,
          -0.4801,  0.1482, -0.4787,  0.4564,  0.5282, -0.0547, -0.0528,
          -0.5550,  0.3639, -0.4086, -0.7012,  0.8103,  0.1020, -0.4002,
           0.2143,  0.0387, -0.8617, -0.0580],
         [ 0.4721, -1.1704, -0.4670,  0.4080,  0.7731, -0.5369, -0.1225,
          -0.1873,  0.5665, -0.0537, -0.4690, -0.3829,  0.5608, -0.5407,
          -0.1376,  0.0736, -0.5279,  0.4688,  0.3068, -0.0269, -0.3360,
          -0.6605,  0.5564, -0.7232, -0.5608,  0.8715,  0.2489, -0.3549,
           0.1414, -0.1884, -0.9434,  0.0814]]], grad_fn=<AddBackward0>) torch.Size([1, 4, 32])
#### arr_b tensor([[[ 1.6461,  0.2316, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 1.8767,  0.3045, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.2556,  0.3868, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.6150,  0.4672, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.8471,  0.5928, -0.2817, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.2704,  0.5755, -0.2752, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.8558,  0.6120, -0.2765, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 4.3829,  0.6332, -0.2491, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 5.4617,  0.8968, -0.2266, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 5.3630,  0.7951,  0.1857, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415]]]) torch.Size([1, 14, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<AddBackward0>) torch.Size([1, 14, 32])