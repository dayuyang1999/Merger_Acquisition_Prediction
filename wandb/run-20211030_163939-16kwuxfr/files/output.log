
CUDA availability: True
### b: tensor([[[ 4.1410e+04, -1.9563e+04,  3.6361e+04, -1.8110e+04,  4.7706e+03,
           3.9116e+03,  2.2010e+04,  3.8287e+03, -1.2365e+04,  2.1090e+04,
          -2.2920e+04, -2.1204e+04, -3.0570e+04, -1.6119e+04,  6.2493e+04,
           3.0289e+04,  2.4961e+04, -5.4672e+03,  9.9757e+03,  1.9201e+03,
           3.4250e+04,  1.2760e+04,  2.9229e+04,  3.2224e+04,  1.7054e+04,
           2.4100e+04,  1.2886e+04, -3.9599e+04, -3.3347e+04, -3.5596e+04,
          -2.2057e+04, -2.0955e+04],
         [ 4.8854e+04, -3.3087e+04,  5.7076e+04, -3.4455e+04, -1.4668e+04,
          -2.0497e+04,  3.0087e+04,  5.0734e+03, -4.0010e+04,  4.3576e+04,
          -7.8812e+03, -9.5751e+03, -2.8402e+04, -1.7533e+04,  6.7477e+04,
           4.0073e+04,  3.6488e+04,  1.6331e+04, -4.1269e+03,  8.9105e+03,
           7.4163e+04, -8.0534e+03,  5.3936e+04,  6.4545e+04,  4.6074e+04,
           5.6091e+04, -1.2589e+04, -2.1870e+04, -5.2012e+04, -2.2458e+04,
          -2.3647e+04, -8.5570e+03],
         [ 2.3921e+04, -2.8160e+04,  6.6172e+04, -3.5479e+03,  1.8638e+04,
          -2.1630e+04,  8.6415e+02, -2.7585e+04, -2.6680e+04,  5.1770e+04,
           5.4147e+03, -3.7635e+04, -6.7001e+04, -6.1349e+04,  8.3903e+04,
           2.6223e+04,  2.2961e+04, -4.1791e+03, -1.7531e+04,  9.9669e+03,
           3.6909e+04,  2.0023e+04,  4.7429e+04,  9.0826e+04,  8.0591e+04,
           5.2609e+04,  2.0196e+04, -8.6879e+03, -2.2594e+04, -4.0744e+04,
          -5.3498e+04,  2.4279e+04],
         [ 2.3921e+04, -2.8160e+04,  6.6172e+04, -3.5479e+03,  1.8638e+04,
          -2.1630e+04,  8.6415e+02, -2.7585e+04, -2.6680e+04,  5.1770e+04,
           5.4147e+03, -3.7635e+04, -6.7001e+04, -6.1349e+04,  8.3903e+04,
           2.6223e+04,  2.2961e+04, -4.1791e+03, -1.7531e+04,  9.9669e+03,
           3.6909e+04,  2.0023e+04,  4.7429e+04,  9.0826e+04,  8.0591e+04,
           5.2609e+04,  2.0196e+04, -8.6879e+03, -2.2594e+04, -4.0744e+04,
          -5.3498e+04,  2.4279e+04],
         [ 2.3921e+04, -2.8160e+04,  6.6172e+04, -3.5479e+03,  1.8638e+04,
          -2.1630e+04,  8.6415e+02, -2.7585e+04, -2.6680e+04,  5.1770e+04,
           5.4147e+03, -3.7635e+04, -6.7001e+04, -6.1349e+04,  8.3903e+04,
           2.6223e+04,  2.2961e+04, -4.1791e+03, -1.7531e+04,  9.9669e+03,
           3.6909e+04,  2.0023e+04,  4.7429e+04,  9.0826e+04,  8.0591e+04,
           5.2609e+04,  2.0196e+04, -8.6879e+03, -2.2594e+04, -4.0744e+04,
          -5.3498e+04,  2.4279e+04],
         [ 4.9297e+04, -2.5539e+04,  6.0501e+04, -2.5826e+04, -1.0579e+04,
          -6.8872e+03,  1.6361e+02, -3.4506e+03, -1.8443e+04,  5.7522e+04,
          -2.1255e+04, -5.1972e+03, -5.6897e+04, -3.0020e+04,  4.9836e+04,
           7.4458e+04,  1.9621e+04,  2.4573e+04,  2.1410e+04,  7.8454e+02,
           3.1833e+04,  5.2407e+03,  2.9313e+04,  7.0861e+04,  2.0321e+04,
           6.0409e+04,  4.0936e+04, -3.9286e+04, -5.6098e+04, -1.4359e+04,
          -4.6114e+04, -2.1099e+04],
         [ 3.5818e+04, -9.9334e+03,  2.6527e+04, -1.7304e+04,  1.6383e+03,
          -1.5339e+03, -1.6588e+03,  7.5086e+02, -2.1209e+03,  3.7135e+04,
          -3.0582e+04,  9.0712e+03, -1.9748e+04, -7.2471e+04,  5.8498e+04,
           6.9494e+03,  5.7721e+02, -4.1966e+04, -1.2771e+04, -1.8886e+04,
           3.4897e+04, -9.0353e+03,  1.2214e+04,  3.2118e+04, -1.8142e+04,
           1.2676e+04,  1.2197e+04, -1.7356e+04, -2.9664e+04, -5.5677e+04,
          -2.2524e+04, -7.0290e+02],
         [-4.3787e+01, -4.1432e+04,  1.8997e+04, -2.6649e+04, -8.0392e+03,
           1.0285e+04,  2.8402e+04,  8.6854e+03, -1.3641e+04,  6.9622e+04,
           1.7452e+04, -7.5083e+03, -9.2510e+04, -8.8683e+03,  7.1823e+03,
           3.7397e+04,  1.3590e+05,  1.7505e+04,  2.2353e+04,  1.2592e+04,
           7.1249e+04, -3.5187e+04,  5.1875e+04,  5.7709e+04,  4.3521e+04,
          -2.8384e+04,  6.7091e+04, -3.0343e+04, -7.5809e+04, -5.4489e+04,
          -1.1261e+04, -1.1933e+04],
         [ 3.3627e+04, -2.7033e+04,  1.5846e+04, -2.4481e+04,  3.2910e+04,
          -6.4379e+03, -1.9416e+04, -7.5723e+03, -1.9094e+04,  2.0849e+04,
           2.4794e+03, -1.0500e+04, -4.3304e+04, -4.3751e+04,  6.3935e+04,
           7.3171e+04,  2.0721e+04, -2.6076e+04,  1.2103e+04,  1.4601e+03,
           5.3082e+04, -2.1503e+04,  3.6487e+04,  4.5708e+04,  1.0398e+04,
           3.4864e+04,  2.7787e+04, -3.7193e+04, -1.4658e+04, -3.3819e+04,
          -1.6449e+04,  8.6664e+03],
         [ 1.3815e+04, -4.0630e+04,  3.6836e+04, -2.7177e+04, -9.7656e+03,
          -9.7384e+03,  2.6578e+04,  1.3149e+04, -4.7302e+03,  4.3204e+04,
          -1.9694e+04,  9.3600e+03, -3.4313e+04,  8.4298e+03,  7.8476e+03,
           1.4196e+04,  4.3700e+04,  6.3061e+04, -3.7662e+03,  6.1530e+04,
           3.3812e+04, -1.3487e+04,  4.0549e+04,  8.3375e+04,  3.1405e+04,
           4.6803e+04, -3.5450e+03, -2.5908e+04, -5.0085e+04, -1.0335e+04,
           4.9882e+03, -6.4676e+03]]], grad_fn=<IndexSelectBackward>)
### rate: tensor([[ -29043.5332, -151300.6875, -262517.8438, -262516.0312, -262498.7812,
         -144528.2969,  -36491.2148,   49688.1602, -175372.5781,  -76527.8438]],
       grad_fn=<AddBackward0>)
### b: tensor([[[-4.3787e+01, -4.1432e+04,  1.8997e+04,  ..., -5.4489e+04,
          -1.1261e+04, -1.1933e+04],
         [ 3.4538e+04, -6.2781e+04,  5.2855e+04,  ..., -3.9399e+03,
          -5.9544e+04, -1.7984e+04],
         [ 6.1721e+04, -2.1550e+04,  9.3456e+04,  ..., -4.8482e+03,
          -1.2863e+04, -4.2235e+04],
         ...,
         [ 3.5612e+03, -3.6980e+04,  7.8825e+04,  ...,  1.7647e+04,
          -5.1542e+04, -2.7161e+04],
         [ 6.1721e+04, -2.1550e+04,  9.3456e+04,  ..., -4.8482e+03,
          -1.2863e+04, -4.2235e+04],
         [ 1.5168e+04, -2.4562e+04,  2.5973e+04,  ..., -2.5568e+04,
          -5.3927e+04,  5.4533e+03]]], grad_fn=<IndexSelectBackward>)
### rate: tensor([[  49683.9766, -373621.6250, -156555.2812,    3798.6382, -210016.2656,
         -262511.1562,  -16806.0586, -163325.6719, -163811.4062,  -44048.8086,
          -16817.1699, -163807.4531,  -36475.8906, -262497.9688, -315851.5938,
         -144509.7344, -163301.8281, -175357.2031,  -29035.4043,  -29048.1328,
         -209105.9531, -382042.0312,   49685.2617,  -29042.0762, -163808.2656,
          -76572.3203, -315872.7188,  -49589.5820, -209121.0781,    3795.2661,
         -373616.4375,    3812.0359,  -76536.8203, -209162.0469, -163790.4375,
         -175386.5469, -209160.6406, -175360.6562, -209167.5156,   49678.7227,
          -36486.9727, -373599.7812, -151302.0000, -156539.3594,  -76558.2031,
         -175358.1094,  -16805.9316,  -76541.6562, -373610.0938, -144509.9062,
         -262516.9062, -107404.3984, -210026.2031, -382133.3438,  -16808.2969,
         -163338.6562, -210015.7188,  -76531.3750, -151306.6250, -151299.5312,
         -151300.0312, -382122.8438, -315859.5312, -163336.5469,  -16804.4043,
         -382133.8750, -144513.6719,  -29053.0781, -151303.7344,  -49589.4961,
          -36484.0742,  -49602.9688,  -36471.5117, -163326.9219, -163805.3125,
         -373606.4375,  -36477.0977, -156538.2500, -107414.8203,  -29042.0996,
          -44099.3125, -210028.0781, -315854.5000, -144531.2812, -210024.8594,
           49680.8047,  -29057.6699, -144510.4844, -210015.5156,  -44073.5781,
         -107450.5938, -156566.8594,   49684.3750, -210014.8906,  -36478.4766,
         -373609.5000,  -16825.2988, -382123.7500, -156538.8125,  -16804.3105]],
       grad_fn=<AddBackward0>)
### event lambdas:  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], grad_fn=<SigmoidBackward>)
### non event lambdas:  tensor([[1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SigmoidBackward>)
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 195, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 324, in forward
    arr_b_idx_i = torch.stack(arr_b_idx_i).squeeze()
TypeError: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor