
CUDA availability: True
  0%|                                                                                                                                                                                       | 0/496 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 124, in <module>
    main()
  File "main.py", line 120, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 84, in train
    loss.backward() # required_graph = True
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 141, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 50, in _make_grads
    raise RuntimeError("grad can be implicitly created only for scalar outputs")
RuntimeError: grad can be implicitly created only for scalar outputs
#### mat_b tensor([[[ 0.2473, -0.4007, -0.0635, -0.0069, -0.1135,  0.6446, -0.0283,
          -0.3095, -0.4487, -0.4262, -0.4402,  0.1690, -0.0762, -0.2915,
           0.6033, -0.2023,  0.1313,  0.1047,  0.4376,  0.0717, -0.0280,
           0.0531, -0.1932,  0.0010, -0.1607, -0.4163,  0.0324, -0.1770,
          -0.0249, -0.6594,  0.1519, -0.5356],
         [ 0.2477, -0.3990, -0.0627, -0.0024, -0.1164,  0.6514, -0.0206,
          -0.3163, -0.4506, -0.4350, -0.4428,  0.1672, -0.0727, -0.2937,
           0.5958, -0.1984,  0.1324,  0.0995,  0.4357,  0.0730, -0.0219,
           0.0521, -0.1852, -0.0047, -0.1566, -0.4160,  0.0262, -0.1781,
          -0.0285, -0.6673,  0.1570, -0.5377],
         [ 0.2482, -0.3964, -0.0612,  0.0038, -0.1226,  0.6618, -0.0081,
          -0.3284, -0.4549, -0.4471, -0.4485,  0.1661, -0.0661, -0.2975,
           0.5840, -0.1924,  0.1340,  0.0905,  0.4341,  0.0747, -0.0126,
           0.0501, -0.1727, -0.0146, -0.1505, -0.4166,  0.0163, -0.1793,
          -0.0344, -0.6805,  0.1638, -0.5409],
         [ 0.2486, -0.3939, -0.0598,  0.0097, -0.1283,  0.6717,  0.0037,
          -0.3396, -0.4589, -0.4585, -0.4538,  0.1649, -0.0600, -0.3010,
           0.5730, -0.1867,  0.1355,  0.0822,  0.4323,  0.0761, -0.0036,
           0.0483, -0.1611, -0.0238, -0.1446, -0.4171,  0.0070, -0.1805,
          -0.0400, -0.6928,  0.1703, -0.5437],
         [ 0.2461, -0.3874, -0.0632,  0.0145, -0.1318,  0.6810,  0.0169,
          -0.3417, -0.4714, -0.4748, -0.4485,  0.1534, -0.0597, -0.3050,
           0.5656, -0.1844,  0.1395,  0.0739,  0.4324,  0.0720,  0.0059,
           0.0429, -0.1506, -0.0306, -0.1314, -0.4215, -0.0047, -0.1816,
          -0.0339, -0.6970,  0.1825, -0.5478],
         [ 0.2463, -0.3856, -0.0606,  0.0181, -0.1425,  0.6904,  0.0299,
          -0.3574, -0.4790, -0.4814, -0.4584,  0.1577, -0.0497, -0.3093,
           0.5542, -0.1788,  0.1404,  0.0628,  0.4342,  0.0728,  0.0146,
           0.0402, -0.1384, -0.0428, -0.1265, -0.4252, -0.0144, -0.1812,
          -0.0409, -0.7126,  0.1851, -0.5506],
         [ 0.2462, -0.3825, -0.0573,  0.0246, -0.1549,  0.7041,  0.0480,
          -0.3773, -0.4883, -0.4937, -0.4696,  0.1600, -0.0379, -0.3148,
           0.5383, -0.1709,  0.1421,  0.0486,  0.4344,  0.0734,  0.0275,
           0.0366, -0.1215, -0.0581, -0.1187, -0.4287, -0.0281, -0.1815,
          -0.0500, -0.7323,  0.1913, -0.5543],
         [ 0.2463, -0.3806, -0.0537,  0.0300, -0.1659,  0.7157,  0.0629,
          -0.3954, -0.4955, -0.5030, -0.4805,  0.1632, -0.0272, -0.3192,
           0.5248, -0.1641,  0.1433,  0.0366,  0.4344,  0.0742,  0.0384,
           0.0339, -0.1075, -0.0713, -0.1133, -0.4313, -0.0391, -0.1817,
          -0.0592, -0.7494,  0.1958, -0.5569],
         [ 0.2489, -0.3901, -0.0562,  0.0174, -0.1435,  0.6882,  0.0254,
          -0.3634, -0.4701, -0.4733, -0.4674,  0.1682, -0.0456, -0.3078,
           0.5537, -0.1772,  0.1375,  0.0649,  0.4332,  0.0775,  0.0117,
           0.0441, -0.1405, -0.0428, -0.1352, -0.4216, -0.0095, -0.1808,
          -0.0505, -0.7171,  0.1776, -0.5484],
         [ 0.2489, -0.3901, -0.0562,  0.0174, -0.1435,  0.6882,  0.0254,
          -0.3634, -0.4701, -0.4733, -0.4674,  0.1682, -0.0456, -0.3078,
           0.5537, -0.1772,  0.1375,  0.0649,  0.4332,  0.0775,  0.0117,
           0.0441, -0.1405, -0.0428, -0.1352, -0.4216, -0.0095, -0.1808,
          -0.0505, -0.7171,  0.1776, -0.5484],
         [ 0.2489, -0.3901, -0.0562,  0.0174, -0.1435,  0.6882,  0.0254,
          -0.3634, -0.4701, -0.4733, -0.4674,  0.1682, -0.0456, -0.3078,
           0.5537, -0.1772,  0.1375,  0.0649,  0.4332,  0.0775,  0.0117,
           0.0441, -0.1405, -0.0428, -0.1352, -0.4216, -0.0095, -0.1808,
          -0.0505, -0.7171,  0.1776, -0.5484],
         [ 0.2452, -0.3740, -0.0485,  0.0459, -0.1796,  0.7412,  0.0936,
          -0.4258, -0.5060, -0.5328, -0.4945,  0.1568, -0.0118, -0.3271,
           0.4961, -0.1502,  0.1469,  0.0160,  0.4267,  0.0744,  0.0635,
           0.0290, -0.0795, -0.0929, -0.0988, -0.4311, -0.0632, -0.1859,
          -0.0756, -0.7781,  0.2134, -0.5617],
         [ 0.2489, -0.3901, -0.0562,  0.0174, -0.1435,  0.6882,  0.0254,
          -0.3634, -0.4701, -0.4733, -0.4674,  0.1682, -0.0456, -0.3078,
           0.5537, -0.1772,  0.1375,  0.0649,  0.4332,  0.0775,  0.0117,
           0.0441, -0.1405, -0.0428, -0.1352, -0.4216, -0.0095, -0.1808,
          -0.0505, -0.7171,  0.1776, -0.5484],
         [ 0.2507, -0.3831, -0.0415,  0.0439, -0.1767,  0.7351,  0.0805,
          -0.4303, -0.4856, -0.5171, -0.5069,  0.1737, -0.0088, -0.3223,
           0.4991, -0.1489,  0.1405,  0.0245,  0.4238,  0.0838,  0.0558,
           0.0385, -0.0874, -0.0891, -0.1163, -0.4228, -0.0504, -0.1854,
          -0.0905, -0.7807,  0.2005, -0.5559],
         [ 0.2516, -0.3854, -0.0416,  0.0410, -0.1731,  0.7296,  0.0729,
          -0.4249, -0.4802, -0.5101, -0.5056,  0.1766, -0.0117, -0.3202,
           0.5047, -0.1512,  0.1391,  0.0298,  0.4242,  0.0852,  0.0500,
           0.0408, -0.0936, -0.0841, -0.1213, -0.4211, -0.0443, -0.1849,
          -0.0896, -0.7755,  0.1958, -0.5544],
         [ 0.2489, -0.3901, -0.0562,  0.0174, -0.1435,  0.6882,  0.0254,
          -0.3634, -0.4701, -0.4733, -0.4674,  0.1682, -0.0456, -0.3078,
           0.5537, -0.1772,  0.1375,  0.0649,  0.4332,  0.0775,  0.0117,
           0.0441, -0.1405, -0.0428, -0.1352, -0.4216, -0.0095, -0.1808,
          -0.0505, -0.7171,  0.1776, -0.5484],
         [ 0.2489, -0.3901, -0.0562,  0.0174, -0.1435,  0.6882,  0.0254,
          -0.3634, -0.4701, -0.4733, -0.4674,  0.1682, -0.0456, -0.3078,
           0.5537, -0.1772,  0.1375,  0.0649,  0.4332,  0.0775,  0.0117,
           0.0441, -0.1405, -0.0428, -0.1352, -0.4216, -0.0095, -0.1808,
          -0.0505, -0.7171,  0.1776, -0.5484],
         [ 0.2489, -0.3901, -0.0562,  0.0174, -0.1435,  0.6882,  0.0254,
          -0.3634, -0.4701, -0.4733, -0.4674,  0.1682, -0.0456, -0.3078,
           0.5537, -0.1772,  0.1375,  0.0649,  0.4332,  0.0775,  0.0117,
           0.0441, -0.1405, -0.0428, -0.1352, -0.4216, -0.0095, -0.1808,
          -0.0505, -0.7171,  0.1776, -0.5484],
         [ 0.2535, -0.3904, -0.0443,  0.0347, -0.1563,  0.7128,  0.0492,
          -0.4019, -0.4625, -0.4953, -0.4943,  0.1764, -0.0257, -0.3131,
           0.5223, -0.1592,  0.1364,  0.0488,  0.4216,  0.0882,  0.0333,
           0.0473, -0.1137, -0.0645, -0.1338, -0.4133, -0.0264, -0.1854,
          -0.0828, -0.7541,  0.1869, -0.5501],
         [ 0.2489, -0.3915, -0.0553,  0.0131, -0.1492,  0.6857,  0.0248,
          -0.3668, -0.4746, -0.4646, -0.4722,  0.1762, -0.0416, -0.3083,
           0.5556, -0.1783,  0.1366,  0.0625,  0.4389,  0.0767,  0.0095,
           0.0431, -0.1416, -0.0455, -0.1372, -0.4263, -0.0082, -0.1781,
          -0.0506, -0.7194,  0.1711, -0.5482],
         [ 0.2489, -0.3909, -0.0557,  0.0149, -0.1468,  0.6867,  0.0250,
          -0.3654, -0.4728, -0.4682, -0.4702,  0.1729, -0.0433, -0.3081,
           0.5548, -0.1778,  0.1369,  0.0635,  0.4366,  0.0770,  0.0104,
           0.0435, -0.1412, -0.0444, -0.1364, -0.4244, -0.0087, -0.1792,
          -0.0506, -0.7184,  0.1738, -0.5483],
         [ 0.2489, -0.3912, -0.0555,  0.0141, -0.1479,  0.6862,  0.0249,
          -0.3661, -0.4736, -0.4666, -0.4711,  0.1744, -0.0425, -0.3082,
           0.5552, -0.1780,  0.1368,  0.0630,  0.4377,  0.0769,  0.0100,
           0.0433, -0.1414, -0.0449, -0.1368, -0.4252, -0.0085, -0.1787,
          -0.0506, -0.7189,  0.1726, -0.5483],
         [ 0.2489, -0.3913, -0.0554,  0.0136, -0.1485,  0.6860,  0.0248,
          -0.3664, -0.4741, -0.4656, -0.4716,  0.1752, -0.0421, -0.3083,
           0.5554, -0.1782,  0.1367,  0.0628,  0.4383,  0.0768,  0.0098,
           0.0432, -0.1415, -0.0452, -0.1370, -0.4257, -0.0084, -0.1784,
          -0.0506, -0.7191,  0.1719, -0.5483],
         [ 0.2474, -0.4002, -0.0643, -0.0041, -0.1076,  0.6455, -0.0296,
          -0.3048, -0.4442, -0.4321, -0.4349,  0.1622, -0.0808, -0.2903,
           0.6036, -0.2021,  0.1320,  0.1081,  0.4327,  0.0721, -0.0275,
           0.0543, -0.1938,  0.0047, -0.1598, -0.4121,  0.0328, -0.1789,
          -0.0240, -0.6553,  0.1569, -0.5354]]], grad_fn=<AddBackward0>) torch.Size([1, 24, 32])