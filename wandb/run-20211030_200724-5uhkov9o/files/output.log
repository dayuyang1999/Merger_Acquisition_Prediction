
CUDA availability: True
  0%|â–                                                                                                                                                                                               | 1/496 [00:00<01:52,  4.41it/s]
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 199, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)*1000
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 359, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
batch number:  0
#### arr_b tensor([[[ 1.8379,  0.2894, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.2110,  0.3705, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.5651,  0.4496, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.7936,  0.5734, -0.2880, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2106,  0.5563, -0.2816, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.7871,  0.5923, -0.2828, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 4.3062,  0.6131, -0.2558, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.3688,  0.8728, -0.2337, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.2716,  0.7726,  0.1724, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.0517,  0.7243,  0.2206, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 4.3124,  0.7035,  0.3014, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3322, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3955, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3666, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3503, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 1.5588,  0.3374, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469]]]) torch.Size([1, 23, 14])
#### mat_b tensor([[[ 3.1350e-01, -3.7800e-01, -2.3769e-01, -5.3691e-01, -2.3926e-01,
           1.1890e-01,  3.0942e-01,  1.2271e-01,  2.5626e-01, -1.1412e-01,
           2.1509e-01,  1.9252e-01, -4.6957e-02,  2.2894e-01, -1.2399e-01,
           2.7781e-02,  7.6175e-02,  1.5178e-01,  4.2724e-01, -1.2185e-01,
           3.1223e-01,  1.7091e-01, -7.1997e-02, -3.5036e-01,  2.7624e-01,
           2.4670e-02,  4.5118e-01, -3.0126e-02,  1.3236e-02,  2.0075e-01,
          -3.6387e-01, -3.4018e-01],
         [ 3.0046e-01, -3.9535e-01, -2.4535e-01, -5.4378e-01, -2.4035e-01,
           1.2647e-01,  3.2107e-01,  1.2685e-01,  2.6581e-01, -1.1830e-01,
           2.1676e-01,  2.1000e-01, -4.0424e-02,  2.2362e-01, -1.2268e-01,
           1.9850e-02,  7.7201e-02,  1.5797e-01,  4.4311e-01, -1.2494e-01,
           3.0205e-01,  1.6335e-01, -6.8289e-02, -3.4821e-01,  2.7923e-01,
           2.9927e-02,  4.4073e-01, -3.3463e-02,  9.8768e-03,  1.9526e-01,
          -3.6940e-01, -3.3392e-01],
         [ 2.8822e-01, -4.1158e-01, -2.5248e-01, -5.5022e-01, -2.4129e-01,
           1.3337e-01,  3.3201e-01,  1.3070e-01,  2.7480e-01, -1.2217e-01,
           2.1826e-01,  2.2634e-01, -3.4356e-02,  2.1869e-01, -1.2138e-01,
           1.2549e-02,  7.8243e-02,  1.6383e-01,  4.5789e-01, -1.2782e-01,
           2.9242e-01,  1.5635e-01, -6.4748e-02, -3.4631e-01,  2.8191e-01,
           3.4722e-02,  4.3096e-01, -3.6509e-02,  6.7368e-03,  1.9005e-01,
          -3.7463e-01, -3.2801e-01],
         [ 2.7234e-01, -4.2180e-01, -2.5598e-01, -5.5065e-01, -2.4341e-01,
           1.3870e-01,  3.4635e-01,  1.3758e-01,  2.8167e-01, -1.2929e-01,
           2.1732e-01,  2.3121e-01, -2.7943e-02,  2.0820e-01, -1.1949e-01,
           5.3484e-03,  7.6110e-02,  1.6715e-01,  4.7111e-01, -1.3429e-01,
           2.7778e-01,  1.5440e-01, -6.5650e-02, -3.3721e-01,  2.8287e-01,
           4.2443e-02,  4.2401e-01, -3.6086e-02,  1.2742e-03,  1.7867e-01,
          -3.7550e-01, -3.1973e-01],
         [ 2.5905e-01, -4.4389e-01, -2.6099e-01, -5.5971e-01, -2.4461e-01,
           1.4598e-01,  3.5093e-01,  1.4034e-01,  2.8742e-01, -1.3401e-01,
           2.1989e-01,  2.4996e-01, -2.3804e-02,  2.0134e-01, -1.2296e-01,
          -2.1368e-03,  7.8242e-02,  1.7504e-01,  4.8787e-01, -1.3629e-01,
           2.6747e-01,  1.4843e-01, -6.1481e-02, -3.3182e-01,  2.8367e-01,
           4.7711e-02,  4.1393e-01, -4.0310e-02,  1.5963e-03,  1.7231e-01,
          -3.8173e-01, -3.1049e-01],
         [ 2.4063e-01, -4.7210e-01, -2.6909e-01, -5.7117e-01, -2.4604e-01,
           1.5558e-01,  3.6086e-01,  1.4486e-01,  2.9742e-01, -1.4012e-01,
           2.2272e-01,  2.7499e-01, -1.6966e-02,  1.9258e-01, -1.2523e-01,
          -1.2277e-02,  8.0854e-02,  1.8522e-01,  5.1044e-01, -1.3955e-01,
           2.5283e-01,  1.3991e-01, -5.5755e-02, -3.2615e-01,  2.8543e-01,
           5.4682e-02,  3.9990e-01, -4.5406e-02,  1.6449e-04,  1.6364e-01,
          -3.8994e-01, -2.9896e-01],
         [ 2.2569e-01, -4.9701e-01, -2.7614e-01, -5.8184e-01, -2.4709e-01,
           1.6348e-01,  3.6811e-01,  1.4803e-01,  3.0583e-01, -1.4470e-01,
           2.2536e-01,  2.9747e-01, -1.1537e-02,  1.8585e-01, -1.2747e-01,
          -2.0370e-02,  8.3690e-02,  1.9421e-01,  5.2952e-01, -1.4169e-01,
           2.4111e-01,  1.3258e-01, -5.0139e-02, -3.2240e-01,  2.8678e-01,
           5.9880e-02,  3.8792e-01, -5.0100e-02, -4.3068e-04,  1.5703e-01,
          -3.9734e-01, -2.8931e-01],
         [ 2.6577e-01, -4.4561e-01, -2.6235e-01, -5.6390e-01, -2.4327e-01,
           1.4558e-01,  3.4422e-01,  1.3623e-01,  2.8668e-01, -1.2990e-01,
           2.2183e-01,  2.5658e-01, -2.6007e-02,  2.0786e-01, -1.2416e-01,
          -8.9824e-05,  8.1095e-02,  1.7599e-01,  4.8538e-01, -1.3206e-01,
           2.7486e-01,  1.4567e-01, -5.8104e-02, -3.3907e-01,  2.8442e-01,
           4.3530e-02,  4.1394e-01, -4.2870e-02,  4.9720e-03,  1.7944e-01,
          -3.8456e-01, -3.1412e-01],
         [ 2.6577e-01, -4.4561e-01, -2.6235e-01, -5.6390e-01, -2.4327e-01,
           1.4558e-01,  3.4422e-01,  1.3623e-01,  2.8668e-01, -1.2990e-01,
           2.2183e-01,  2.5658e-01, -2.6007e-02,  2.0786e-01, -1.2416e-01,
          -8.9824e-05,  8.1095e-02,  1.7599e-01,  4.8538e-01, -1.3206e-01,
           2.7486e-01,  1.4567e-01, -5.8104e-02, -3.3907e-01,  2.8442e-01,
           4.3530e-02,  4.1394e-01, -4.2870e-02,  4.9720e-03,  1.7944e-01,
          -3.8456e-01, -3.1412e-01],
         [ 2.6577e-01, -4.4561e-01, -2.6235e-01, -5.6390e-01, -2.4327e-01,
           1.4558e-01,  3.4422e-01,  1.3623e-01,  2.8668e-01, -1.2990e-01,
           2.2183e-01,  2.5658e-01, -2.6007e-02,  2.0786e-01, -1.2416e-01,
          -8.9824e-05,  8.1095e-02,  1.7599e-01,  4.8538e-01, -1.3206e-01,
           2.7486e-01,  1.4567e-01, -5.8104e-02, -3.3907e-01,  2.8442e-01,
           4.3530e-02,  4.1394e-01, -4.2870e-02,  4.9720e-03,  1.7944e-01,
          -3.8456e-01, -3.1412e-01],
         [ 1.9535e-01, -5.3986e-01, -2.9433e-01, -5.9994e-01, -2.4903e-01,
           1.7794e-01,  3.9508e-01,  1.5718e-01,  3.3016e-01, -1.5227e-01,
           2.2854e-01,  3.3999e-01,  3.3444e-03,  1.7471e-01, -1.2331e-01,
          -3.6012e-02,  8.7853e-02,  2.1028e-01,  5.6596e-01, -1.4793e-01,
           2.1533e-01,  1.1684e-01, -3.8605e-02, -3.2103e-01,  2.9206e-01,
           6.9642e-02,  3.6317e-01, -5.7189e-02, -7.8649e-03,  1.4396e-01,
          -4.1156e-01, -2.7392e-01],
         [ 2.6577e-01, -4.4561e-01, -2.6235e-01, -5.6390e-01, -2.4327e-01,
           1.4558e-01,  3.4422e-01,  1.3623e-01,  2.8668e-01, -1.2990e-01,
           2.2183e-01,  2.5658e-01, -2.6007e-02,  2.0786e-01, -1.2416e-01,
          -8.9824e-05,  8.1095e-02,  1.7599e-01,  4.8538e-01, -1.3206e-01,
           2.7486e-01,  1.4567e-01, -5.8104e-02, -3.3907e-01,  2.8442e-01,
           4.3530e-02,  4.1394e-01, -4.2870e-02,  4.9720e-03,  1.7944e-01,
          -3.8456e-01, -3.1412e-01],
         [ 2.1169e-01, -5.3480e-01, -2.9643e-01, -6.0457e-01, -2.4658e-01,
           1.7552e-01,  3.8377e-01,  1.4845e-01,  3.2774e-01, -1.4340e-01,
           2.3098e-01,  3.4646e-01, -1.8204e-03,  1.8927e-01, -1.2235e-01,
          -2.9444e-02,  9.2296e-02,  2.0898e-01,  5.5566e-01, -1.4046e-01,
           2.3287e-01,  1.1236e-01, -3.3488e-02, -3.3813e-01,  2.9442e-01,
           5.9666e-02,  3.6638e-01, -6.0917e-02, -2.6746e-03,  1.5892e-01,
          -4.1479e-01, -2.8448e-01],
         [ 2.1975e-01, -5.2536e-01, -2.9366e-01, -6.0152e-01, -2.4575e-01,
           1.7247e-01,  3.7808e-01,  1.4580e-01,  3.2346e-01, -1.4061e-01,
           2.3062e-01,  3.3928e-01, -4.9337e-03,  1.9373e-01, -1.2241e-01,
          -2.5798e-02,  9.1998e-02,  2.0565e-01,  5.4728e-01, -1.3827e-01,
           2.4014e-01,  1.1443e-01, -3.5046e-02, -3.4111e-01,  2.9401e-01,
           5.6519e-02,  3.7133e-01, -5.9960e-02, -1.0628e-03,  1.6372e-01,
          -4.1256e-01, -2.8932e-01],
         [ 2.6577e-01, -4.4561e-01, -2.6235e-01, -5.6390e-01, -2.4327e-01,
           1.4558e-01,  3.4422e-01,  1.3623e-01,  2.8668e-01, -1.2990e-01,
           2.2183e-01,  2.5658e-01, -2.6007e-02,  2.0786e-01, -1.2416e-01,
          -8.9824e-05,  8.1095e-02,  1.7599e-01,  4.8538e-01, -1.3206e-01,
           2.7486e-01,  1.4567e-01, -5.8104e-02, -3.3907e-01,  2.8442e-01,
           4.3530e-02,  4.1394e-01, -4.2870e-02,  4.9720e-03,  1.7944e-01,
          -3.8456e-01, -3.1412e-01],
         [ 2.6577e-01, -4.4561e-01, -2.6235e-01, -5.6390e-01, -2.4327e-01,
           1.4558e-01,  3.4422e-01,  1.3623e-01,  2.8668e-01, -1.2990e-01,
           2.2183e-01,  2.5658e-01, -2.6007e-02,  2.0786e-01, -1.2416e-01,
          -8.9824e-05,  8.1095e-02,  1.7599e-01,  4.8538e-01, -1.3206e-01,
           2.7486e-01,  1.4567e-01, -5.8104e-02, -3.3907e-01,  2.8442e-01,
           4.3530e-02,  4.1394e-01, -4.2870e-02,  4.9720e-03,  1.7944e-01,
          -3.8456e-01, -3.1412e-01],
         [ 2.6577e-01, -4.4561e-01, -2.6235e-01, -5.6390e-01, -2.4327e-01,
           1.4558e-01,  3.4422e-01,  1.3623e-01,  2.8668e-01, -1.2990e-01,
           2.2183e-01,  2.5658e-01, -2.6007e-02,  2.0786e-01, -1.2416e-01,
          -8.9824e-05,  8.1095e-02,  1.7599e-01,  4.8538e-01, -1.3206e-01,
           2.7486e-01,  1.4567e-01, -5.8104e-02, -3.3907e-01,  2.8442e-01,
           4.3530e-02,  4.1394e-01, -4.2870e-02,  4.9720e-03,  1.7944e-01,
          -3.8456e-01, -3.1412e-01],
         [ 2.4478e-01, -4.8912e-01, -2.8635e-01, -5.8808e-01, -2.4302e-01,
           1.6164e-01,  3.6787e-01,  1.3981e-01,  3.1372e-01, -1.3141e-01,
           2.2776e-01,  3.1205e-01, -1.2277e-02,  2.0836e-01, -1.1754e-01,
          -1.4198e-02,  8.9348e-02,  1.9316e-01,  5.1904e-01, -1.3306e-01,
           2.6119e-01,  1.2130e-01, -4.1180e-02, -3.5237e-01,  2.9388e-01,
           4.6950e-02,  3.8740e-01, -5.4633e-02, -3.2628e-04,  1.7827e-01,
          -4.0398e-01, -3.0685e-01],
         [ 2.6631e-01, -4.5038e-01, -2.5813e-01, -5.6593e-01, -2.4372e-01,
           1.4532e-01,  3.3380e-01,  1.3417e-01,  2.8029e-01, -1.3078e-01,
           2.2306e-01,  2.5639e-01, -2.9605e-02,  2.0565e-01, -1.3099e-01,
           6.5321e-04,  8.2009e-02,  1.7731e-01,  4.8560e-01, -1.3068e-01,
           2.7570e-01,  1.4828e-01, -5.8321e-02, -3.3399e-01,  2.8160e-01,
           4.3793e-02,  4.1539e-01, -4.3969e-02,  1.0118e-02,  1.7853e-01,
          -3.8463e-01, -3.1059e-01],
         [ 2.6609e-01, -4.4842e-01, -2.5987e-01, -5.6509e-01, -2.4354e-01,
           1.4543e-01,  3.3810e-01,  1.3502e-01,  2.8292e-01, -1.3042e-01,
           2.2256e-01,  2.5647e-01, -2.8123e-02,  2.0655e-01, -1.2817e-01,
           3.4671e-04,  8.1632e-02,  1.7677e-01,  4.8552e-01, -1.3125e-01,
           2.7536e-01,  1.4721e-01, -5.8234e-02, -3.3609e-01,  2.8277e-01,
           4.3685e-02,  4.1479e-01, -4.3518e-02,  7.9979e-03,  1.7891e-01,
          -3.8460e-01, -3.1204e-01],
         [ 2.6619e-01, -4.4932e-01, -2.5907e-01, -5.6547e-01, -2.4362e-01,
           1.4538e-01,  3.3614e-01,  1.3464e-01,  2.8172e-01, -1.3059e-01,
           2.2279e-01,  2.5643e-01, -2.8799e-02,  2.0614e-01, -1.2946e-01,
           4.8651e-04,  8.1804e-02,  1.7702e-01,  4.8556e-01, -1.3099e-01,
           2.7551e-01,  1.4770e-01, -5.8274e-02, -3.3513e-01,  2.8224e-01,
           4.3734e-02,  4.1506e-01, -4.3724e-02,  8.9658e-03,  1.7873e-01,
          -3.8461e-01, -3.1138e-01],
         [ 2.6624e-01, -4.4982e-01, -2.5862e-01, -5.6569e-01, -2.4367e-01,
           1.4535e-01,  3.3503e-01,  1.3442e-01,  2.8104e-01, -1.3068e-01,
           2.2292e-01,  2.5641e-01, -2.9181e-02,  2.0591e-01, -1.3018e-01,
           5.6546e-04,  8.1901e-02,  1.7716e-01,  4.8558e-01, -1.3084e-01,
           2.7560e-01,  1.4798e-01, -5.8297e-02, -3.3459e-01,  2.8194e-01,
           4.3762e-02,  4.1522e-01, -4.3840e-02,  9.5117e-03,  1.7864e-01,
          -3.8462e-01, -3.1101e-01],
         [ 3.2290e-01, -3.6149e-01, -2.3506e-01, -5.3032e-01, -2.3802e-01,
           1.1336e-01,  3.0821e-01,  1.2103e-01,  2.5369e-01, -1.1036e-01,
           2.1286e-01,  1.7951e-01, -4.9274e-02,  2.3451e-01, -1.2004e-01,
           3.3246e-02,  7.4841e-02,  1.4611e-01,  4.1520e-01, -1.2044e-01,
           3.1941e-01,  1.7473e-01, -7.4644e-02, -3.5566e-01,  2.7598e-01,
           2.0420e-02,  4.5805e-01, -2.6801e-02,  1.1999e-02,  2.0555e-01,
          -3.5948e-01, -3.4749e-01]]], grad_fn=<AddBackward0>) torch.Size([1, 23, 32])
batch number:  1
#### arr_b tensor([[[ 1.8379,  0.2894, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.2110,  0.3705, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.5651,  0.4496, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.7936,  0.5734, -0.2880, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2106,  0.5563, -0.2816, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.7871,  0.5923, -0.2828, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 4.3062,  0.6131, -0.2558, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.3688,  0.8728, -0.2337, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.2716,  0.7726,  0.1724, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.0517,  0.7243,  0.2206, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 4.3124,  0.7035,  0.3014, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3322, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3955, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3666, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3503, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 1.5588,  0.3374, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469]]]) torch.Size([1, 23, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<AddBackward0>) torch.Size([1, 23, 32])