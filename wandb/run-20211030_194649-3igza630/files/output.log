
CUDA availability: True
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 200, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)*1000
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 360, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
#### arr_b tensor([[[ 1.8379,  0.2894, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.2110,  0.3705, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.5651,  0.4496, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 2.7936,  0.5734, -0.2880, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2106,  0.5563, -0.2816, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.7871,  0.5923, -0.2828, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 4.3062,  0.6131, -0.2558, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.3688,  0.8728, -0.2337, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.2716,  0.7726,  0.1724, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 5.0517,  0.7243,  0.2206, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.4857, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 4.3124,  0.7035,  0.3014, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3322, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3955, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3666, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 3.2400,  0.3503, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469],
         [ 1.5588,  0.3374, -0.0809, -0.3468, -0.3469, -0.3469, -0.3469,
          -0.3469, -0.3469, -0.3469, -0.3468, -0.3469, -0.3469, -0.3469]]]) torch.Size([1, 23, 14])
#### mat_b tensor([[[-6.0044e-04,  4.5626e-01,  3.1969e-02, -2.0814e-04,  2.3753e-01,
          -1.2017e-03, -1.7372e-03, -1.5770e-03, -9.9187e-04,  8.6720e-02,
           1.3097e-02, -2.3353e-04, -1.0168e-03, -3.3019e-03, -1.6844e-03,
          -7.0279e-04, -2.5840e-03,  5.6699e-02, -2.7885e-03,  5.7986e-02,
           1.5949e-01, -1.9554e-03, -2.2163e-03, -1.6875e-03,  8.7973e-02,
          -1.0675e-03, -3.2939e-03,  7.1121e-02,  9.3452e-02, -2.6628e-04,
          -2.4833e-03,  1.4096e-01],
         [ 1.4898e-02,  1.5973e-01, -7.3328e-04, -8.6252e-04,  2.4693e-01,
           3.4498e-03, -1.4503e-03,  5.1852e-02, -7.1714e-04,  5.0661e-02,
           9.4591e-02, -1.9564e-04, -9.9117e-04, -4.1802e-04, -1.3910e-03,
          -3.8747e-04, -1.2369e-03, -7.9005e-04,  2.7670e-03,  6.7624e-02,
           3.3237e-02, -2.1951e-03, -1.1813e-03, -3.0306e-03,  1.6250e-02,
           2.3699e-02, -1.8374e-03,  1.0553e-01, -3.7406e-04,  1.2518e-01,
          -3.1663e-03,  2.9021e-02],
         [-1.3699e-03,  2.2051e-01,  8.6096e-02, -5.7658e-04,  3.7780e-01,
           4.5283e-02, -1.8778e-03,  2.1266e-02, -5.7289e-04,  3.7233e-01,
           2.3197e-01, -4.4340e-04,  1.0861e-01, -6.8215e-04, -1.2261e-03,
          -1.2331e-04, -1.3263e-03,  1.0475e-01,  3.6117e-02,  1.9916e-01,
           1.1988e-01, -1.5900e-03, -2.2976e-03, -2.4704e-03,  2.5085e-01,
          -2.4300e-04, -1.9625e-03,  8.8080e-02,  1.4203e-01,  1.5117e-01,
          -2.3380e-03, -2.9431e-04],
         [ 9.6799e-02,  4.5025e-01,  1.2246e-01,  1.1522e-01,  5.7020e-01,
          -1.5929e-03, -2.8922e-03, -1.6208e-03, -1.4919e-04,  4.0128e-01,
           1.2681e-01,  6.4727e-02,  1.3975e-01, -7.9097e-04, -1.8944e-03,
          -2.9023e-03, -3.3194e-03,  2.7607e-01, -1.2445e-03,  1.6604e-01,
           2.3163e-01, -3.4812e-03, -2.6907e-03, -1.1796e-03,  2.4897e-01,
           1.1214e-01, -5.4123e-03, -1.7507e-03,  1.4575e-01,  7.3302e-02,
          -1.5762e-03,  3.1047e-01],
         [ 9.3146e-02,  4.7090e-01,  8.1183e-02,  1.0863e-01,  6.6691e-01,
          -1.9852e-03, -2.1621e-03, -2.5801e-03, -2.9535e-04,  2.9237e-01,
           2.2511e-01, -7.9098e-04,  1.7705e-01,  6.5846e-03, -1.8918e-03,
          -3.4950e-03, -3.2921e-03,  1.6221e-01, -2.7119e-03,  1.5626e-01,
           3.5575e-01, -5.4703e-03, -1.8897e-03, -3.4808e-03,  9.6279e-02,
           5.4105e-02, -5.4413e-03, -1.6235e-03,  6.5619e-02,  1.9197e-01,
          -2.6470e-03,  3.7988e-01],
         [ 2.4696e-01,  6.1815e-01,  1.1118e-01,  2.6862e-01,  6.7776e-01,
          -1.2038e-03, -5.3386e-03, -8.6909e-04, -6.0792e-04,  5.3661e-01,
           1.6414e-01,  1.2291e-01,  2.8610e-01, -1.6266e-03, -1.4743e-03,
          -4.3381e-03, -3.5353e-03,  2.5047e-01, -2.6684e-03,  3.1958e-01,
           4.1987e-02, -5.0343e-03, -2.0485e-03, -1.5107e-03,  1.3292e-01,
           3.1177e-02, -6.6623e-03, -4.0278e-04,  1.9161e-01,  2.2959e-01,
          -8.3173e-04,  3.8041e-01],
         [-1.0688e-03,  5.4885e-01,  5.3312e-01, -3.9153e-04,  7.5082e-01,
          -3.6139e-04, -3.5865e-03, -3.0694e-03,  1.5483e-01,  5.3232e-01,
          -1.2823e-04, -1.3364e-03,  9.4215e-02, -1.8108e-03, -6.9941e-04,
          -6.8346e-03, -4.0380e-03,  1.0679e-01, -4.2465e-03,  1.1347e-01,
           3.0864e-01, -3.1906e-03, -3.1622e-03, -1.5284e-03,  1.0004e-01,
           8.9192e-02, -4.1829e-03, -1.4103e-03,  3.1716e-01,  1.5214e-01,
          -2.4643e-03,  2.9577e-01],
         [-3.0759e-04,  3.9513e-01,  1.4262e-02, -1.0083e-03,  5.4449e-01,
           1.1947e-01, -2.6470e-03, -1.4836e-04, -9.0376e-04,  3.8702e-01,
           3.0432e-01, -3.0727e-04,  6.6597e-02,  2.8944e-02, -2.1985e-03,
          -1.1482e-03, -1.3311e-03,  1.2542e-01,  2.3303e-02,  2.8075e-01,
           1.3367e-01, -2.7497e-03, -2.8306e-03, -3.9316e-03,  1.6248e-01,
           1.0594e-02, -3.2989e-03,  1.1411e-02,  1.6091e-01,  2.1958e-01,
          -3.2915e-03,  1.3259e-01],
         [-3.0461e-04,  4.8136e-01, -2.2445e-03, -1.2991e-03,  4.5713e-02,
          -8.5822e-04,  4.1767e-02,  1.9536e-01, -3.6920e-03,  1.9296e-01,
          -6.1913e-04, -2.0132e-03, -9.9121e-04, -2.1569e-04, -3.7560e-03,
          -8.2018e-04, -1.3874e-03,  8.5041e-02, -1.4006e-03, -3.4968e-04,
          -2.7691e-04, -1.0925e-03, -3.0398e-03, -2.9889e-03, -2.2072e-04,
           6.9357e-02, -4.0512e-03, -6.0778e-05,  4.7669e-02,  9.0817e-02,
          -4.2954e-03,  2.3850e-01],
         [ 5.5415e-02,  5.8120e-01,  5.7219e-02,  9.0022e-02,  4.8772e-01,
          -1.3388e-03, -2.4267e-03, -8.4647e-04, -8.1029e-04,  4.5349e-01,
           1.5017e-01, -1.6442e-03,  4.4049e-02,  2.6092e-02, -1.4633e-03,
          -4.3735e-03, -3.1962e-03, -1.9807e-05, -1.4528e-03, -3.2442e-04,
           2.0137e-01, -4.5990e-03, -1.1043e-03, -1.1981e-03,  7.5281e-03,
           2.0326e-01, -3.9120e-03, -2.7509e-03,  1.2618e-01,  1.3272e-01,
          -7.1954e-04,  3.3186e-01],
         [-2.3922e-03,  8.4143e-01,  2.4168e-01, -6.3089e-04,  8.4871e-01,
          -3.3865e-03, -2.2150e-03, -1.9212e-03, -3.2877e-04,  5.5945e-01,
          -1.5584e-03, -4.9062e-04, -1.5201e-03, -1.5678e-03, -4.9016e-03,
          -4.8691e-03, -5.5720e-03,  4.0104e-01, -3.3570e-03, -6.1124e-04,
           8.9133e-03, -5.6267e-03, -4.3858e-03,  4.1224e-02,  3.7724e-01,
           2.9711e-01, -6.1012e-03, -1.8651e-03,  8.7557e-02,  2.0178e-01,
          -2.4821e-03,  7.5734e-01],
         [ 1.1548e-01,  6.3944e-01, -2.7751e-04,  1.7235e-02,  3.8673e-01,
          -1.4835e-03, -1.8133e-03, -3.8396e-04, -2.0941e-03,  3.3281e-01,
          -1.2750e-03, -8.6957e-05,  1.1226e-02, -1.2791e-03, -3.3988e-03,
          -1.2168e-03, -3.8310e-03,  2.3193e-01, -4.1165e-03,  8.0288e-02,
           2.3214e-02, -3.7074e-03, -2.9353e-03, -8.2589e-04,  2.1834e-01,
           1.4820e-01, -5.6359e-03, -1.4803e-03,  3.9515e-02,  1.3438e-01,
          -2.4654e-03,  3.8841e-01],
         [ 4.0297e-02,  5.0880e-01,  3.8120e-01,  1.4061e-01,  9.1401e-01,
          -2.7248e-03, -3.9329e-03, -3.4660e-04,  1.1584e-03,  6.6916e-01,
           5.6347e-01,  2.4003e-02,  2.7901e-01, -1.0814e-03,  1.5200e-02,
          -9.3346e-03, -5.1416e-03,  1.6584e-01, -1.4288e-03,  1.3128e-01,
           2.4755e-02, -6.1422e-03, -9.0864e-04, -4.7129e-04, -2.3398e-05,
           1.4632e-01, -8.1444e-03, -2.2486e-03,  2.4647e-01,  2.2703e-01,
          -1.3076e-03,  2.6638e-01],
         [ 3.1798e-01,  7.0658e-01,  7.0321e-02,  3.0625e-01,  1.0002e+00,
          -1.3297e-03, -1.2227e-03, -3.2998e-03, -1.7299e-03,  5.7340e-01,
           2.2645e-01, -4.9674e-04,  2.3186e-01,  3.7911e-02, -3.6125e-03,
          -5.8825e-03, -4.2883e-03,  1.6457e-01, -4.2948e-03,  4.0015e-01,
           3.1032e-01, -7.3171e-03, -2.7413e-03, -4.4401e-03,  1.3726e-01,
           2.0364e-01, -8.8428e-03, -1.5125e-03, -5.7508e-04,  3.6213e-01,
          -3.4539e-03,  7.0544e-01],
         [ 2.5374e-03,  4.0196e-01, -6.7703e-04, -2.4416e-04,  5.3456e-01,
           3.5160e-02, -2.5053e-03,  8.8372e-04, -1.1319e-03,  3.3727e-01,
           2.9578e-01, -2.6806e-04, -4.0202e-04, -1.4571e-03, -2.6650e-03,
          -2.2968e-03, -1.5030e-03,  6.5579e-02,  1.0785e-01,  2.6305e-01,
           6.1435e-02, -3.5009e-03, -1.4743e-03, -3.3530e-03,  7.1264e-02,
          -3.3640e-04, -3.6616e-03,  1.1158e-01,  2.2904e-01,  2.0448e-01,
          -3.7855e-03,  8.7671e-02],
         [-2.2916e-03,  2.7656e-01,  1.4412e-01, -2.0189e-03,  6.5662e-01,
           1.6454e-01, -2.4034e-03,  3.7829e-02, -4.3144e-04,  5.2059e-01,
           2.5411e-01, -3.9753e-04,  8.4765e-02,  7.3005e-02, -1.8565e-03,
          -3.5829e-03, -8.9698e-04,  2.1235e-02,  7.0091e-02,  1.0475e-01,
          -7.4034e-05, -2.5909e-03, -2.8673e-03, -1.6736e-03,  2.4614e-01,
           8.6720e-02, -2.2564e-03, -5.8896e-04,  9.6496e-02,  2.1468e-01,
          -1.9465e-03,  3.9963e-02],
         [ 1.7357e-01,  4.3207e-01,  2.8235e-01,  1.1781e-01,  3.9482e-01,
          -9.7418e-04, -2.5762e-03, -1.0229e-03,  1.9817e-02,  3.5679e-01,
           7.3253e-02, -8.2841e-04, -3.4260e-04, -3.8543e-03,  8.6295e-02,
          -2.3488e-03, -2.9598e-03,  2.4512e-01, -4.4377e-03,  1.9914e-01,
           2.3374e-01, -2.9764e-03, -3.6569e-04, -3.5280e-03,  7.4059e-02,
           1.6710e-01, -4.0879e-03, -9.0133e-04,  8.7771e-02,  8.5585e-02,
          -2.1024e-03,  2.9088e-02],
         [-1.9832e-03,  5.0719e-01, -9.1130e-05, -2.7060e-03,  2.0144e-01,
           3.0261e-02,  8.6320e-02,  2.6155e-01, -3.3911e-03,  3.9666e-01,
           5.3942e-02, -2.1797e-03, -3.5636e-04, -7.8701e-04, -4.0977e-03,
          -1.1453e-03, -2.5697e-03,  1.6564e-01, -2.4704e-03, -3.8854e-04,
          -1.8500e-03, -2.1988e-03, -3.4014e-03, -4.0414e-03,  1.5233e-01,
           1.5669e-01, -5.0024e-03,  1.3158e-01,  1.8915e-01,  1.9449e-01,
          -4.9989e-03,  1.8670e-01],
         [-4.8708e-05,  3.1193e-01,  1.8466e-01,  7.9462e-02,  9.7919e-01,
          -1.4953e-03, -4.3045e-03, -2.6985e-03,  2.5535e-01,  4.0940e-01,
           3.3001e-01,  7.8651e-02,  8.7439e-03, -2.0082e-04, -2.2148e-03,
          -4.6213e-03, -2.7984e-03,  9.0623e-02, -6.5106e-04,  2.1361e-01,
           1.7224e-01, -6.4628e-03, -1.9812e-03, -3.0438e-04,  2.3726e-01,
           8.0604e-03, -3.5406e-03, -1.6744e-03,  3.0817e-02,  2.2602e-01,
          -6.1669e-04,  3.4272e-01],
         [ 1.6355e-02,  4.7767e-01,  2.1139e-01,  1.1757e-01,  4.8446e-01,
          -1.1458e-03, -2.7688e-03,  6.1896e-02, -2.3806e-04,  6.4189e-01,
           8.5783e-02, -2.4483e-03,  1.3553e-01, -7.3674e-05, -5.5042e-04,
          -4.3026e-03, -3.3375e-03,  5.2477e-02, -2.3735e-03, -2.0638e-04,
           7.6386e-02, -4.5193e-03, -8.9218e-04, -1.1570e-03,  1.2723e-01,
           2.2528e-01, -3.7424e-03, -1.4733e-03,  5.5232e-02,  1.7147e-01,
           3.8376e-02,  3.1046e-01],
         [ 8.2137e-02,  2.9313e-01,  2.2122e-01, -6.8230e-04,  2.0506e-01,
           2.3266e-01, -3.4048e-03,  1.4452e-01, -2.3769e-04,  1.6755e-01,
          -1.4981e-03, -7.6702e-04,  8.5065e-03, -1.7208e-03,  4.3312e-03,
          -1.4504e-03, -1.1078e-03, -5.4829e-04, -3.0053e-03,  1.3540e-01,
          -6.2278e-04, -1.1976e-03, -1.1821e-03, -4.5605e-03,  2.0582e-03,
          -3.8204e-04, -1.8496e-03,  1.2784e-01,  9.1016e-02,  1.6921e-01,
          -3.4822e-03, -5.6924e-05],
         [-1.1220e-03,  3.5063e-01,  1.2122e-01, -9.5471e-04,  3.6741e-01,
           5.8896e-02, -2.1770e-03, -5.1129e-04, -7.8447e-04,  2.3318e-01,
           8.6762e-02, -5.2784e-04,  1.5305e-01,  1.8220e-02, -1.5500e-03,
          -6.8374e-04, -2.4534e-03,  4.3133e-02, -2.1391e-04,  1.8513e-01,
           1.6561e-01, -1.6395e-03, -3.0188e-03, -4.4003e-03,  1.6180e-01,
           3.4074e-02, -3.2062e-03,  1.3961e-01,  2.1645e-01,  2.0007e-01,
          -4.0055e-03,  2.1964e-01],
         [ 2.3160e-02,  3.1755e-01, -5.9205e-04,  3.9988e-02,  1.5560e-01,
          -1.3688e-03, -1.5750e-03, -1.4376e-03, -1.2531e-03,  1.0003e-01,
          -1.9203e-04, -5.2667e-04, -4.7663e-04, -2.4281e-03, -1.1777e-03,
          -1.2376e-03, -1.6198e-03,  5.1707e-02, -2.8637e-03,  7.7248e-02,
           8.6306e-02, -1.3699e-03, -1.6303e-03, -1.5750e-03,  4.6603e-02,
          -9.1537e-04, -3.4824e-03,  4.0708e-02, -3.4268e-04,  9.1366e-03,
          -1.7398e-03,  1.4191e-01]]], grad_fn=<LeakyReluBackward0>) torch.Size([1, 23, 32])
#### arr_b tensor([[[ 1.6416,  0.2271, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.8722,  0.3001, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.2511,  0.3824, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.6105,  0.4627, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.8426,  0.5884, -0.2861, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2659,  0.5711, -0.2796, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.8512,  0.6076, -0.2809, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3783,  0.6288, -0.2535, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.4571,  0.8924, -0.2310, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.3584,  0.7907,  0.1813, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.1351,  0.7417,  0.2302, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3846,  0.7205,  0.3123, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3435, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4078, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3785, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3619, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.5889,  0.3488, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459]]]) torch.Size([1, 24, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<LeakyReluBackward0>) torch.Size([1, 24, 32])