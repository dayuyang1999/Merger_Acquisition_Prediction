
CUDA availability: True
  1%|█▏                                                                                                                                              | 4/496 [00:00<01:33,  5.27it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 107, in train
    loss, pos_timing_loss, neg_timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 195, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 365, in forward
    z_vt_i = self.gnn_choice(features_i.squeeze(), edges_i.squeeze()) # (N_i_1, embedding_z)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 468, in forward
    x = self.convs[i](x, edge_index)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 508, in forward
    out = F.normalize(out, p=2)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 4270, in normalize
    denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 197, in format_stack
    return format_list(extract_stack(f, limit=limit))
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 39, in format_list
    return StackSummary.from_list(extracted_list).format()
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 426, in format
    row.append('    {}\n'.format(frame.line.strip()))
KeyboardInterrupt
torch.Size([1, 17]) torch.Size([1, 17, 32])
torch.Size([1, 17])
-1
torch.Size([1, 170]) torch.Size([1, 170, 32])
torch.Size([1, 170])
-1
### event lambdas:  tensor([[0.6893, 0.7821, 0.3741, 0.4641, 0.5459, 0.3658, 0.6780, 1.0966, 0.4069,
         0.3188, 0.3589, 0.3238, 0.6778, 0.2769, 0.7025, 0.6235, 0.3571],
        [0.5889, 0.6722, 0.3124, 0.3903, 0.4618, 0.3053, 0.5788, 0.9599, 0.3407,
         0.2650, 0.2994, 0.2693, 0.5787, 0.2294, 0.6007, 0.5304, 0.2977],
        [0.5174, 0.5934, 0.2699, 0.3389, 0.4028, 0.2637, 0.5082, 0.8594, 0.2949,
         0.2283, 0.2584, 0.2320, 0.5081, 0.1971, 0.5282, 0.4644, 0.2570],
        [0.7495, 0.8476, 0.4123, 0.5094, 0.5970, 0.4033, 0.7375, 1.1763, 0.4478,
         0.3523, 0.3959, 0.3577, 0.7374, 0.3066, 0.7636, 0.6798, 0.3938],
        [0.6245, 0.7113, 0.3340, 0.4162, 0.4914, 0.3265, 0.6139, 1.0089, 0.3639,
         0.2838, 0.3202, 0.2883, 0.6138, 0.2459, 0.6368, 0.5633, 0.3185],
        [0.5817, 0.6643, 0.3080, 0.3850, 0.4558, 0.3010, 0.5716, 0.9499, 0.3360,
         0.2613, 0.2952, 0.2654, 0.5715, 0.2261, 0.5934, 0.5237, 0.2936],
        [0.8529, 0.9592, 0.4797, 0.5886, 0.6858, 0.4696, 0.8399, 1.3098, 0.5197,
         0.4118, 0.4612, 0.4179, 0.8397, 0.3597, 0.8682, 0.7768, 0.4589],
        [1.1308, 1.2550, 0.6726, 0.8104, 0.9303, 0.6596, 1.1153, 1.6517, 0.7236,
         0.5844, 0.6487, 0.5924, 1.1152, 0.5155, 1.1488, 1.0403, 0.6457],
        [1.0739, 1.1949, 0.6318, 0.7640, 0.8796, 0.6193, 1.0589, 1.5834, 0.6806,
         0.5476, 0.6089, 0.5552, 1.0587, 0.4821, 1.0914, 0.9861, 0.6061],
        [0.9233, 1.0347, 0.5270, 0.6436, 0.7470, 0.5162, 0.9096, 1.3986, 0.5699,
         0.4538, 0.5071, 0.4604, 0.9095, 0.3974, 0.9394, 0.8432, 0.5046],
        [0.8688, 0.9762, 0.4903, 0.6009, 0.6995, 0.4800, 0.8556, 1.3300, 0.5309,
         0.4211, 0.4714, 0.4274, 0.8554, 0.3681, 0.8842, 0.7917, 0.4691],
        [1.0466, 1.1659, 0.6124, 0.7419, 0.8554, 0.6003, 1.0318, 1.5503, 0.6602,
         0.5302, 0.5901, 0.5377, 1.0316, 0.4663, 1.0638, 0.9601, 0.5873],
        [1.2109, 1.3395, 0.7312, 0.8766, 1.0023, 0.7174, 1.1949, 1.7466, 0.7851,
         0.6375, 0.7058, 0.6461, 1.1948, 0.5640, 1.2296, 1.1170, 0.7027],
        [1.4817, 1.6222, 0.9379, 1.1066, 1.2494, 0.9217, 1.4640, 2.0583, 1.0010,
         0.8271, 0.9081, 0.8373, 1.4638, 0.7387, 1.5022, 1.3777, 0.9044],
        [1.1700, 1.2964, 0.7011, 0.8427, 0.9655, 0.6877, 1.1543, 1.6984, 0.7536,
         0.6102, 0.6765, 0.6185, 1.1541, 0.5391, 1.1883, 1.0778, 0.6734],
        [1.0736, 1.1945, 0.6315, 0.7638, 0.8793, 0.6191, 1.0586, 1.5830, 0.6804,
         0.5474, 0.6087, 0.5550, 1.0584, 0.4819, 1.0911, 0.9857, 0.6059],
        [0.9227, 1.0340, 0.5266, 0.6431, 0.7465, 0.5158, 0.9090, 1.3978, 0.5695,
         0.4534, 0.5067, 0.4601, 0.9088, 0.3970, 0.9387, 0.8426, 0.5042]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(128.0692, grad_fn=<NegBackward>) non event loss:  tensor([761252.7897], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(6.3646, grad_fn=<SumBackward0>)
torch.Size([1, 6]) torch.Size([1, 6, 32])
torch.Size([1, 6])
-1
torch.Size([1, 60]) torch.Size([1, 60, 32])
torch.Size([1, 60])
-1
### event lambdas:  tensor([[0.1022, 0.1118, 0.1689, 0.0942, 0.0901, 0.1685],
        [0.1023, 0.1119, 0.1690, 0.0943, 0.0902, 0.1687],
        [0.1017, 0.1113, 0.1681, 0.0938, 0.0897, 0.1678],
        [0.1017, 0.1113, 0.1681, 0.0938, 0.0897, 0.1678],
        [0.1107, 0.1211, 0.1824, 0.1021, 0.0977, 0.1821],
        [0.0993, 0.1087, 0.1642, 0.0915, 0.0875, 0.1639]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(76.5470, grad_fn=<NegBackward>) non event loss:  tensor([43057.6367], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.1374, grad_fn=<SumBackward0>)
torch.Size([1, 9]) torch.Size([1, 9, 32])
torch.Size([1, 9])
-1
torch.Size([1, 90]) torch.Size([1, 90, 32])
torch.Size([1, 90])
-1
### event lambdas:  tensor([[0.1111, 0.0626, 0.0999, 0.0776, 0.1007, 0.1132, 0.0615, 0.2479, 0.0523],
        [0.0484, 0.0269, 0.0434, 0.0335, 0.0437, 0.0493, 0.0264, 0.1122, 0.0224],
        [0.0426, 0.0237, 0.0382, 0.0295, 0.0385, 0.0434, 0.0232, 0.0992, 0.0197],
        [0.0352, 0.0195, 0.0315, 0.0243, 0.0318, 0.0359, 0.0192, 0.0823, 0.0162],
        [0.0370, 0.0205, 0.0331, 0.0256, 0.0334, 0.0377, 0.0202, 0.0864, 0.0171],
        [0.0367, 0.0203, 0.0328, 0.0253, 0.0331, 0.0374, 0.0200, 0.0857, 0.0169],
        [0.0282, 0.0156, 0.0253, 0.0195, 0.0255, 0.0288, 0.0154, 0.0663, 0.0130],
        [0.0431, 0.0240, 0.0387, 0.0298, 0.0390, 0.0440, 0.0235, 0.1004, 0.0199],
        [0.0431, 0.0239, 0.0386, 0.0298, 0.0389, 0.0440, 0.0235, 0.1003, 0.0199]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(268.7977, grad_fn=<NegBackward>) non event loss:  tensor([30930.3929], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.1452, grad_fn=<SumBackward0>)
torch.Size([1, 5]) torch.Size([1, 5, 32])
torch.Size([1, 5])
-1
torch.Size([1, 50]) torch.Size([1, 50, 32])
torch.Size([1, 50])
-1
### event lambdas:  tensor([[0.0200, 0.0288, 0.0292, 0.0206, 0.0315],
        [0.0200, 0.0287, 0.0292, 0.0206, 0.0314],
        [0.0138, 0.0199, 0.0202, 0.0142, 0.0218],
        [0.0086, 0.0124, 0.0126, 0.0089, 0.0136],
        [0.0114, 0.0165, 0.0167, 0.0118, 0.0180]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(100.5309, grad_fn=<NegBackward>) non event loss:  tensor([5501.6730], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0259, grad_fn=<SumBackward0>)
torch.Size([1, 7]) torch.Size([1, 7, 32])
torch.Size([1, 7])
-1
torch.Size([1, 70]) torch.Size([1, 70, 32])
torch.Size([1, 70])
-1
### event lambdas:  tensor([[0.0175, 0.0052, 0.0177, 0.0165, 0.0095, 0.0218, 0.0179],
        [0.0175, 0.0052, 0.0177, 0.0165, 0.0095, 0.0218, 0.0179],
        [0.0157, 0.0046, 0.0160, 0.0148, 0.0086, 0.0196, 0.0161],
        [0.0062, 0.0018, 0.0063, 0.0058, 0.0034, 0.0077, 0.0063],
        [0.0057, 0.0017, 0.0058, 0.0054, 0.0031, 0.0072, 0.0059],
        [0.0038, 0.0011, 0.0039, 0.0036, 0.0021, 0.0048, 0.0039],
        [0.0067, 0.0020, 0.0068, 0.0063, 0.0036, 0.0084, 0.0068]],
       grad_fn=<SoftplusBackward>)