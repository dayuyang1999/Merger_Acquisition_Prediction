
CUDA availability: True
  0%|                                                                                                                                                       | 0/496 [00:00<?, ?it/s]/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in SigmoidBackward. Traceback of forward call that caused the error:
  File "main.py", line 126, in <module>
    main()
  File "main.py", line 122, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 81, in train
    loss, timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 183, in forward
    event_lambdas = self.timing_net(mat_b, mat_c, event_data) # (L3, )
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 267, in forward
    lambda_dt = torch.sigmoid(rate)  # (L3, )
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378062065/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
  0%|                                                                                                                                                       | 0/496 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 126, in <module>
    main()
  File "main.py", line 122, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 84, in train
    loss.backward() # required_graph = True
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Function 'SigmoidBackward' returned nan values in its 0th output.
#### arr_b tensor([[[ 1.5976,  0.2143, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 1.8232,  0.2856, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 2.1937,  0.3661, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 2.5452,  0.4447, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 2.7722,  0.5675, -0.2877, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.1862,  0.5506, -0.2814, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.7586,  0.5863, -0.2826, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 4.2741,  0.6070, -0.2558, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 5.3292,  0.8649, -0.2338, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 5.2327,  0.7654,  0.1694, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 5.0143,  0.7175,  0.2172, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.4805, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 4.2803,  0.6967,  0.2975, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.3281, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462],
         [ 3.2154,  0.3909, -0.0821, -0.3462, -0.3462, -0.3462, -0.3462,
          -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462, -0.3462]]]) torch.Size([1, 21, 14])
#### mat_b tensor([[[-3.0658e-01, -5.6017e-02, -7.9909e-02,  1.6441e-01, -8.1452e-02,
          -9.8195e-02,  1.0015e-01,  8.7783e-02, -2.4451e-01,  1.3430e-01,
           2.2917e-02, -1.5820e-01, -1.6210e-01,  1.3938e-02, -9.8486e-02,
           1.6177e-01,  8.2022e-02,  1.1547e-01,  9.1816e-03,  2.0631e-02,
           9.0957e-02, -1.2862e-02, -4.6954e-01, -1.7122e-01, -1.2940e-01,
           4.2345e-02, -5.1896e-02, -2.6091e-01, -5.0564e-02,  1.5849e-01,
           1.6113e-01,  1.0413e-01],
         [-3.2436e-01, -6.2071e-02, -8.3420e-02,  1.7586e-01, -8.4172e-02,
          -1.0922e-01,  9.2066e-02,  9.8337e-02, -2.4926e-01,  1.4456e-01,
           2.9921e-02, -1.5876e-01, -1.7918e-01,  2.3047e-02, -1.1931e-01,
           1.5094e-01,  8.2721e-02,  1.3941e-01,  2.2411e-02,  2.5541e-02,
           8.5278e-02, -2.5281e-02, -4.8121e-01, -1.9597e-01, -1.2025e-01,
           3.6560e-02, -5.6182e-02, -2.7034e-01, -4.1759e-02,  1.6166e-01,
           1.6273e-01,  1.0727e-01],
         [-3.5300e-01, -6.5488e-02, -9.0178e-02,  1.9014e-01, -9.1354e-02,
          -1.2364e-01,  7.8599e-02,  1.1014e-01, -2.6541e-01,  1.6396e-01,
           4.5766e-02, -1.5586e-01, -2.0979e-01,  3.8320e-02, -1.5432e-01,
           1.3491e-01,  7.8936e-02,  1.8175e-01,  4.1022e-02,  3.4749e-02,
           7.6833e-02, -4.0049e-02, -5.0090e-01, -2.3569e-01, -1.0572e-01,
           2.9053e-02, -6.1190e-02, -2.8910e-01, -2.6148e-02,  1.6995e-01,
           1.6806e-01,  1.0743e-01],
         [-3.8005e-01, -7.0246e-02, -9.8573e-02,  2.1133e-01, -9.8888e-02,
          -1.2982e-01,  7.2928e-02,  1.1429e-01, -2.8900e-01,  1.9072e-01,
           7.4952e-02, -1.5740e-01, -2.3589e-01,  4.7118e-02, -1.8744e-01,
           1.1845e-01,  7.3234e-02,  2.2142e-01,  5.1814e-02,  4.3006e-02,
           6.6764e-02, -5.0740e-02, -5.2617e-01, -2.6896e-01, -9.3834e-02,
           2.8120e-02, -6.6539e-02, -3.0138e-01, -1.9694e-02,  1.7785e-01,
           1.8004e-01,  1.0854e-01],
         [-4.1383e-01, -9.0879e-02, -8.2001e-02,  2.1248e-01, -1.1028e-01,
          -1.3448e-01,  5.7524e-02,  1.3394e-01, -3.0577e-01,  1.9772e-01,
           9.3930e-02, -1.6345e-01, -2.8631e-01,  4.2507e-02, -1.9730e-01,
           1.1881e-01,  7.4958e-02,  2.5575e-01,  5.7512e-02,  4.6176e-02,
           6.7956e-02, -5.9023e-02, -5.4923e-01, -2.8047e-01, -8.8106e-02,
           2.3254e-02, -8.3013e-02, -3.1499e-01, -7.0128e-04,  1.7869e-01,
           2.0669e-01,  1.1722e-01],
         [-4.4305e-01, -8.4464e-02, -1.0010e-01,  2.2639e-01, -1.1310e-01,
          -1.3237e-01,  5.5475e-02,  1.2912e-01, -3.4137e-01,  2.2909e-01,
           1.3196e-01, -1.6258e-01, -3.1275e-01,  5.2724e-02, -2.3626e-01,
           1.1381e-01,  6.3813e-02,  3.0478e-01,  6.8425e-02,  5.6129e-02,
           6.0768e-02, -7.0061e-02, -5.7932e-01, -3.1628e-01, -8.2711e-02,
           2.3776e-02, -9.2040e-02, -3.3774e-01,  1.2469e-02,  1.8718e-01,
           2.2387e-01,  1.0623e-01],
         [-4.9037e-01, -8.7366e-02, -1.1516e-01,  2.6517e-01, -1.1476e-01,
          -1.2660e-01,  5.2025e-02,  1.2946e-01, -3.8995e-01,  2.7287e-01,
           1.8350e-01, -1.6265e-01, -3.4690e-01,  6.9463e-02, -2.8788e-01,
           1.0072e-01,  4.0943e-02,  3.6243e-01,  8.4597e-02,  7.8993e-02,
           5.5277e-02, -8.5228e-02, -6.2007e-01, -3.5791e-01, -8.2622e-02,
           2.2989e-02, -9.8823e-02, -3.6864e-01,  4.0484e-02,  2.0642e-01,
           2.5750e-01,  9.2695e-02],
         [-5.3055e-01, -9.2844e-02, -1.3048e-01,  3.0923e-01, -1.1372e-01,
          -1.1925e-01,  4.9940e-02,  1.2770e-01, -4.3416e-01,  3.1343e-01,
           2.3146e-01, -1.6172e-01, -3.6979e-01,  8.6699e-02, -3.3710e-01,
           8.8381e-02,  1.5045e-02,  4.0880e-01,  1.0149e-01,  1.0122e-01,
           5.1312e-02, -1.0157e-01, -6.5568e-01, -3.9205e-01, -8.5907e-02,
           2.1055e-02, -1.0225e-01, -4.0015e-01,  7.1337e-02,  2.2726e-01,
           2.9005e-01,  7.7803e-02],
         [-4.2819e-01, -6.9616e-02, -1.2535e-01,  2.4267e-01, -1.0363e-01,
          -1.2937e-01,  6.9006e-02,  1.1114e-01, -3.4170e-01,  2.4134e-01,
           1.3464e-01, -1.5840e-01, -2.8165e-01,  6.3330e-02, -2.5123e-01,
           1.0377e-01,  5.5832e-02,  2.9620e-01,  7.1852e-02,  6.0390e-02,
           5.5599e-02, -7.0685e-02, -5.7731e-01, -3.2470e-01, -8.0392e-02,
           2.7363e-02, -8.0754e-02, -3.3788e-01,  1.8521e-03,  1.9316e-01,
           2.1122e-01,  9.8277e-02],
         [-4.2819e-01, -6.9616e-02, -1.2535e-01,  2.4267e-01, -1.0363e-01,
          -1.2937e-01,  6.9006e-02,  1.1114e-01, -3.4170e-01,  2.4134e-01,
           1.3464e-01, -1.5840e-01, -2.8165e-01,  6.3330e-02, -2.5123e-01,
           1.0377e-01,  5.5832e-02,  2.9620e-01,  7.1852e-02,  6.0390e-02,
           5.5599e-02, -7.0685e-02, -5.7731e-01, -3.2470e-01, -8.0392e-02,
           2.7363e-02, -8.0754e-02, -3.3788e-01,  1.8521e-03,  1.9316e-01,
           2.1122e-01,  9.8277e-02],
         [-4.2819e-01, -6.9616e-02, -1.2535e-01,  2.4267e-01, -1.0363e-01,
          -1.2937e-01,  6.9006e-02,  1.1114e-01, -3.4170e-01,  2.4134e-01,
           1.3464e-01, -1.5840e-01, -2.8165e-01,  6.3330e-02, -2.5123e-01,
           1.0377e-01,  5.5832e-02,  2.9620e-01,  7.1852e-02,  6.0390e-02,
           5.5599e-02, -7.0685e-02, -5.7731e-01, -3.2470e-01, -8.0392e-02,
           2.7363e-02, -8.0754e-02, -3.3788e-01,  1.8521e-03,  1.9316e-01,
           2.1122e-01,  9.8277e-02],
         [-6.3060e-01, -1.1299e-01, -1.5345e-01,  4.1071e-01, -1.1919e-01,
          -1.1828e-01,  3.8402e-02,  1.4076e-01, -5.2263e-01,  3.9988e-01,
           3.2403e-01, -1.6721e-01, -4.2104e-01,  1.2385e-01, -4.3263e-01,
           3.7094e-02, -2.9653e-02,  5.0185e-01,  1.3497e-01,  1.4340e-01,
           3.4703e-02, -1.2859e-01, -7.3305e-01, -4.6177e-01, -7.5609e-02,
           1.8993e-02, -1.0459e-01, -4.3896e-01,  1.1831e-01,  2.6714e-01,
           3.4773e-01,  7.0761e-02],
         [-4.2819e-01, -6.9616e-02, -1.2535e-01,  2.4267e-01, -1.0363e-01,
          -1.2937e-01,  6.9006e-02,  1.1114e-01, -3.4170e-01,  2.4134e-01,
           1.3464e-01, -1.5840e-01, -2.8165e-01,  6.3330e-02, -2.5123e-01,
           1.0377e-01,  5.5832e-02,  2.9620e-01,  7.1852e-02,  6.0390e-02,
           5.5599e-02, -7.0685e-02, -5.7731e-01, -3.2470e-01, -8.0392e-02,
           2.7363e-02, -8.0754e-02, -3.3788e-01,  1.8521e-03,  1.9316e-01,
           2.1122e-01,  9.8277e-02],
         [-5.7172e-01, -1.0266e-01, -2.0457e-01,  4.3468e-01, -9.5423e-02,
          -1.1358e-01,  7.4232e-02,  1.1558e-01, -4.8726e-01,  4.1062e-01,
           3.1461e-01, -1.6853e-01, -3.6050e-01,  1.3329e-01, -4.6292e-01,
           6.9636e-03, -3.6532e-02,  4.6485e-01,  1.4252e-01,  1.5794e-01,
           3.4420e-02, -1.3085e-01, -7.2934e-01, -4.8463e-01, -5.5810e-02,
           2.0500e-02, -8.9447e-02, -4.5463e-01,  8.2850e-02,  2.8559e-01,
           3.2224e-01,  6.5675e-02],
         [-5.4763e-01, -9.5629e-02, -2.0742e-01,  4.1715e-01, -9.1316e-02,
          -1.1263e-01,  8.1348e-02,  1.1102e-01, -4.6734e-01,  3.9515e-01,
           2.9574e-01, -1.6855e-01, -3.4147e-01,  1.2762e-01, -4.4953e-01,
           1.0788e-02, -2.8617e-02,  4.4119e-01,  1.3696e-01,  1.5234e-01,
           3.7714e-02, -1.2338e-01, -7.1442e-01, -4.7392e-01, -5.1809e-02,
           2.2264e-02, -8.4931e-02, -4.4894e-01,  6.7468e-02,  2.7964e-01,
           3.0719e-01,  6.7298e-02],
         [-4.2819e-01, -6.9616e-02, -1.2535e-01,  2.4267e-01, -1.0363e-01,
          -1.2937e-01,  6.9006e-02,  1.1114e-01, -3.4170e-01,  2.4134e-01,
           1.3464e-01, -1.5840e-01, -2.8165e-01,  6.3330e-02, -2.5123e-01,
           1.0377e-01,  5.5832e-02,  2.9620e-01,  7.1852e-02,  6.0390e-02,
           5.5599e-02, -7.0685e-02, -5.7731e-01, -3.2470e-01, -8.0392e-02,
           2.7363e-02, -8.0754e-02, -3.3788e-01,  1.8521e-03,  1.9316e-01,
           2.1122e-01,  9.8277e-02],
         [-4.2819e-01, -6.9616e-02, -1.2535e-01,  2.4267e-01, -1.0363e-01,
          -1.2937e-01,  6.9006e-02,  1.1114e-01, -3.4170e-01,  2.4134e-01,
           1.3464e-01, -1.5840e-01, -2.8165e-01,  6.3330e-02, -2.5123e-01,
           1.0377e-01,  5.5832e-02,  2.9620e-01,  7.1852e-02,  6.0390e-02,
           5.5599e-02, -7.0685e-02, -5.7731e-01, -3.2470e-01, -8.0392e-02,
           2.7363e-02, -8.0754e-02, -3.3788e-01,  1.8521e-03,  1.9316e-01,
           2.1122e-01,  9.8277e-02],
         [-4.2819e-01, -6.9616e-02, -1.2535e-01,  2.4267e-01, -1.0363e-01,
          -1.2937e-01,  6.9006e-02,  1.1114e-01, -3.4170e-01,  2.4134e-01,
           1.3464e-01, -1.5840e-01, -2.8165e-01,  6.3330e-02, -2.5123e-01,
           1.0377e-01,  5.5832e-02,  2.9620e-01,  7.1852e-02,  6.0390e-02,
           5.5599e-02, -7.0685e-02, -5.7731e-01, -3.2470e-01, -8.0392e-02,
           2.7363e-02, -8.0754e-02, -3.3788e-01,  1.8521e-03,  1.9316e-01,
           2.1122e-01,  9.8277e-02],
         [-4.8708e-01, -8.2360e-02, -1.9641e-01,  3.7397e-01, -8.8406e-02,
          -1.2324e-01,  8.8006e-02,  1.0473e-01, -4.0667e-01,  3.4579e-01,
           2.2809e-01, -1.6722e-01, -2.8506e-01,  1.1249e-01, -3.8414e-01,
           1.9999e-02,  1.1272e-03,  3.6210e-01,  1.1548e-01,  1.2203e-01,
           3.7875e-02, -9.9571e-02, -6.6012e-01, -4.2351e-01, -4.7290e-02,
           2.8901e-02, -6.7523e-02, -3.9491e-01,  1.7597e-02,  2.5316e-01,
           2.5314e-01,  8.7392e-02],
         [-4.2695e-01, -5.3340e-02, -1.3428e-01,  2.2786e-01, -9.5060e-02,
          -1.1342e-01,  7.5282e-02,  9.9573e-02, -3.5443e-01,  2.3996e-01,
           1.3971e-01, -1.5415e-01, -2.7679e-01,  6.3611e-02, -2.5175e-01,
           1.2362e-01,  4.6671e-02,  2.9704e-01,  7.0041e-02,  6.5391e-02,
           6.5504e-02, -6.6454e-02, -5.7681e-01, -3.1944e-01, -9.3615e-02,
           3.0062e-02, -8.1910e-02, -3.5463e-01,  1.3718e-02,  1.9466e-01,
           2.1930e-01,  7.8741e-02],
         [-4.2746e-01, -6.0049e-02, -1.3060e-01,  2.3396e-01, -9.8592e-02,
          -1.2000e-01,  7.2695e-02,  1.0434e-01, -3.4918e-01,  2.4053e-01,
           1.3762e-01, -1.5590e-01, -2.7879e-01,  6.3495e-02, -2.5154e-01,
           1.1544e-01,  5.0447e-02,  2.9669e-01,  7.0788e-02,  6.3329e-02,
           6.1421e-02, -6.8198e-02, -5.7702e-01, -3.2161e-01, -8.8165e-02,
           2.8949e-02, -8.1433e-02, -3.4773e-01,  8.8271e-03,  1.9404e-01,
           2.1597e-01,  8.6793e-02]]], grad_fn=<AddBackward0>) torch.Size([1, 21, 32])