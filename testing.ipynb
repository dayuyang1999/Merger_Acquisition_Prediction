{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "#from optimization import Adam\n",
    "\n",
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MAPredNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPredNet, self).__init__()\n",
    "        # hyperparams\n",
    "        self.s_year = 1997\n",
    "        self.e_year = 2020\n",
    "        self.a_freq_fv_dim = 14\n",
    "        self.target_fv_dim = 17\n",
    "        self.embedding_b = 32\n",
    "        self.embedding_c = 16\n",
    "        self.embedding_z = 32\n",
    "        self.dropout_ratio = 0.25\n",
    "\n",
    "\n",
    "\n",
    "        # define models\n",
    "        self.timing_net = Timing_Net(self.embedding_b, self.embedding_c)\n",
    "        # # input: (L1, a_freq_fv_dim); output: (L1, embedding_b)\n",
    "        self.b_net = nn.Sequential(\n",
    "                        nn.Linear(in_features=self.a_freq_fv_dim, out_features=64), \n",
    "                        nn.Dropout(self.dropout_ratio), \n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(in_features=64, out_features=self.embedding_b))         \n",
    "        self.c_net =  nn.GRU(input_size = 3, hidden_size = self.embedding_c, batch_first=True)\n",
    "        self.choice_net = ChoiceNet(self.embedding_b, self.embedding_c, self.embedding_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def MCestimator(self, arr, estimate_length):\n",
    "        '''\n",
    "        Monte Carlo Estimator \n",
    "        input: \n",
    "            arr: 1d arr\n",
    "            estimate_length: scalar\n",
    "        '''\n",
    "        estimation =  estimate_length*(1/(arr.size(0)-1))*torch.sum(arr)\n",
    "        \n",
    "        return estimation\n",
    "\n",
    "    def likelihood(self, event_time_ll, non_event_time_ll):\n",
    "        '''\n",
    "        compute likelihood loss\n",
    "        input are all scalars\n",
    "        loss: small = good\n",
    "        '''\n",
    "        loss = - event_time_ll + non_event_time_ll\n",
    "        return loss\n",
    "\n",
    "    def forward(self, arr_b, arr_c, arr_delta_time, event_data, non_event_data, estimate_length, choice_data_dict):\n",
    "        '''\n",
    "        WARNING: \n",
    "            1. This Version only works for batch_size = 1 (for a single firm)..... (if batch size > 1, have to padding at first to make arr_b, arr_c has same size\n",
    "                    if padding, the number of event of every year for all firms should be the same (in dict_idx[year]))\n",
    "            2. all idx must be put into list in time order\n",
    "            3. year is always an integer\n",
    "\n",
    "        Assume:\n",
    "        L1 = Length_of_year_cross\n",
    "        L2 = length_of_self+peer_events\n",
    "        L3 = length_of_self event\n",
    "        L_Neg = Length of negative samples\n",
    "\n",
    "        N_i_2 = N_self_event_ith_year\n",
    "\n",
    "\n",
    "        Inputs:\n",
    "            arr_b: raw financial variables; (L1, 14) \n",
    "\n",
    "            arr_c: raw peer/self effect variables; (L2, 3)\n",
    "\n",
    "            arr_delta_time: array of delta_time; (L3, 1); \n",
    "                arr_delta_time is corresponding to arr_c: time delta in i row = (t_event_i+1 - t_event_i) \n",
    "\n",
    "            event_data: \n",
    "                arr_b_idx: lst of indeces, [3, 3, 4, 4, 4, 5, 9 ...]\n",
    "                    length = L3\n",
    "                    element: integer as row number in arr_b; for true event\n",
    "                arr_c_idx: lst of indeces, [3, 3, 4, 4, 4, 5, 9 ...]\n",
    "                    length = L3\n",
    "                    element: integer as row number in arr_c; for true event\n",
    "                arr_delta_time:  (same as...)\n",
    "                    array !! not idx  (L3, )\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "            \n",
    "            non_event_data: for negative sampling in timing model only; idea: draw time point from Unif(0, MAX_T). \n",
    "                                pick up the corresponding b, c, delta_t.\n",
    "                arr_b_idx:  lst of indeces, \n",
    "                    length: L_Neg\n",
    "                arr_c_idx: \n",
    "                arr_t_non: array, not index!\n",
    "                \n",
    "\n",
    "            estimate_length: scalar, for negative sampling MC estimator, \n",
    "                max(time)  - min(time)\n",
    "\n",
    "\n",
    "            choice_data_dict: for choice model only, a dict contains:(invariant for firms,  variant by year)\n",
    "                - dict_idx: A dict split arr_b_idx and arr_c_idx to year. e.g. {1997: [true_tar_idxs, arr_b_idx, arr_c_idx], '1998': [arr_b_idx, arr_c_idx], ...} \n",
    "                            year is \"the year that self event happens\" (so the size varies for differnt firms)\n",
    "                    \n",
    "\n",
    "                    - arr_b_idx_i: lst, N_i_2 \n",
    "                    - arr_c_idx_i: lst, N_i_2 \n",
    "                \n",
    "                - true_tar_idxs_i: torch tensor, one-hot, size = (N_i_2, N_i_1) # for each row, only 1 (true acquirer) is 1, others are 0.\n",
    "                        N_i_1 = N_i (every firm could be in target candidate)\n",
    "                - node features np.array: [N_i, in_channels_i]  # inchannels = \n",
    "                - network structure np.array(edges): [2, N_edges_i] # the idx here is corresponding to node feature array\n",
    "\n",
    "                \n",
    "        output:\n",
    "        \n",
    "        '''\n",
    "        # check input data \n",
    "        # arr_b_idx, arr_c_idx, arr_delta_time = event_data # expand\n",
    "        # arr_b_idx_non, arr_c_idx_non, arr_delta_time_non = non_event_data\n",
    "        # assert (len(arr_b_idx) == len(arr_c_idx)), \"the size of input indeces dismatch for event data\"\n",
    "        # assert (len(arr_b_idx_non) == len(arr_c_idx_non)), \"the size of input indeces dismatch for non-event data\"\n",
    "        # assert (arr_c.size[0] == arr_delta_time.size[0]), \"the size of input array dismatch\"\n",
    "\n",
    "\n",
    "        # transform to embedding\n",
    "        #arr_b = arr_b\n",
    "        #print(type(arr_b.squeeze().numpy()[0, 0]))\n",
    "        mat_b = self.b_net(arr_b) # (L1, embedding_b)\n",
    "        mat_c = self.c_net(arr_c) # (L2, embedding_c)\n",
    "        \n",
    "\n",
    "        # timing model\n",
    "        event_lambda = self.timing_net(mat_b, mat_c, event_data) # (L3, )\n",
    "        non_event_lambdas = self.timing_net(mat_b, mat_c,  non_event_data) # (L_Neg, )\n",
    "\n",
    "        event_time_ll = torch.sum(torch.log(event_lambda))  # out = scalar\n",
    "        non_event_time_ll = self.MCestimator(non_event_lambdas, estimate_length)  # out = scalar\n",
    "\n",
    "        log_likelihood_loss = self.likelihood(event_time_ll, non_event_time_ll)  # out = scalar\n",
    "        \n",
    "\n",
    "        # choice model\n",
    "        event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)\n",
    "\n",
    "        total_loss = log_likelihood_loss + event_choice_loss\n",
    "        return total_loss, log_likelihood_loss, event_choice_loss\n",
    "    \n",
    "\n",
    "\n",
    "class Timing_Net(nn.Module):\n",
    "    def __init__(self, embedding_b, embedding_c):\n",
    "        super(Timing_Net, self).__init__()\n",
    "        self.phi =  torch.randn(1, requires_grad=True)\n",
    "        self.w_b =  torch.randn(embedding_b, requires_grad=True) # (embedding_b, )\n",
    "        self.w_c =  torch.randn(embedding_b, requires_grad=True)# (embedding_c, )\n",
    "        self.omega = torch.randn(1, requires_grad=True)# scalar\n",
    "\n",
    "    def f_lambda(self, x):\n",
    "        '''\n",
    "        inputs\n",
    "        '''\n",
    "        lambda_dt = self.phi*torch.log(1+torch.exp(x/self.phi))\n",
    "\n",
    "        return lambda_dt\n",
    "\n",
    "    def forward(self, mat_b, mat_c, arr_delta_time, event_data):\n",
    "        '''\n",
    "        Assume:\n",
    "            L1 = Length_of_year_cross\n",
    "            L2: length_of_self+peer_events\n",
    "            L3: length_of_self event\n",
    "            \n",
    "        Input: \n",
    "            mat_b: (L1, embedding_b)\n",
    "            mat_c: (L2, embedding_c)\n",
    "            arr_delta_time: (L2, embedding_c)\n",
    "\n",
    "        \n",
    "        '''\n",
    "        arr_b_idx, arr_c_idx, arr_delta_t = event_data # expand\n",
    "        b = mat_b[arr_b_idx] # (L3, embedding_b)\n",
    "        c = mat_c[arr_c_idx] # (L3, embedding_c)\n",
    "        delta_t = arr_delta_t # (L3, 1)\n",
    "\n",
    "        # (L3, )\n",
    "        rate = torch.matmul(b, self.w_b)  + torch.matmul(c, self.w_c) + self.omega * torch.exp(-self.omega * delta_t) \n",
    "        lambda_dt = self.f_lambda(rate)  # (L3, )\n",
    "        return lambda_dt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ChoiceNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_b, embedding_c, embedding_z):\n",
    "        super(ChoiceNet, self).__init__()\n",
    "        # hyperparams\n",
    "        self.embedding_z = embedding_z\n",
    "        self.embedding_b, self.embedding_c = embedding_b, embedding_c\n",
    "        self.gnn_hidden_dim = 64\n",
    "        self.gnn_model_type = \"GraphSage\" # GraphSage or GAT\n",
    "        self.target_fv_dim = 17\n",
    "        self.dropout_ratio = 0.25\n",
    "\n",
    "        # build modules\n",
    "        self.gnn_choice = GNN_Stack(self.target_fv_dim, self.gnn_hidden_dim, self.embedding_z, self.gnn_model_type)\n",
    "        self.transform = nn.Sequential(  # input dim = (N_of_self_event, embedding_b + embedding_c), output dim = (N_of_self_event, embedding_z)\n",
    "                        nn.Linear(in_features=self.embedding_b + self.embedding_c, out_features=64), nn.Dropout(self.dropout_ratio), nn.ReLU(),\n",
    "                        nn.Linear(in_features=64, out_features=self.embedding_z))\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, mat_b, mat_c, choice_data_dict, s_year, e_year):\n",
    "        '''\n",
    "        input:\n",
    "            choice_data_dict:  {1997: }\n",
    "                length of dict: number of years (for all firms, always pass the full dataset into ChoiceNet)\n",
    "         \n",
    "        '''\n",
    "        choice_loss_lst = []\n",
    "        for year in range(s_year, e_year):\n",
    "            '''\n",
    "            At the ith iteration of the loop,\n",
    "                compute binary cross entropy loss for choice problem of i-th year\n",
    "                based on all of the MA event occurred in that year\n",
    "            \n",
    "            N_i_1 = number of candidate target in i-th year\n",
    "            N_i_2 = number of self events in i-th year\n",
    "\n",
    "            '''\n",
    "            dict_idx, true_tar_idxs_i, features_i, edges_i = choice_data_dict[year] # list=:[N_i_1], arrays: (N_i_1, 22) , (2, |E|)\n",
    "            arr_b_idx_i, arr_c_idx_i = dict_idx \n",
    "            # true_tar_idxs_i: tensor, one-hot, size = (N_i_2, N_i_1)\n",
    "            # arr_b_idx_i, arr_c_idx_i: list: length = N_i_2 and N_i_2 \n",
    "            # if there's no self event in ith year, continue\n",
    "            if len(arr_b_idx_i == 0): \n",
    "                continue\n",
    "            else:\n",
    "                # GNN part\n",
    "                '''\n",
    "                always pass the entire graph for i-th year into GNN\n",
    "                '''\n",
    "                \n",
    "                #assert len(true_tar_idxs_i) == features_i.size(0), \"number of self events mismatch in choice data\"\n",
    "                z_vt_i = self.gnn_choice(features_i, edges_i) # (N_i_1, embedding_z)\n",
    "\n",
    "                # z_dt : (N_i_2, embedding_z)\n",
    "                z_dt = self.transform(torch.cat((mat_b[arr_b_idx_i], mat_c[arr_c_idx_i]), dim=1))  # z_dt : (N_i_2, embedding_b + embedding_c)\n",
    "\n",
    "                # broadcasting\n",
    "                z_vt_i = z_vt_i.unsqueeze(0) # (1, N_i_1, embedding_z)\n",
    "                z_dt_i = z_dt_i.unsqueeze(1) # (N_i_2, 1, embedding_z)\n",
    "                logits_i = (z_dt_i * z_vt_i).sum(axis=-1) # (N_i_2, N_i_1)\n",
    "                choice_l = self.loss(nn.Sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)\n",
    "                choice_loss_lst.append(choice_l) # append a scalar\n",
    "\n",
    "        return torch.sum(choice_loss_lst)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "##################### GNN ###########################################\n",
    "            \n",
    "\n",
    "\n",
    "class GNN_Stack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, model_type, emb=True):\n",
    "        super(GNN_Stack, self).__init__()\n",
    "\n",
    "        # arguments\n",
    "        self.model_type = model_type\n",
    "        self.num_layers = 2\n",
    "        self.heads = 1\n",
    "        self.dropout = 0.25\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        conv_model = self.build_conv_model(self.model_type)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (self.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(self.num_layers-1):\n",
    "            self.convs.append(conv_model(self.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(self.heads * hidden_dim, hidden_dim), nn.Dropout(self.dropout), \n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = self.dropout\n",
    "        self.num_layers = self.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == 'GraphSage':\n",
    "            return GraphSage \n",
    "        elif model_type == 'GAT':\n",
    "            return GAT\n",
    "\n",
    "    def forward(self, X, E):\n",
    "        '''\n",
    "        X: the node features: [N, input_dim]\n",
    "        E: the network structure: [2, E]  # note that the idx is corresponding to X\n",
    "\n",
    "\n",
    "        tar_net_fv_i, tar_net_E_i\n",
    "        \n",
    "        '''\n",
    "        x, edge_index = X, E \n",
    "          \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GraphSage(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, normalize = True,\n",
    "                 bias = False, **kwargs):  \n",
    "        super(GraphSage, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.lin_l = nn.Linear(self.in_channels, self.out_channels)\n",
    "        self.lin_r = nn.Linear(self.in_channels, self.out_channels)\n",
    "\n",
    "        \n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        prop = self.propagate(edge_index, x=(x, x), size=size)\n",
    "        out = self.lin_l(x) + self.lin_r(prop)\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "        out = x_j\n",
    "        return out\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = self.node_dim\n",
    "        out = torch_scatter.scatter(inputs, index, node_dim, dim_size=dim_size, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GAT(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, heads = 2,\n",
    "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "        self.att_l = None\n",
    "        self.att_r = None\n",
    "\n",
    "\n",
    "        # self.lin_l is the linear transformation that you apply to embeddings \n",
    "        self.lin_l = nn.Linear(self.in_channels, self.out_channels * self.heads)\n",
    "        self.lin_r = self.lin_l\n",
    "\n",
    "        self.att_l = nn.Parameter(torch.zeros(self.heads, self.out_channels))\n",
    "        self.att_r = nn.Parameter(torch.zeros(self.heads, self.out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
    "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
    "        nn.init.xavier_uniform_(self.att_l)\n",
    "        nn.init.xavier_uniform_(self.att_r)\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        \n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "\n",
    "        x_l = self.lin_l(x).reshape(-1, H, C)\n",
    "        x_r = self.lin_r(x).reshape(-1, H, C)\n",
    "        alpha_l = self.att_l * x_l\n",
    "        alpha_r = self.att_r * x_r\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), alpha=(alpha_l, alpha_r), size=size)\n",
    "        out = out.reshape(-1, H*C)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
    "\n",
    "        alpha = F.leaky_relu(alpha_i + alpha_j, negative_slope=self.negative_slope)\n",
    "        if ptr:\n",
    "            att_weight = F.softmax(alpha_i + alpha_j, ptr)\n",
    "        else:\n",
    "            att_weight = pyg.utils.softmax(alpha_i + alpha_j, index)\n",
    "        att_weight = F.dropout(att_weight, p=self.dropout)\n",
    "        out = att_weight * x_j\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "        out = torch_scatter.scatter(inputs, index, self.node_dim, dim_size=dim_size, reduce='sum')\n",
    "    \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_net.0.weight torch.float32\n",
      "b_net.0.bias torch.float32\n",
      "b_net.3.weight torch.float32\n",
      "b_net.3.bias torch.float32\n",
      "c_net.weight_ih_l0 torch.float32\n",
      "c_net.weight_hh_l0 torch.float32\n",
      "c_net.bias_ih_l0 torch.float32\n",
      "c_net.bias_hh_l0 torch.float32\n",
      "choice_net.gnn_choice.convs.0.lin_l.weight torch.float32\n",
      "choice_net.gnn_choice.convs.0.lin_l.bias torch.float32\n",
      "choice_net.gnn_choice.convs.0.lin_r.weight torch.float32\n",
      "choice_net.gnn_choice.convs.0.lin_r.bias torch.float32\n",
      "choice_net.gnn_choice.convs.1.lin_l.weight torch.float32\n",
      "choice_net.gnn_choice.convs.1.lin_l.bias torch.float32\n",
      "choice_net.gnn_choice.convs.1.lin_r.weight torch.float32\n",
      "choice_net.gnn_choice.convs.1.lin_r.bias torch.float32\n",
      "choice_net.gnn_choice.post_mp.0.weight torch.float32\n",
      "choice_net.gnn_choice.post_mp.0.bias torch.float32\n",
      "choice_net.gnn_choice.post_mp.2.weight torch.float32\n",
      "choice_net.gnn_choice.post_mp.2.bias torch.float32\n",
      "choice_net.transform.0.weight torch.float32\n",
      "choice_net.transform.0.bias torch.float32\n",
      "choice_net.transform.3.weight torch.float32\n",
      "choice_net.transform.3.bias torch.float32\n"
     ]
    }
   ],
   "source": [
    "model = MAPredNet()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = MADataset()\n",
    "#loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-d525384e7d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiming_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice_loss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_delta_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_event_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice_data_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-c9fb7c23cd08>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, arr_b, arr_c, arr_delta_time, event_data, non_event_data, estimate_length, choice_data_dict)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0marr_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mmat_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (L1, embedding_b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mmat_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (L2, embedding_c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "data_size = len(dataset)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# build model\n",
    "model = MAPredNet()\n",
    "\n",
    "model = model.float()\n",
    "#model = model.to(device)\n",
    "opt = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_e = 0\n",
    "    timing_loss_e = 0\n",
    "    choice_loss_e = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in loader:\n",
    "        batch = tuple(batch)\n",
    "        arr_b, arr_c, arr_delta_time, event_data, non_event_data, estimate_length, choice_data_dict = batch\n",
    "        \n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss, timing_loss, choice_loss  = model(arr_b, arr_c, arr_delta_time, event_data, non_event_data, estimate_length, choice_data_dict)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_e += loss\n",
    "        timing_loss_e += timing_loss\n",
    "        choice_loss_e += choice_loss\n",
    "        break\n",
    "    \n",
    "    loss_e /= data_size\n",
    "    timing_loss_e /= data_size\n",
    "    choice_loss_e /= data_size\n",
    "    # writer.add_scalar('training timing loss', timing_loss_e, epoch)\n",
    "    # writer.add_scalar('training choice loss', choice_loss_e, epoch)\n",
    "    # writer.add_scalar('total loss', loss_e, epoch)\n",
    "\n",
    "    print(\"Epoch {}. Total Loss: {:.4f}. Timing MLE loss: {:.4f}. Choice BCE loss {:.4f}\".format(\n",
    "            epoch, loss_e, timing_loss_e, choice_loss_e))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.single(np.array([[1],[2]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tt[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = np.array([[1.],[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tt1[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d99b08e1309bbda1a7a60a7fe146f82e64d62b20f5527f533414b8454a1fbeeb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('GNN': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
