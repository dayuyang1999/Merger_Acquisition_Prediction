
CUDA availability: True
torch.Size([1, 24, 1])
-1
torch.Size([1, 240, 1])
-1
### event lambdas:  tensor([[[0.0014, 0.0008, 0.0008, 0.0025, 0.0010, 0.0015, 0.0060, 0.0155,
          0.0030, 0.0021, 0.0021, 0.0292, 0.0010, 0.0011, 0.0012, 0.0013,
          0.0015, 0.0016, 0.0018, 0.0018, 0.0214, 0.0013, 0.0014, 0.0013],
         [0.0019, 0.0011, 0.0011, 0.0033, 0.0014, 0.0020, 0.0082, 0.0210,
          0.0040, 0.0028, 0.0029, 0.0394, 0.0014, 0.0015, 0.0016, 0.0018,
          0.0020, 0.0022, 0.0024, 0.0024, 0.0289, 0.0018, 0.0019, 0.0018],
         [0.0010, 0.0006, 0.0006, 0.0018, 0.0008, 0.0011, 0.0045, 0.0116,
          0.0022, 0.0015, 0.0016, 0.0218, 0.0008, 0.0008, 0.0009, 0.0010,
          0.0011, 0.0012, 0.0013, 0.0013, 0.0160, 0.0010, 0.0010, 0.0010],
         [0.0010, 0.0006, 0.0006, 0.0018, 0.0007, 0.0011, 0.0044, 0.0114,
          0.0022, 0.0015, 0.0015, 0.0215, 0.0007, 0.0008, 0.0009, 0.0010,
          0.0011, 0.0012, 0.0013, 0.0013, 0.0157, 0.0010, 0.0010, 0.0010],
         [0.0007, 0.0004, 0.0004, 0.0012, 0.0005, 0.0008, 0.0030, 0.0078,
          0.0015, 0.0010, 0.0011, 0.0148, 0.0005, 0.0006, 0.0006, 0.0007,
          0.0007, 0.0008, 0.0009, 0.0009, 0.0108, 0.0007, 0.0007, 0.0007],
         [0.0013, 0.0008, 0.0008, 0.0024, 0.0010, 0.0014, 0.0059, 0.0150,
          0.0029, 0.0020, 0.0020, 0.0284, 0.0010, 0.0011, 0.0011, 0.0013,
          0.0014, 0.0015, 0.0017, 0.0017, 0.0208, 0.0013, 0.0014, 0.0013],
         [0.0014, 0.0008, 0.0008, 0.0025, 0.0010, 0.0015, 0.0061, 0.0156,
          0.0030, 0.0021, 0.0021, 0.0295, 0.0010, 0.0011, 0.0012, 0.0014,
          0.0015, 0.0016, 0.0018, 0.0018, 0.0216, 0.0013, 0.0014, 0.0013],
         [0.0007, 0.0004, 0.0004, 0.0012, 0.0005, 0.0008, 0.0031, 0.0079,
          0.0015, 0.0010, 0.0011, 0.0149, 0.0005, 0.0006, 0.0006, 0.0007,
          0.0008, 0.0008, 0.0009, 0.0009, 0.0109, 0.0007, 0.0007, 0.0007],
         [0.0009, 0.0005, 0.0005, 0.0016, 0.0006, 0.0009, 0.0038, 0.0098,
          0.0019, 0.0013, 0.0013, 0.0185, 0.0006, 0.0007, 0.0007, 0.0009,
          0.0009, 0.0010, 0.0011, 0.0011, 0.0136, 0.0008, 0.0009, 0.0008],
         [0.0009, 0.0005, 0.0005, 0.0016, 0.0007, 0.0010, 0.0040, 0.0103,
          0.0020, 0.0014, 0.0014, 0.0194, 0.0007, 0.0007, 0.0008, 0.0009,
          0.0010, 0.0010, 0.0012, 0.0012, 0.0142, 0.0009, 0.0009, 0.0009],
         [0.0014, 0.0008, 0.0008, 0.0025, 0.0010, 0.0015, 0.0060, 0.0155,
          0.0030, 0.0021, 0.0021, 0.0292, 0.0010, 0.0011, 0.0012, 0.0014,
          0.0015, 0.0016, 0.0018, 0.0018, 0.0214, 0.0013, 0.0014, 0.0013],
         [0.0007, 0.0004, 0.0004, 0.0013, 0.0005, 0.0008, 0.0032, 0.0083,
          0.0016, 0.0011, 0.0011, 0.0157, 0.0005, 0.0006, 0.0006, 0.0007,
          0.0008, 0.0008, 0.0009, 0.0010, 0.0115, 0.0007, 0.0008, 0.0007],
         [0.0018, 0.0010, 0.0011, 0.0033, 0.0013, 0.0020, 0.0081, 0.0207,
          0.0040, 0.0028, 0.0028, 0.0389, 0.0014, 0.0015, 0.0016, 0.0018,
          0.0020, 0.0021, 0.0024, 0.0024, 0.0286, 0.0018, 0.0019, 0.0018],
         [0.0008, 0.0004, 0.0004, 0.0014, 0.0006, 0.0008, 0.0033, 0.0086,
          0.0016, 0.0011, 0.0012, 0.0162, 0.0006, 0.0006, 0.0006, 0.0007,
          0.0008, 0.0009, 0.0010, 0.0010, 0.0119, 0.0007, 0.0008, 0.0007],
         [0.0019, 0.0011, 0.0011, 0.0033, 0.0014, 0.0020, 0.0082, 0.0210,
          0.0040, 0.0028, 0.0029, 0.0395, 0.0014, 0.0015, 0.0016, 0.0018,
          0.0020, 0.0022, 0.0024, 0.0024, 0.0290, 0.0018, 0.0019, 0.0018],
         [0.0021, 0.0012, 0.0012, 0.0037, 0.0015, 0.0022, 0.0090, 0.0231,
          0.0044, 0.0031, 0.0031, 0.0433, 0.0015, 0.0017, 0.0017, 0.0020,
          0.0022, 0.0024, 0.0026, 0.0027, 0.0318, 0.0020, 0.0021, 0.0020],
         [0.0024, 0.0014, 0.0014, 0.0043, 0.0018, 0.0026, 0.0105, 0.0268,
          0.0051, 0.0036, 0.0037, 0.0503, 0.0018, 0.0019, 0.0020, 0.0024,
          0.0026, 0.0028, 0.0031, 0.0031, 0.0370, 0.0023, 0.0024, 0.0023],
         [0.0006, 0.0003, 0.0004, 0.0011, 0.0004, 0.0006, 0.0026, 0.0067,
          0.0013, 0.0009, 0.0009, 0.0128, 0.0004, 0.0005, 0.0005, 0.0006,
          0.0006, 0.0007, 0.0008, 0.0008, 0.0093, 0.0006, 0.0006, 0.0006],
         [0.0009, 0.0005, 0.0005, 0.0016, 0.0007, 0.0010, 0.0039, 0.0101,
          0.0019, 0.0013, 0.0014, 0.0190, 0.0007, 0.0007, 0.0008, 0.0009,
          0.0010, 0.0010, 0.0011, 0.0012, 0.0139, 0.0009, 0.0009, 0.0009],
         [0.0021, 0.0012, 0.0012, 0.0037, 0.0015, 0.0023, 0.0091, 0.0234,
          0.0045, 0.0031, 0.0032, 0.0439, 0.0015, 0.0017, 0.0018, 0.0020,
          0.0023, 0.0024, 0.0027, 0.0027, 0.0322, 0.0020, 0.0021, 0.0020],
         [0.0005, 0.0003, 0.0003, 0.0010, 0.0004, 0.0006, 0.0024, 0.0061,
          0.0012, 0.0008, 0.0008, 0.0115, 0.0004, 0.0004, 0.0005, 0.0005,
          0.0006, 0.0006, 0.0007, 0.0007, 0.0084, 0.0005, 0.0005, 0.0005],
         [0.0005, 0.0003, 0.0003, 0.0010, 0.0004, 0.0006, 0.0024, 0.0061,
          0.0012, 0.0008, 0.0008, 0.0115, 0.0004, 0.0004, 0.0005, 0.0005,
          0.0006, 0.0006, 0.0007, 0.0007, 0.0084, 0.0005, 0.0005, 0.0005],
         [0.0014, 0.0008, 0.0008, 0.0024, 0.0010, 0.0015, 0.0060, 0.0153,
          0.0029, 0.0020, 0.0021, 0.0288, 0.0010, 0.0011, 0.0011, 0.0013,
          0.0015, 0.0016, 0.0017, 0.0018, 0.0211, 0.0013, 0.0014, 0.0013],
         [0.0013, 0.0008, 0.0008, 0.0024, 0.0010, 0.0014, 0.0059, 0.0151,
          0.0029, 0.0020, 0.0020, 0.0284, 0.0010, 0.0011, 0.0011, 0.0013,
          0.0014, 0.0015, 0.0017, 0.0017, 0.0208, 0.0013, 0.0014, 0.0013]]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(3662.4241, grad_fn=<NegBackward>) non event loss:  tensor([719797.5616], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(8.9855, grad_fn=<SumBackward0>)
torch.Size([1, 5, 1])
-1
torch.Size([1, 50, 1])
-1
### event lambdas:  tensor([[[9.1595e-05, 1.1459e-04, 1.0029e-04, 9.4095e-05, 7.2614e-05],
         [1.0285e-04, 1.2867e-04, 1.1262e-04, 1.0566e-04, 8.1540e-05],
         [1.2209e-04, 1.5274e-04, 1.3368e-04, 1.2542e-04, 9.6791e-05],
         [1.3809e-04, 1.7275e-04, 1.5120e-04, 1.4186e-04, 1.0948e-04],
         [2.2547e-04, 2.8206e-04, 2.4686e-04, 2.3162e-04, 1.7875e-04]]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(223.3334, grad_fn=<NegBackward>) non event loss:  tensor([1424.6331], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(2.0423, grad_fn=<SumBackward0>)
torch.Size([1, 16, 1])
-1
torch.Size([1, 160, 1])
-1
### event lambdas:  tensor([[[3.4690e-05, 3.4185e-05, 3.4900e-05, 3.6999e-05, 3.2823e-05,
          3.2148e-05, 6.2473e-05, 2.0894e-05, 2.2005e-05, 2.2253e-05,
          1.9997e-05, 2.1128e-05, 1.8540e-05, 9.8039e-06, 1.0023e-05,
          1.0440e-05],
         [4.6926e-05, 4.6243e-05, 4.7210e-05, 5.0050e-05, 4.4401e-05,
          4.3487e-05, 8.4508e-05, 2.8264e-05, 2.9766e-05, 3.0102e-05,
          2.7050e-05, 2.8580e-05, 2.5080e-05, 1.3262e-05, 1.3558e-05,
          1.4122e-05],
         [2.6095e-05, 2.5715e-05, 2.6253e-05, 2.7831e-05, 2.4690e-05,
          2.4182e-05, 4.6993e-05, 1.5717e-05, 1.6552e-05, 1.6739e-05,
          1.5042e-05, 1.5892e-05, 1.3946e-05, 7.3746e-06, 7.5392e-06,
          7.8531e-06],
         [3.1263e-05, 3.0807e-05, 3.1452e-05, 3.3344e-05, 2.9580e-05,
          2.8971e-05, 5.6300e-05, 1.8830e-05, 1.9830e-05, 2.0054e-05,
          1.8021e-05, 1.9040e-05, 1.6708e-05, 8.8352e-06, 9.0324e-06,
          9.4084e-06],
         [4.7090e-05, 4.6404e-05, 4.7375e-05, 5.0224e-05, 4.4556e-05,
          4.3639e-05, 8.4803e-05, 2.8363e-05, 2.9870e-05, 3.0207e-05,
          2.7145e-05, 2.8680e-05, 2.5167e-05, 1.3308e-05, 1.3605e-05,
          1.4172e-05],
         [3.2525e-05, 3.2052e-05, 3.2722e-05, 3.4690e-05, 3.0775e-05,
          3.0141e-05, 5.8574e-05, 1.9590e-05, 2.0631e-05, 2.0864e-05,
          1.8749e-05, 1.9809e-05, 1.7383e-05, 9.1920e-06, 9.3972e-06,
          9.7884e-06],
         [5.0917e-05, 5.0175e-05, 5.1225e-05, 5.4306e-05, 4.8176e-05,
          4.7185e-05, 9.1693e-05, 3.0668e-05, 3.2297e-05, 3.2662e-05,
          2.9351e-05, 3.1010e-05, 2.7212e-05, 1.4390e-05, 1.4711e-05,
          1.5323e-05],
         [4.6282e-05, 4.5608e-05, 4.6562e-05, 4.9363e-05, 4.3791e-05,
          4.2890e-05, 8.3348e-05, 2.7876e-05, 2.9357e-05, 2.9689e-05,
          2.6679e-05, 2.8188e-05, 2.4736e-05, 1.3080e-05, 1.3372e-05,
          1.3929e-05],
         [5.3131e-05, 5.2358e-05, 5.3453e-05, 5.6668e-05, 5.0272e-05,
          4.9237e-05, 9.5682e-05, 3.2001e-05, 3.3702e-05, 3.4082e-05,
          3.0627e-05, 3.2359e-05, 2.8396e-05, 1.5016e-05, 1.5351e-05,
          1.5990e-05],
         [2.4660e-05, 2.4301e-05, 2.4810e-05, 2.6302e-05, 2.3333e-05,
          2.2853e-05, 4.4410e-05, 1.4853e-05, 1.5642e-05, 1.5819e-05,
          1.4215e-05, 1.5019e-05, 1.3180e-05, 6.9693e-06, 7.1248e-06,
          7.4214e-06],
         [2.4509e-05, 2.4152e-05, 2.4657e-05, 2.6140e-05, 2.3190e-05,
          2.2713e-05, 4.4138e-05, 1.4762e-05, 1.5546e-05, 1.5722e-05,
          1.4128e-05, 1.4927e-05, 1.3099e-05, 6.9265e-06, 7.0811e-06,
          7.3759e-06],
         [2.1924e-05, 2.1605e-05, 2.2057e-05, 2.3384e-05, 2.0744e-05,
          2.0317e-05, 3.9483e-05, 1.3205e-05, 1.3907e-05, 1.4064e-05,
          1.2638e-05, 1.3353e-05, 1.1717e-05, 6.1960e-06, 6.3343e-06,
          6.5980e-06],
         [4.7986e-05, 4.7287e-05, 4.8276e-05, 5.1179e-05, 4.5403e-05,
          4.4468e-05, 8.6415e-05, 2.8902e-05, 3.0438e-05, 3.0781e-05,
          2.7661e-05, 2.9225e-05, 2.5646e-05, 1.3561e-05, 1.3864e-05,
          1.4441e-05],
         [3.2292e-05, 3.1822e-05, 3.2487e-05, 3.4441e-05, 3.0554e-05,
          2.9925e-05, 5.8153e-05, 1.9450e-05, 2.0483e-05, 2.0714e-05,
          1.8614e-05, 1.9667e-05, 1.7258e-05, 9.1260e-06, 9.3297e-06,
          9.7181e-06],
         [4.7152e-05, 4.6466e-05, 4.7438e-05, 5.0291e-05, 4.4614e-05,
          4.3696e-05, 8.4914e-05, 2.8400e-05, 2.9909e-05, 3.0247e-05,
          2.7181e-05, 2.8717e-05, 2.5200e-05, 1.3326e-05, 1.3623e-05,
          1.4190e-05],
         [3.1438e-05, 3.0980e-05, 3.1628e-05, 3.3531e-05, 2.9746e-05,
          2.9134e-05, 5.6616e-05, 1.8935e-05, 1.9941e-05, 2.0166e-05,
          1.8122e-05, 1.9147e-05, 1.6802e-05, 8.8847e-06, 9.0830e-06,
          9.4612e-06]]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(2720.1790, grad_fn=<NegBackward>) non event loss:  tensor([3215.2609], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.0387, grad_fn=<SumBackward0>)
torch.Size([1, 12, 1])
-1
torch.Size([1, 120, 1])
-1
### event lambdas:  tensor([[[1.1579e-05, 1.1998e-05, 7.5234e-06, 1.3624e-05, 5.8838e-05,
          1.5734e-06, 2.0397e-06, 2.0460e-06, 2.0538e-06, 1.3949e-06,
          2.1165e-06, 2.1188e-06],
         [1.3588e-05, 1.4080e-05, 8.8284e-06, 1.5987e-05, 6.9044e-05,
          1.8463e-06, 2.3935e-06, 2.4009e-06, 2.4101e-06, 1.6369e-06,
          2.4837e-06, 2.4864e-06],
         [1.6604e-05, 1.7205e-05, 1.0788e-05, 1.9536e-05, 8.4369e-05,
          2.2561e-06, 2.9248e-06, 2.9338e-06, 2.9450e-06, 2.0002e-06,
          3.0350e-06, 3.0383e-06],
         [1.3588e-05, 1.4080e-05, 8.8284e-06, 1.5987e-05, 6.9044e-05,
          1.8463e-06, 2.3935e-06, 2.4009e-06, 2.4101e-06, 1.6369e-06,
          2.4837e-06, 2.4864e-06],
         [1.2756e-05, 1.3217e-05, 8.2876e-06, 1.5008e-05, 6.4815e-05,
          1.7332e-06, 2.2469e-06, 2.2538e-06, 2.2624e-06, 1.5366e-06,
          2.3315e-06, 2.3341e-06],
         [1.4170e-05, 1.4683e-05, 9.2064e-06, 1.6672e-05, 7.2000e-05,
          1.9253e-06, 2.4960e-06, 2.5037e-06, 2.5133e-06, 1.7069e-06,
          2.5900e-06, 2.5928e-06],
         [1.1005e-05, 1.1403e-05, 7.1503e-06, 1.2948e-05, 5.5920e-05,
          1.4953e-06, 1.9385e-06, 1.9445e-06, 1.9520e-06, 1.3257e-06,
          2.0116e-06, 2.0138e-06],
         [1.0465e-05, 1.0844e-05, 6.7997e-06, 1.2313e-05, 5.3178e-05,
          1.4220e-06, 1.8435e-06, 1.8492e-06, 1.8562e-06, 1.2607e-06,
          1.9129e-06, 1.9150e-06],
         [9.3964e-06, 9.7365e-06, 6.1051e-06, 1.1056e-05, 4.7746e-05,
          1.2768e-06, 1.6552e-06, 1.6603e-06, 1.6666e-06, 1.1319e-06,
          1.7175e-06, 1.7194e-06],
         [1.7365e-05, 1.7993e-05, 1.1282e-05, 2.0431e-05, 8.8235e-05,
          2.3595e-06, 3.0588e-06, 3.0682e-06, 3.0800e-06, 2.0918e-06,
          3.1740e-06, 3.1775e-06],
         [1.4445e-05, 1.4968e-05, 9.3853e-06, 1.6996e-05, 7.3399e-05,
          1.9627e-06, 2.5445e-06, 2.5523e-06, 2.5621e-06, 1.7401e-06,
          2.6403e-06, 2.6432e-06],
         [1.6426e-05, 1.7021e-05, 1.0672e-05, 1.9327e-05, 8.3465e-05,
          2.2319e-06, 2.8934e-06, 2.9024e-06, 2.9135e-06, 1.9788e-06,
          3.0024e-06, 3.0057e-06]]], grad_fn=<SoftplusBackward>)
  1%|▊                                                                                                                                               | 3/496 [00:00<02:29,  3.30it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 107, in train
    loss, pos_timing_loss, neg_timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 195, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 364, in forward
    z_vt_i = self.gnn_choice(features_i.squeeze(), edges_i.squeeze()) # (N_i_1, embedding_z)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 469, in forward
    x = F.dropout(x, p=self.dropout)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 1076, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 197, in format_stack
    return format_list(extract_stack(f, limit=limit))
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 211, in extract_stack
    stack = StackSummary.extract(walk_stack(f), limit=limit)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 347, in extract
    for f, lineno in frame_gen:
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/traceback.py", line 301, in walk_stack
    yield f, f.f_lineno
KeyboardInterrupt