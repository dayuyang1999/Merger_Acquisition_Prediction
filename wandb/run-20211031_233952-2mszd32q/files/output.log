
CUDA availability: True
  1%|█▍                                                                                                                                              | 5/496 [00:00<01:42,  4.81it/s]
-1
-1
### event lambdas:  tensor([[[0.1278, 0.1096, 0.0986, 0.0952, 0.0814, 0.0817, 0.0817, 0.0817,
          0.7802, 0.1543]],
        [[0.0305, 0.0260, 0.0233, 0.0225, 0.0191, 0.0192, 0.0192, 0.0192,
          0.2380, 0.0372]],
        [[0.0396, 0.0338, 0.0302, 0.0292, 0.0248, 0.0249, 0.0249, 0.0249,
          0.3002, 0.0482]],
        [[0.0684, 0.0584, 0.0524, 0.0505, 0.0431, 0.0432, 0.0432, 0.0432,
          0.4783, 0.0830]],
        [[0.1228, 0.1053, 0.0946, 0.0914, 0.0781, 0.0784, 0.0784, 0.0784,
          0.7572, 0.1482]],
        [[0.1376, 0.1181, 0.1062, 0.1026, 0.0878, 0.0881, 0.0881, 0.0881,
          0.8236, 0.1659]],
        [[0.0849, 0.0726, 0.0651, 0.0628, 0.0536, 0.0538, 0.0538, 0.0538,
          0.5696, 0.1028]],
        [[0.1284, 0.1102, 0.0990, 0.0956, 0.0818, 0.0821, 0.0821, 0.0820,
          0.7829, 0.1550]],
        [[0.0383, 0.0326, 0.0292, 0.0282, 0.0240, 0.0241, 0.0240, 0.0240,
          0.2912, 0.0466]],
        [[0.0620, 0.0529, 0.0475, 0.0458, 0.0390, 0.0392, 0.0391, 0.0391,
          0.4412, 0.0754]]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(265.4411, grad_fn=<NegBackward>) non event loss:  tensor([35794.9427], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(4.1930, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[[0.0072, 0.0054, 0.0038, 0.0309, 0.0060]],
        [[0.0066, 0.0050, 0.0035, 0.0284, 0.0055]],
        [[0.0070, 0.0053, 0.0037, 0.0304, 0.0059]],
        [[0.0053, 0.0040, 0.0028, 0.0228, 0.0044]],
        [[0.0198, 0.0150, 0.0104, 0.0836, 0.0165]]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(118.6815, grad_fn=<NegBackward>) non event loss:  tensor([2915.6733], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.9427, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[[0.0036, 0.0024, 0.0011, 0.0011, 0.0010, 0.0009]],
        [[0.0019, 0.0013, 0.0006, 0.0006, 0.0005, 0.0004]],
        [[0.0017, 0.0011, 0.0005, 0.0005, 0.0005, 0.0004]],
        [[0.0021, 0.0014, 0.0006, 0.0006, 0.0006, 0.0005]],
        [[0.0013, 0.0009, 0.0004, 0.0004, 0.0004, 0.0003]],
        [[0.0040, 0.0027, 0.0012, 0.0012, 0.0011, 0.0010]]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(252.8655, grad_fn=<NegBackward>) non event loss:  tensor([354.8646], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(1.4517, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[[2.8285e-04, 1.9400e-04, 9.2516e-05, 3.6846e-05, 8.9704e-05,
          8.9345e-05]],
        [[3.3548e-04, 2.3009e-04, 1.0973e-04, 4.3702e-05, 1.0639e-04,
          1.0597e-04]],
        [[3.5938e-04, 2.4649e-04, 1.1755e-04, 4.6816e-05, 1.1397e-04,
          1.1352e-04]],
        [[4.9915e-04, 3.4236e-04, 1.6327e-04, 6.5028e-05, 1.5831e-04,
          1.5768e-04]],
        [[5.3243e-04, 3.6518e-04, 1.7416e-04, 6.9364e-05, 1.6887e-04,
          1.6819e-04]],
        [[1.0260e-03, 7.0379e-04, 3.3568e-04, 1.3370e-04, 3.2547e-04,
          3.2417e-04]]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(311.7462, grad_fn=<NegBackward>) non event loss:  tensor([99.8547], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0809, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[[2.4940e-04, 6.2734e-05, 7.6845e-05, 4.7199e-05, 4.0405e-04,
          3.0472e-05, 2.9348e-05, 2.4096e-05, 8.6133e-05, 1.3839e-04,
          5.5833e-04, 4.5649e-05]],
        [[4.6611e-04, 1.1726e-04, 1.4363e-04, 8.8220e-05, 7.5509e-04,
          5.6956e-05, 5.4855e-05, 4.5039e-05, 1.6099e-04, 2.5866e-04,
          1.0433e-03, 8.5322e-05]],
        [[7.7877e-04, 1.9593e-04, 2.4000e-04, 1.4741e-04, 1.2615e-03,
          9.5173e-05, 9.1664e-05, 7.5261e-05, 2.6900e-04, 4.3220e-04,
          1.7428e-03, 1.4257e-04]],
        [[4.8343e-04, 1.2161e-04, 1.4897e-04, 9.1497e-05, 7.8313e-04,
          5.9072e-05, 5.6894e-05, 4.6713e-05, 1.6697e-04, 2.6827e-04,
          1.0821e-03, 8.8492e-05]],
        [[3.9352e-04, 9.8993e-05, 1.2126e-04, 7.4479e-05, 6.3751e-04,
          4.8084e-05, 4.6311e-05, 3.8024e-05, 1.3591e-04, 2.1838e-04,
          8.8090e-04, 7.2033e-05]],
        [[6.3097e-04, 1.5874e-04, 1.9444e-04, 1.1943e-04, 1.0221e-03,
          7.7106e-05, 7.4263e-05, 6.0974e-05, 2.1794e-04, 3.5016e-04,
          1.4122e-03, 1.1551e-04]],
        [[5.5478e-04, 1.3957e-04, 1.7096e-04, 1.0501e-04, 8.9870e-04,
          6.7793e-05, 6.5293e-05, 5.3609e-05, 1.9162e-04, 3.0788e-04,
          1.2417e-03, 1.0156e-04]],
        [[4.6568e-04, 1.1715e-04, 1.4350e-04, 8.8139e-05, 7.5440e-04,
          5.6903e-05, 5.4805e-05, 4.4998e-05, 1.6084e-04, 2.5843e-04,
          1.0424e-03, 8.5244e-05]],
        [[7.3869e-04, 1.8585e-04, 2.2765e-04, 1.3983e-04, 1.1966e-03,
          9.0274e-05, 8.6945e-05, 7.1387e-05, 2.5516e-04, 4.0995e-04,
          1.6532e-03, 1.3523e-04]],
        [[6.3592e-04, 1.5999e-04, 1.9597e-04, 1.2037e-04, 1.0301e-03,
          7.7711e-05, 7.4846e-05, 6.1453e-05, 2.1965e-04, 3.5291e-04,
          1.4233e-03, 1.1641e-04]],
        [[6.7822e-04, 1.7063e-04, 2.0901e-04, 1.2838e-04, 1.0986e-03,
          8.2882e-05, 7.9826e-05, 6.5541e-05, 2.3427e-04, 3.7639e-04,
          1.5179e-03, 1.2416e-04]],
        [[3.5182e-04, 8.8502e-05, 1.0841e-04, 6.6586e-05, 5.6997e-04,
          4.2988e-05, 4.1403e-05, 3.3994e-05, 1.2151e-04, 1.9524e-04,
          7.8757e-04, 6.4399e-05]]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(1245.8308, grad_fn=<NegBackward>) non event loss:  tensor([63.1602], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0155, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[[3.8399e-05, 4.4832e-06, 2.0795e-05, 4.7610e-06, 2.2287e-05,
          2.3257e-05, 7.0301e-05]],
        [[1.0684e-04, 1.2474e-05, 5.7858e-05, 1.3247e-05, 6.2009e-05,
          6.4707e-05, 1.9559e-04]],
        [[1.2772e-04, 1.4912e-05, 6.9167e-05, 1.5836e-05, 7.4130e-05,
          7.7356e-05, 2.3382e-04]],
        [[1.2536e-04, 1.4637e-05, 6.7890e-05, 1.5544e-05, 7.2761e-05,
          7.5927e-05, 2.2950e-04]],
        [[5.8473e-05, 6.8270e-06, 3.1666e-05, 7.2501e-06, 3.3938e-05,
          3.5415e-05, 1.0705e-04]],
        [[1.2847e-04, 1.5000e-05, 6.9574e-05, 1.5929e-05, 7.4565e-05,
          7.7810e-05, 2.3519e-04]],
        [[4.3138e-05, 5.0365e-06, 2.3361e-05, 5.3486e-06, 2.5037e-05,
          2.6127e-05, 7.8977e-05]]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(499.1615, grad_fn=<NegBackward>) non event loss:  tensor([29.3374], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0252, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[[6.2631e-06, 6.2261e-06, 1.4832e-06, 1.3269e-06, 1.3261e-06,
          2.3204e-06]],
        [[1.4898e-05, 1.4810e-05, 3.5280e-06, 3.1563e-06, 3.1544e-06,
          5.5194e-06]],
        [[1.4488e-05, 1.4403e-05, 3.4310e-06, 3.0695e-06, 3.0676e-06,
          5.3676e-06]],
        [[7.3315e-06, 7.2882e-06, 1.7362e-06, 1.5533e-06, 1.5523e-06,
          2.7162e-06]],
        [[1.0597e-05, 1.0534e-05, 2.5095e-06, 2.2450e-06, 2.2437e-06,
          3.9260e-06]],
        [[1.1478e-05, 1.1410e-05, 2.7182e-06, 2.4318e-06, 2.4303e-06,
          4.2525e-06]]], grad_fn=<SoftplusBackward>)
##### event loss: tensor(446.6055, grad_fn=<NegBackward>) non event loss:  tensor([6.1080], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0207, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[[2.7192e-05, 3.3475e-05, 4.2238e-06, 2.3279e-05]],
        [[9.1914e-06, 1.1315e-05, 1.4277e-06, 7.8688e-06]],
        [[2.3565e-05, 2.9010e-05, 3.6605e-06, 2.0174e-05]],
        [[2.0750e-05, 2.5544e-05, 3.2232e-06, 1.7764e-05]]],
       grad_fn=<SoftplusBackward>)
##### event loss: tensor(181.4328, grad_fn=<NegBackward>) non event loss:  tensor([1.4728], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0116, grad_fn=<SumBackward0>)
-1
-1
### event lambdas:  tensor([[[2.0366e-06, 3.4932e-06, 2.7273e-06, 4.5720e-06, 4.4452e-06,
          4.5779e-06, 4.9529e-07, 3.1073e-07, 2.6694e-07, 1.2334e-06,
          7.6299e-07, 5.3930e-07, 3.4124e-06]],
        [[1.4557e-06, 2.4969e-06, 1.9494e-06, 3.2681e-06, 3.1774e-06,
          3.2723e-06, 3.5403e-07, 2.2211e-07, 1.9081e-07, 8.8165e-07,
          5.4538e-07, 3.8549e-07, 2.4392e-06]],
        [[1.9935e-06, 3.4193e-06, 2.6696e-06, 4.4753e-06, 4.3512e-06,
          4.4810e-06, 4.8481e-07, 3.0415e-07, 2.6129e-07, 1.2073e-06,
          7.4684e-07, 5.2789e-07, 3.3402e-06]],
        [[1.3223e-06, 2.2681e-06, 1.7708e-06, 2.9685e-06, 2.8862e-06,
          2.9723e-06, 3.2158e-07, 2.0175e-07, 1.7332e-07, 8.0083e-07,
          4.9539e-07, 3.5015e-07, 2.2156e-06]],
        [[1.2002e-06, 2.0587e-06, 1.6073e-06, 2.6945e-06, 2.6197e-06,
          2.6979e-06, 2.9189e-07, 1.8312e-07, 1.5732e-07, 7.2690e-07,
          4.4966e-07, 3.1783e-07, 2.0111e-06]],
        [[1.2008e-06, 2.0597e-06, 1.6081e-06, 2.6958e-06, 2.6210e-06,
          2.6992e-06, 2.9203e-07, 1.8321e-07, 1.5739e-07, 7.2726e-07,
          4.4988e-07, 3.1798e-07, 2.0120e-06]],
        [[4.3634e-06, 7.4844e-06, 5.8434e-06, 9.7958e-06, 9.5242e-06,
          9.8084e-06, 1.0612e-06, 6.6575e-07, 5.7193e-07, 2.6427e-06,
          1.6348e-06, 1.1555e-06, 7.3113e-06]],
        [[1.5975e-06, 2.7401e-06, 2.1393e-06, 3.5863e-06, 3.4869e-06,
          3.5909e-06, 3.8851e-07, 2.4373e-07, 2.0939e-07, 9.6750e-07,
          5.9849e-07, 4.2303e-07, 2.6767e-06]],
        [[3.5057e-06, 6.0132e-06, 4.6947e-06, 7.8702e-06, 7.6520e-06,
          7.8803e-06, 8.5258e-07, 5.3488e-07, 4.5950e-07, 2.1232e-06,
          1.3134e-06, 9.2834e-07, 5.8741e-06]],
        [[9.7749e-07, 1.6767e-06, 1.3090e-06, 2.1944e-06, 2.1336e-06,
          2.1973e-06, 2.3773e-07, 1.4914e-07, 1.2812e-07, 5.9201e-07,
          3.6621e-07, 2.5885e-07, 1.6379e-06]],
        [[1.3561e-06, 2.3261e-06, 1.8161e-06, 3.0445e-06, 2.9601e-06,
          3.0484e-06, 3.2981e-07, 2.0691e-07, 1.7775e-07, 8.2134e-07,
          5.0808e-07, 3.5912e-07, 2.2723e-06]],
        [[9.7749e-07, 1.6767e-06, 1.3090e-06, 2.1944e-06, 2.1336e-06,
          2.1973e-06, 2.3773e-07, 1.4914e-07, 1.2812e-07, 5.9201e-07,
          3.6621e-07, 2.5885e-07, 1.6379e-06]],
        [[1.0704e-06, 1.8360e-06, 1.4335e-06, 2.4030e-06, 2.3364e-06,
          2.4061e-06, 2.6032e-07, 1.6332e-07, 1.4030e-07, 6.4828e-07,
          4.0102e-07, 2.8345e-07, 1.7936e-06]]], grad_fn=<SoftplusBackward>)
  2%|██▎                                                                                                                                             | 8/496 [00:01<01:45,  4.64it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 110, in train
    loss.backward() # required_graph = True
KeyboardInterrupt