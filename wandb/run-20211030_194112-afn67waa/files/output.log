
CUDA availability: True
#### arr_b tensor([[[ 1.5904,  0.2124, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 1.8151,  0.2835, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 2.1842,  0.3637, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 2.5344,  0.4419, -0.0828, -0.3458, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 2.7604,  0.5643, -0.2876, -0.3458, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.1728,  0.5475, -0.2813, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.7430,  0.5831, -0.2825, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 4.2565,  0.6037, -0.2558, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.2019,  0.4776, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.2019,  0.4776, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.2019,  0.4776, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 5.3075,  0.8605, -0.2339, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.2019,  0.4776, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 5.2114,  0.7614,  0.1678, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 4.9939,  0.7137,  0.2154, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.2019,  0.4776, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.2019,  0.4776, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.2019,  0.4776, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 4.2627,  0.6930,  0.2953, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459],
         [ 3.2019,  0.3258, -0.0828, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3458, -0.3459, -0.3459, -0.3459]]]) torch.Size([1, 20, 14])
#### mat_b tensor([[[0.0000, 0.0000, 0.2365, 0.0000, 0.0587, 0.0000, 0.0000, 0.0000,
          0.0881, 0.0000, 0.0000, 0.2933, 0.0000, 0.1991, 0.0000, 0.0000,
          0.1812, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0173, 0.0000,
          0.0000, 0.0133, 0.0000, 0.0000, 0.0426, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0866, 0.0000, 0.0000, 0.3166,
          0.0000, 0.0000, 0.0000, 0.0000, 0.1401, 0.1406, 0.0000, 0.0550,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.1342, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.2045, 0.0000, 0.1669, 0.0000, 0.0000, 0.0000,
          0.0930, 0.0000, 0.0000, 0.3629, 0.1933, 0.0000, 0.0000, 0.0000,
          0.1624, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0417, 0.0000,
          0.0000, 0.0579, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.2306, 0.0000, 0.2624, 0.2591, 0.0000, 0.0000, 0.0828,
          0.0000, 0.1379, 0.0000, 0.3166, 0.4668, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.3227, 0.0636, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.5269, 0.2161, 0.0000, 0.2173, 0.1871,
          0.0000, 0.1338, 0.0000, 0.0000, 0.4741, 0.1541, 0.0000, 0.0062,
          0.0000, 0.0000, 0.0000, 0.1096, 0.1208, 0.0000, 0.0000, 0.0000,
          0.0000, 0.3401, 0.0271, 0.0000, 0.0089, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.2390, 0.0883, 0.2907, 0.0000, 0.0000, 0.0000, 0.4735,
          0.0000, 0.4515, 0.0000, 0.2784, 0.5957, 0.0198, 0.0000, 0.0000,
          0.2774, 0.3169, 0.0000, 0.0000, 0.0998, 0.0000, 0.0000, 0.0000,
          0.0000, 0.1590, 0.0704, 0.0000, 0.1986, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3600, 0.0117, 0.0000, 0.0000, 0.0000, 0.0000, 0.7601,
          0.2922, 0.0000, 0.0000, 0.4287, 0.3434, 0.0000, 0.0575, 0.0000,
          0.1279, 0.0000, 0.0000, 0.0639, 0.4067, 0.0000, 0.0234, 0.0000,
          0.0224, 0.0000, 0.0116, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0024, 0.0000, 0.0109, 0.0000, 0.0000, 0.2570, 0.7175,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1028, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0871, 0.0000, 0.0000, 0.0000, 0.0000,
          0.1827, 0.0569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0121, 0.1326, 0.0000, 0.2809, 0.0000, 0.0114, 0.2699,
          0.1946, 0.0000, 0.0053, 0.4948, 0.4805, 0.0000, 0.0000, 0.0000,
          0.3863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0761, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0792],
         [0.0000, 0.0000, 0.4311, 0.0000, 0.0000, 0.0000, 0.0000, 0.3646,
          0.0000, 0.1793, 0.3319, 0.4521, 0.0000, 0.0000, 0.0000, 0.0000,
          0.2568, 0.0000, 0.1435, 0.3891, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0595, 0.0000, 0.1624, 0.0000, 0.0000, 0.0000, 0.0000, 0.0896],
         [0.0000, 0.1002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6023,
          0.0063, 0.3695, 0.0000, 0.4711, 0.2168, 0.0850, 0.0481, 0.0000,
          0.0421, 0.0084, 0.0000, 0.0000, 0.3831, 0.0000, 0.0000, 0.0000,
          0.1175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.2057, 0.0000, 0.0000, 0.0000,
          0.2735, 0.6625, 0.1502, 0.0000, 0.8778, 0.0000, 0.0000, 0.0000,
          0.2945, 0.0000, 0.0000, 0.0000, 0.5725, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0915, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.3005, 0.0636, 0.0000, 0.0000, 0.0000, 0.0000, 0.2647,
          0.0000, 0.1313, 0.0000, 0.0000, 0.2773, 0.0000, 0.0000, 0.1364,
          0.1021, 0.0071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0826, 0.0000, 0.2922, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.3841, 0.0493, 0.0000, 0.0000,
          0.0834, 0.0000, 0.0000, 0.4418, 0.4175, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.1765, 0.1256, 0.0000, 0.0000, 0.0000,
          0.0195, 0.3608, 0.0429, 0.0000, 0.4282, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.2360, 0.0000, 0.1933, 0.0000, 0.0000, 0.2430, 0.3317,
          0.1544, 0.7695, 0.0000, 0.4830, 0.0000, 0.0000, 0.0000, 0.0000,
          0.4860, 0.5149, 0.0000, 0.1116, 0.0000, 0.0000, 0.0000, 0.0000,
          0.1882, 0.0000, 0.0000, 0.0000, 0.6780, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0465, 0.0000, 0.1665, 0.0000, 0.0000, 0.4615,
          0.0000, 0.0000, 0.0000, 0.3611, 0.0000, 0.0000, 0.0000, 0.0000,
          0.1003, 0.0000, 0.1796, 0.1452, 0.0527, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0814, 0.0602, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.3689, 0.0000, 0.1493, 0.0000, 0.0000, 0.0000,
          0.1456, 0.2645, 0.2071, 0.5424, 0.2571, 0.0000, 0.0000, 0.0000,
          0.3674, 0.0000, 0.0562, 0.2241, 0.0000, 0.0000, 0.1470, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0727, 0.1660, 0.0000, 0.2129, 0.0000, 0.0000, 0.0000,
          0.0000, 0.2093, 0.0000, 0.5626, 0.3394, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0163, 0.1109, 0.0000, 0.0000, 0.2559, 0.0000,
          0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.3483, 0.0000, 0.0000, 0.0000,
          0.0000, 0.2775, 0.0000, 0.5177, 0.1170, 0.0206, 0.0000, 0.0305,
          0.0814, 0.0000, 0.0000, 0.3815, 0.0583, 0.0000, 0.0000, 0.0000,
          0.1151, 0.0000, 0.1225, 0.0000, 0.0000, 0.0000, 0.0000, 0.1419],
         [0.0000, 0.0000, 0.0473, 0.0403, 0.0000, 0.0000, 0.2896, 0.3455,
          0.0438, 0.4270, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0885,
          0.2032, 0.0000, 0.0000, 0.2889, 0.2097, 0.0000, 0.0489, 0.0000,
          0.0000, 0.1118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1097]]],
       grad_fn=<ReluBackward0>) torch.Size([1, 20, 32])
#### arr_b tensor([[[ 1.6416,  0.2271, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.8722,  0.3001, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.2511,  0.3824, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.6105,  0.4627, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 2.8426,  0.5884, -0.2861, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2659,  0.5711, -0.2796, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.8512,  0.6076, -0.2809, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3783,  0.6288, -0.2535, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.4571,  0.8924, -0.2310, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.3584,  0.7907,  0.1813, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 5.1351,  0.7417,  0.2302, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4994, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 4.3846,  0.7205,  0.3123, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3435, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.4078, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3785, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 3.2958,  0.3619, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459],
         [ 1.5889,  0.3488, -0.0759, -0.3459, -0.3459, -0.3459, -0.3459,
          -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459, -0.3459]]]) torch.Size([1, 24, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<ReluBackward0>) torch.Size([1, 24, 32])
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 201, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)*1000
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 361, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1