
CUDA availability: True
  1%|██                                                                                                                                                                                                               | 5/496 [00:01<01:51,  4.41it/s]
##### event loss: tensor(-7.6442, grad_fn=<NegBackward>) non event loss:  tensor([24136.9294], dtype=torch.float64, grad_fn=<MulBackward0>)
##### event loss: tensor(31.2776, grad_fn=<NegBackward>) non event loss:  tensor([952.1994], dtype=torch.float64, grad_fn=<MulBackward0>)
##### event loss: tensor(145.1929, grad_fn=<NegBackward>) non event loss:  tensor([2.8006], dtype=torch.float64, grad_fn=<MulBackward0>)
##### event loss: tensor(158.9695, grad_fn=<NegBackward>) non event loss:  tensor([0.0331], dtype=torch.float64, grad_fn=<MulBackward0>)
##### event loss: tensor(259.3464, grad_fn=<NegBackward>) non event loss:  tensor([0.0002], dtype=torch.float64, grad_fn=<MulBackward0>)
  1%|██                                                                                                                                                                                                               | 5/496 [00:01<01:51,  4.41it/s]/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in SoftplusBackward. Traceback of forward call that caused the error:
  File "main.py", line 126, in <module>
    main()
  File "main.py", line 122, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 81, in train
    loss, timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 184, in forward
    event_lambdas = self.timing_net(mat_b, mat_c, event_data) # (L3, )
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 277, in forward
    lambda_dt = F.softplus(torch.transpose(rate, dim0=0, dim1=1))
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378062065/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(
  1%|██                                                                                                                                                                                                               | 5/496 [00:01<02:05,  3.91it/s]
Traceback (most recent call last):
  File "main.py", line 126, in <module>
    main()
  File "main.py", line 122, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 84, in train
    loss.backward() # required_graph = True
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Function 'SoftplusBackward' returned nan values in its 0th output.