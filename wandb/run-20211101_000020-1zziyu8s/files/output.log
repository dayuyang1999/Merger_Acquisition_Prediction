
CUDA availability: True
### event lambdas:  tensor([[1.0575, 1.3938, 2.2745, 2.7383, 2.6234, 2.8007, 2.8401, 3.2258, 2.6112,
         2.8808, 2.3459, 1.8686, 3.0161, 2.9812, 2.3749, 3.0014, 2.5143]],
       grad_fn=<SoftplusBackward>)
##### event loss: -15.00439453125 non event loss:  3809902.2728271484 chocie_l: 7.201343059539795
### event lambdas:  tensor([[0.5907, 0.4052, 0.2308, 0.6049, 0.2549, 0.5545, 0.7295, 0.4385, 0.9765,
         0.9989, 0.7998]], grad_fn=<SoftplusBackward>)
##### event loss: 6.743422508239746 non event loss:  511303.99809265137 chocie_l: 3.0265636444091797
### event lambdas:  tensor([[0.0388, 0.0335, 0.0151, 0.0341, 0.0726, 0.0618, 0.0496, 0.0251, 0.0684,
         0.0250, 0.0276, 0.0294]], grad_fn=<SoftplusBackward>)
##### event loss: 39.80046844482422 non event loss:  43762.91296005249 chocie_l: 0.8259614109992981
### event lambdas:  tensor([[0.0350, 0.0268, 0.0082, 0.0066, 0.0143, 0.0073, 0.0053, 0.0064, 0.0097,
         0.0022, 0.0092, 0.0094, 0.0133, 0.0041, 0.0038, 0.0138, 0.0132, 0.0052,
         0.0121, 0.0089, 0.0096, 0.0074, 0.0217, 0.0381, 0.0097, 0.0061, 0.0075,
         0.0256, 0.0117, 0.0048, 0.0248, 0.0083, 0.0105, 0.0055, 0.0111, 0.0015,
         0.0038, 0.0088, 0.0131, 0.0075, 0.0064, 0.0069, 0.0043, 0.0010, 0.0037,
         0.0033, 0.0071, 0.0047, 0.0056, 0.0040, 0.0056, 0.0075, 0.0050, 0.0222,
         0.0086, 0.0128, 0.0056, 0.0112, 0.0094, 0.0063, 0.0159, 0.0039, 0.0045,
         0.0068, 0.0060, 0.0115, 0.0089, 0.0078, 0.0084, 0.0042, 0.0078, 0.0139,
         0.0130, 0.0110, 0.0107, 0.0076, 0.0137, 0.0164, 0.0031, 0.0050, 0.0022,
         0.0063, 0.0105, 0.0053, 0.0085, 0.0052, 0.0029, 0.0019, 0.0016, 0.0064,
         0.0057, 0.0016, 0.0026, 0.0093, 0.0086, 0.0102, 0.0184, 0.0087, 0.0050]],
       grad_fn=<SoftplusBackward>)
  1%|█                                                                                                                                                                                   | 3/496 [00:00<01:58,  4.15it/s]
##### event loss: 487.32696533203125 non event loss:  62351.401723861694 chocie_l: 0.1260877251625061
### event lambdas:  tensor([[0.0015, 0.0010, 0.0007, 0.0012, 0.0008, 0.0007, 0.0007, 0.0012, 0.0002,
         0.0006, 0.0003, 0.0005, 0.0005, 0.0006, 0.0010, 0.0010, 0.0017]],
       grad_fn=<SoftplusBackward>)
##### event loss: 122.41551208496094 non event loss:  1051.4168328344822 chocie_l: 0.023465152829885483
### event lambdas:  tensor([[2.3383e-04, 2.9910e-04, 4.1929e-04, 1.6066e-04, 1.1173e-04, 1.9862e-04,
         2.2498e-04, 3.0965e-05, 5.9378e-05, 2.9720e-04, 1.0285e-04, 1.6565e-04,
         2.9865e-04, 1.9972e-04]], grad_fn=<SoftplusBackward>)
##### event loss: 121.76995086669922 non event loss:  259.4840306043625 chocie_l: 0.06374341994524002
### event lambdas:  tensor([[5.3004e-05, 4.7307e-05, 2.0062e-04, 1.7718e-05, 8.2488e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: 48.661869049072266 non event loss:  19.373037470970303 chocie_l: 0.029108162969350815
### event lambdas:  tensor([[3.6176e-05, 2.4938e-05, 8.2502e-06, 1.0944e-05, 2.0378e-05, 2.0313e-05,
         1.9226e-05, 3.5588e-05, 2.7384e-05, 2.0725e-05]],
       grad_fn=<SoftplusBackward>)
##### event loss: 107.9520492553711 non event loss:  19.783666212810203 chocie_l: 0.08540985733270645
### event lambdas:  tensor([[3.5028e-06, 3.4764e-06, 1.6761e-06, 1.9383e-06, 3.5712e-07, 6.1568e-07,
         7.9180e-06, 2.5554e-06, 2.0585e-06, 3.0508e-06, 2.0939e-06, 3.9965e-06,
         1.7627e-06, 2.6347e-06, 3.9604e-06, 1.1160e-06, 1.8139e-06, 1.7707e-06,
         3.0804e-06, 4.4740e-06]], grad_fn=<SoftplusBackward>)
  2%|██▉                                                                                                                                                                                 | 8/496 [00:02<02:43,  2.98it/s]
Traceback (most recent call last):
  File "main.py", line 156, in <module>
    main()
  File "main.py", line 152, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 110, in train
    loss.backward() # required_graph = True
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt