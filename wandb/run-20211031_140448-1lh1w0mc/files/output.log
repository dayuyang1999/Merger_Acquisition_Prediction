
CUDA availability: True
  1%|██                                                                                                                                                                                                               | 5/496 [00:00<01:02,  7.83it/s]
### event lambdas:  tensor([[5.2454e+00],
        [4.8832e+01],
        [2.6390e-01],
        [4.1891e-01],
        [1.1394e+01],
        [2.6510e+02]], grad_fn=<AddBackward0>)
##### event loss: tensor(-11.3567, grad_fn=<NegBackward>) non event loss:  tensor([1009677.0605], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(2.8052, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[3.4148e+00],
        [8.1614e-03],
        [3.7448e+01],
        [3.8774e+00],
        [9.3585e+00],
        [1.4226e+00],
        [2.6912e+01],
        [8.3222e-04],
        [1.5423e+01],
        [9.8654e+00],
        [2.1354e-01],
        [4.5058e+01],
        [3.9243e-03],
        [7.2809e+00],
        [4.5387e-02],
        [7.4620e+00],
        [1.3550e-03],
        [2.8810e+00]], grad_fn=<AddBackward0>)
##### event loss: tensor(2.7071, grad_fn=<NegBackward>) non event loss:  tensor([153530.1896], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(4.2264, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[2.8596e+00],
        [2.4446e-01],
        [2.2208e+00],
        [1.9699e-04]], grad_fn=<AddBackward0>)
##### event loss: tensor(8.0925, grad_fn=<NegBackward>) non event loss:  tensor([306975.8214], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1900, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[303.2403],
        [ 89.6365],
        [  2.3356],
        [528.2252],
        [ 45.0565]], grad_fn=<AddBackward0>)
##### event loss: tensor(-21.1360, grad_fn=<NegBackward>) non event loss:  tensor([1152864.6716], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0220, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[2.1949e-09],
        [1.6533e+01],
        [5.0294e-02],
        [2.0386e+00],
        [2.2674e+02]], grad_fn=<AddBackward0>)
##### event loss: tensor(13.9855, grad_fn=<NegBackward>) non event loss:  tensor([615042.6908], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0106, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[1.2954e-10],
        [1.4284e-10],
        [1.0293e-10],
        [3.4321e-10],
        [1.2655e-10],
        [6.2250e-08],
        [1.0004e-10],
        [1.0249e-10],
        [1.0007e-10],
        [1.0012e-10],
        [1.0000e-10],
        [1.1304e-10],
        [1.0000e-10],
        [1.0041e-10],
        [1.2198e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10]], grad_fn=<AddBackward0>)
  3%|█████▍                                                                                                                                                                                                          | 13/496 [00:02<01:42,  4.70it/s]
Traceback (most recent call last):
  File "main.py", line 126, in <module>
    main()
  File "main.py", line 122, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 84, in train
    loss.backward() # required_graph = True
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
### event lambdas:  tensor([[1.2475e+02],
        [8.5950e+01],
        [5.4982e-03],
        [1.0000e-10],
        [1.0184e-10]], grad_fn=<AddBackward0>)
##### event loss: tensor(41.9567, grad_fn=<NegBackward>) non event loss:  tensor([152060.0809], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0502, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [3.0304e+01]], grad_fn=<AddBackward0>)
##### event loss: tensor(88.6921, grad_fn=<NegBackward>) non event loss:  tensor([63657.7551], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0716, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10]], grad_fn=<AddBackward0>)
##### event loss: tensor(92.1034, grad_fn=<NegBackward>) non event loss:  tensor([111769.4350], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0518, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [2.9546e+01],
        [1.0000e-10]], grad_fn=<AddBackward0>)
##### event loss: tensor(88.7175, grad_fn=<NegBackward>) non event loss:  tensor([21839.9396], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0718, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[1.0001e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10]], grad_fn=<AddBackward0>)
##### event loss: tensor(253.2843, grad_fn=<NegBackward>) non event loss:  tensor([1682.8669], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1330, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[1.0000e-10],
        [4.0334e-08],
        [1.0000e-10],
        [1.0000e-10],
        [1.0033e-10]], grad_fn=<AddBackward0>)
##### event loss: tensor(109.1262, grad_fn=<NegBackward>) non event loss:  tensor([1.4861e-06], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.0789, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[1.0000e-10],
        [1.3337e-04],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10]], grad_fn=<AddBackward0>)
##### event loss: tensor(193.1292, grad_fn=<NegBackward>) non event loss:  tensor([1295.1975], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1438, grad_fn=<SumBackward0>)
### event lambdas:  tensor([[1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.1110e-09],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.0000e-10],
        [1.9029e+01]], grad_fn=<AddBackward0>)
##### event loss: tensor(178.8530, grad_fn=<NegBackward>) non event loss:  tensor([1825.3578], dtype=torch.float64, grad_fn=<MulBackward0>) chocie_l: tensor(0.1749, grad_fn=<SumBackward0>)