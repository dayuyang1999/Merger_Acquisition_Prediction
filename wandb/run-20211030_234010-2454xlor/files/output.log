
CUDA availability: True
  0%|â–Ž                                                                                                                                              | 1/496 [00:00<02:07,  3.88it/s]
Traceback (most recent call last):
  File "main.py", line 126, in <module>
    main()
  File "main.py", line 122, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 81, in train
    loss, timing_loss, choice_l = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 197, in forward
    choice_l = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 363, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
#### arr_b tensor([[[ 1.6461,  0.2316, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 1.8767,  0.3045, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.2556,  0.3868, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.6150,  0.4672, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 2.8471,  0.5928, -0.2817, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.2704,  0.5755, -0.2752, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.8558,  0.6120, -0.2765, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 4.3829,  0.6332, -0.2491, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 5.4617,  0.8968, -0.2266, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 3.3003,  0.5038, -0.0715, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415],
         [ 5.3630,  0.7951,  0.1857, -0.3415, -0.3415, -0.3415, -0.3415,
          -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415, -0.3415]]]) torch.Size([1, 14, 14])
#### mat_b tensor([[[-1.0710e-01,  4.3587e-02,  8.5565e-02,  3.6990e-02,  9.5317e-02,
           2.1370e-02, -3.3289e-02, -1.3960e-01,  4.4141e-02, -3.6422e-02,
          -6.1393e-02,  1.4886e-01,  1.2177e-01, -3.3804e-01, -1.1741e-01,
           1.1368e-01,  1.5069e-02,  8.6150e-02, -2.6197e-01, -6.9922e-02,
          -1.0432e-01, -1.2513e-01, -6.5778e-02,  1.9277e-02, -2.2402e-01,
           2.4249e-01,  7.6905e-02,  2.4085e-01, -1.8424e-01,  1.7295e-01,
           2.2445e-02,  1.3409e-01],
         [-9.5394e-02,  4.8869e-02,  7.4143e-02,  4.8551e-02,  1.0599e-01,
           2.0990e-02, -4.9373e-02, -1.6055e-01,  2.4778e-02, -2.9058e-02,
          -4.7428e-02,  1.5284e-01,  1.4431e-01, -3.5181e-01, -1.3530e-01,
           1.3503e-01,  1.5207e-02,  8.9082e-02, -2.8510e-01, -8.6696e-02,
          -1.1192e-01, -1.1580e-01, -7.0207e-02,  2.8132e-02, -2.5089e-01,
           2.4901e-01,  6.4822e-02,  2.6662e-01, -1.7990e-01,  1.5960e-01,
           2.2833e-02,  1.5600e-01],
         [-7.8279e-02,  5.2397e-02,  5.8525e-02,  6.8601e-02,  1.1320e-01,
           1.5009e-02, -7.5900e-02, -1.9356e-01,  1.2612e-02, -2.8478e-02,
          -2.4149e-02,  1.7087e-01,  1.7850e-01, -3.7007e-01, -1.5673e-01,
           1.7071e-01,  6.4113e-03,  9.3584e-02, -3.2160e-01, -1.1190e-01,
          -1.1375e-01, -9.8695e-02, -7.9368e-02,  3.4460e-02, -2.9770e-01,
           2.6560e-01,  4.4678e-02,  3.2146e-01, -1.8584e-01,  1.5250e-01,
           2.8993e-02,  1.8160e-01],
         [-6.3543e-02,  5.4865e-02,  4.4422e-02,  8.7373e-02,  1.1798e-01,
           1.0102e-02, -1.0256e-01, -2.2589e-01,  5.0591e-03, -3.3240e-02,
          -5.5889e-03,  1.9055e-01,  2.1118e-01, -3.8783e-01, -1.7418e-01,
           2.0279e-01, -2.9271e-03,  9.6250e-02, -3.5717e-01, -1.3523e-01,
          -1.1099e-01, -7.8363e-02, -9.1147e-02,  3.8393e-02, -3.4044e-01,
           2.8055e-01,  2.6262e-02,  3.7642e-01, -1.9307e-01,  1.4851e-01,
           3.5243e-02,  2.0676e-01],
         [-1.9429e-02,  4.5429e-02,  3.6255e-02,  1.0216e-01,  1.4132e-01,
           1.7279e-02, -1.2246e-01, -2.9725e-01,  1.2542e-02, -2.3342e-02,
          -4.8748e-03,  2.1878e-01,  2.2896e-01, -4.1084e-01, -2.1881e-01,
           2.1533e-01, -1.5462e-03,  1.1037e-01, -4.0074e-01, -1.5467e-01,
          -1.3062e-01, -6.0227e-02, -1.0793e-01,  3.1266e-02, -3.5768e-01,
           2.7914e-01, -2.1697e-04,  4.1605e-01, -2.2849e-01,  1.4022e-01,
           3.4412e-02,  2.4586e-01],
         [-1.6530e-02,  4.8398e-02,  1.8654e-02,  1.2598e-01,  1.4808e-01,
          -7.1976e-04, -1.4873e-01, -3.2773e-01,  8.8288e-03, -3.4122e-02,
           2.1189e-02,  2.3808e-01,  2.7480e-01, -4.3136e-01, -2.4340e-01,
           2.4716e-01, -1.4395e-02,  1.1627e-01, -4.3780e-01, -1.9084e-01,
          -1.1830e-01, -3.3654e-02, -1.1643e-01,  3.0839e-02, -4.1277e-01,
           3.0562e-01, -2.1177e-02,  4.7107e-01, -2.3840e-01,  1.4726e-01,
           3.9453e-02,  2.6592e-01],
         [-5.6695e-03,  5.0595e-02, -3.0714e-03,  1.5967e-01,  1.6152e-01,
          -1.8095e-02, -1.8864e-01, -3.8002e-01, -1.4732e-03, -4.3571e-02,
           5.4755e-02,  2.6631e-01,  3.3123e-01, -4.5925e-01, -2.7412e-01,
           2.9186e-01, -2.6194e-02,  1.2591e-01, -4.9298e-01, -2.3380e-01,
          -1.1022e-01, -2.5066e-03, -1.3045e-01,  3.1379e-02, -4.8217e-01,
           3.4077e-01, -5.1868e-02,  5.5117e-01, -2.5350e-01,  1.5470e-01,
           4.5127e-02,  3.0256e-01],
         [-3.2229e-03,  5.0909e-02, -1.9669e-02,  1.9339e-01,  1.7647e-01,
          -3.4263e-02, -2.2462e-01, -4.2653e-01, -1.6572e-02, -5.0743e-02,
           8.5275e-02,  2.8937e-01,  3.7791e-01, -4.8207e-01, -2.9339e-01,
           3.3153e-01, -3.1139e-02,  1.3618e-01, -5.4111e-01, -2.6728e-01,
          -1.0406e-01,  1.8400e-02, -1.3915e-01,  3.0116e-02, -5.3896e-01,
           3.7855e-01, -7.8993e-02,  6.2108e-01, -2.6642e-01,  1.6843e-01,
           4.7979e-02,  3.3807e-01],
         [-4.9944e-02,  5.8821e-02,  1.6909e-02,  1.2552e-01,  1.2952e-01,
          -1.1836e-02, -1.4805e-01, -2.8179e-01, -3.9462e-03, -4.6981e-02,
           3.3777e-02,  2.2448e-01,  2.8074e-01, -4.2158e-01, -2.1234e-01,
           2.5718e-01, -2.1801e-02,  1.0492e-01, -4.2069e-01, -1.8852e-01,
          -9.7213e-02, -3.7351e-02, -1.0792e-01,  3.9853e-02, -4.2584e-01,
           3.1853e-01, -8.8264e-03,  4.7078e-01, -2.0972e-01,  1.5385e-01,
           4.4298e-02,  2.4595e-01],
         [-4.9944e-02,  5.8821e-02,  1.6909e-02,  1.2552e-01,  1.2952e-01,
          -1.1836e-02, -1.4805e-01, -2.8179e-01, -3.9462e-03, -4.6981e-02,
           3.3777e-02,  2.2448e-01,  2.8074e-01, -4.2158e-01, -2.1234e-01,
           2.5718e-01, -2.1801e-02,  1.0492e-01, -4.2069e-01, -1.8852e-01,
          -9.7213e-02, -3.7351e-02, -1.0792e-01,  3.9853e-02, -4.2584e-01,
           3.1853e-01, -8.8264e-03,  4.7078e-01, -2.0972e-01,  1.5385e-01,
           4.4298e-02,  2.4595e-01],
         [-4.9944e-02,  5.8821e-02,  1.6909e-02,  1.2552e-01,  1.2952e-01,
          -1.1836e-02, -1.4805e-01, -2.8179e-01, -3.9462e-03, -4.6981e-02,
           3.3777e-02,  2.2448e-01,  2.8074e-01, -4.2158e-01, -2.1234e-01,
           2.5718e-01, -2.1801e-02,  1.0492e-01, -4.2069e-01, -1.8852e-01,
          -9.7213e-02, -3.7351e-02, -1.0792e-01,  3.9853e-02, -4.2584e-01,
           3.1853e-01, -8.8264e-03,  4.7078e-01, -2.0972e-01,  1.5385e-01,
           4.4298e-02,  2.4595e-01],
         [ 3.4828e-02,  5.7862e-02, -5.7733e-02,  2.5704e-01,  2.0462e-01,
          -4.0707e-02, -3.0769e-01, -5.3744e-01, -4.9233e-02, -6.1553e-02,
           1.3434e-01,  3.4559e-01,  4.5778e-01, -5.3248e-01, -3.3407e-01,
           4.2795e-01, -3.7055e-02,  1.5659e-01, -6.4876e-01, -3.2492e-01,
          -1.0850e-01,  6.1010e-02, -1.6541e-01,  3.6512e-02, -6.4779e-01,
           4.3823e-01, -1.2899e-01,  7.8973e-01, -2.9512e-01,  1.7112e-01,
           5.5893e-02,  4.2908e-01],
         [-4.9944e-02,  5.8821e-02,  1.6909e-02,  1.2552e-01,  1.2952e-01,
          -1.1836e-02, -1.4805e-01, -2.8179e-01, -3.9462e-03, -4.6981e-02,
           3.3777e-02,  2.2448e-01,  2.8074e-01, -4.2158e-01, -2.1234e-01,
           2.5718e-01, -2.1801e-02,  1.0492e-01, -4.2069e-01, -1.8852e-01,
          -9.7213e-02, -3.7351e-02, -1.0792e-01,  3.9853e-02, -4.2584e-01,
           3.1853e-01, -8.8264e-03,  4.7078e-01, -2.0972e-01,  1.5385e-01,
           4.4298e-02,  2.4595e-01],
         [-3.2832e-02,  7.5094e-02, -5.1181e-02,  2.5181e-01,  1.7155e-01,
          -5.0211e-02, -2.9859e-01, -4.4317e-01, -8.0403e-02, -7.9177e-02,
           1.4544e-01,  3.1174e-01,  4.4248e-01, -5.0436e-01, -2.5515e-01,
           4.3759e-01, -3.6498e-02,  1.3596e-01, -6.0288e-01, -2.9750e-01,
          -7.8661e-02,  3.3681e-02, -1.4169e-01,  5.2176e-02, -6.4159e-01,
           4.5848e-01, -9.6077e-02,  7.7123e-01, -2.3829e-01,  1.8659e-01,
           5.9884e-02,  3.9235e-01]]], grad_fn=<AddBackward0>) torch.Size([1, 14, 32])
#### grad: tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]])
#### arr_b tensor([[[ 1.6107,  0.2177, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 1.8378,  0.2896, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 2.2109,  0.3706, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 2.5649,  0.4498, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 2.7934,  0.5735, -0.2877, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2103,  0.5564, -0.2813, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.7867,  0.5924, -0.2826, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 4.3058,  0.6133, -0.2556, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 5.3681,  0.8729, -0.2334, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 5.2710,  0.7727,  0.1726, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 5.0511,  0.7244,  0.2207, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.4858, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 4.3120,  0.7036,  0.3016, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.3324, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.3956, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.3668, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466],
         [ 3.2397,  0.3505, -0.0807, -0.3466, -0.3466, -0.3466, -0.3466,
          -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466, -0.3466]]]) torch.Size([1, 23, 14])
#### mat_b tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan],
         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]],
       grad_fn=<AddBackward0>) torch.Size([1, 23, 32])