
CUDA availability: True
Traceback (most recent call last):
  File "main.py", line 89, in <module>
    main()
  File "main.py", line 85, in main
    model_trained = train(dataset, config,  device)
  File "main.py", line 48, in train
    loss, timing_loss, choice_loss  = model(arr_b.float(), arr_c.float(), arr_delta_time.float(), event_data, non_event_data, estimate_length, choice_data_dict)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 195, in forward
    event_choice_loss = self.choice_net(mat_b, mat_c, choice_data_dict, self.s_year, self.e_year)*1000
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/dalab5/Projects/MA_packed/model.py", line 355, in forward
    choice_l = F.binary_cross_entropy(torch.sigmoid(logits_i), true_tar_idxs_i)  # inputs are both (N_i_2, N_i_1)
  File "/home/dalab5/miniconda3/envs/GNN/lib/python3.8/site-packages/torch/nn/functional.py", line 2759, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: all elements of input should be between 0 and 1
### arr_b_idx_i tensor(0)
tensor([[[ 0.1194,  0.1098,  0.0540,  ..., -0.0017,  0.0459,  0.0214]]],
       grad_fn=<SumBackward1>)
### arr_b_idx_i tensor(1)
tensor([[[ 0.0504, -0.0111,  0.0287,  ...,  0.0161, -0.0108, -0.0002]]],
       grad_fn=<SumBackward1>)
### arr_b_idx_i tensor(5)
tensor([[[-0.0012,  0.0136,  0.1447,  ...,  0.0473,  0.0876,  0.1706]]],
       grad_fn=<SumBackward1>)
### arr_b_idx_i tensor(8)
tensor([[[ 0.0152, -0.0945, -0.0470,  ...,  0.0297,  0.0500,  0.0176]]],
       grad_fn=<SumBackward1>)
### arr_b_idx_i tensor(13)
tensor([[[0.0408, 0.1476, 0.1197,  ..., 0.1453, 0.1724, 0.0792]]],
       grad_fn=<SumBackward1>)
### arr_b_idx_i tensor(16)
tensor([[[-0.0433, -0.0148,  0.0103,  ...,  0.0117, -0.0409, -0.0035]]],
       grad_fn=<SumBackward1>)
### arr_b_idx_i tensor([19, 19])
tensor([[[-0.0221,  0.0853,  0.0161,  ..., -0.0155, -0.0099, -0.0463],
         [-0.0667, -0.0210, -0.0627,  ..., -0.0838, -0.0974, -0.0786]]],
       grad_fn=<SumBackward1>)
### arr_b_idx_i tensor(21)
tensor([[[-0.0055, -0.0012,  0.0338,  ..., -0.0025, -0.0064, -0.0177]]],
       grad_fn=<SumBackward1>)
### arr_b_idx_i tensor([3, 3])
tensor([[[nan, nan, nan,  ..., nan, nan, nan],
         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<SumBackward1>)